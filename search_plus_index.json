{"./":{"url":"./","title":"Introduction","keywords":"","body":"关于本书 本书为电子书形式，内容为本人多年的 Kubernetes运维过程中实战经验进行系统性记录 ，以备快速查找，仅记录 在实践中学习，在学习中实践 "},"docs/Kubernetes/k8s-common-commands.html":{"url":"docs/Kubernetes/k8s-common-commands.html","title":"k8s常用批量命令","keywords":"","body":" 仅限腾讯云TKE集群中使用，其他容器集群或者自建仅参考，依赖于环境 1，节点相关 表格输出各节点占用的 podCIDR kubectl get no -o=custom-columns=INTERNAL-IP:.metadata.name,EXTERNAL-IP:.status.addresses[1].address,CIDR:.spec.podCIDR INTERNAL-IP EXTERNAL-IP CIDR 172.30.2.5 139.186.202.9 172.16.0.0/26 表格输出各节点总可用资源 (Allocatable) kubectl get no -o=custom-columns=\"NODE:.metadata.name,ALLOCATABLE CPU:.status.allocatable.cpu,ALLOCATABLE MEMORY:.status.allocatable.memory\" NODE ALLOCATABLE CPU ALLOCATABLE MEMORY 172.30.2.5 1930m 1347064Ki 输出各节点已分配资源的情况 kubectl get nodes --no-headers | awk '{print $1}' | xargs -I {} sh -c \"echo {} ; kubectl describe node {} | grep Allocated -A 5 | grep -ve Event -ve Allocated -ve percent -ve --;\" 2，查询所有节点挂载卷信息 kubectl get node -ocustom-columns='节点名称:.metadata.name,容器网段:.spec.podCIDR,eni-ip配额:.status.capacity.\"tke\\.cloud\\.tencent\\.com\\/eni-ip\",挂卷:.status.volumesInUse' 3，查询所有节点IP和实例ID ## kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:节点IP.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID ##查看节点的内核版本 kubectl get node -ocustom-columns=节点名称:.metadata.name,节点内核:.status.nodeInfo.kernelVersion,节点UUID:.status.nodeInfo.machineID,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\" ##全部信息 kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID,节点内核:.status.nodeInfo.kernelVersion,节点UUID:.status.nodeInfo.machineID,可用区:.metadata.labels.\"failure-domain\\.beta\\.kubernetes\\.io\\/zone\" kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name} {.status.addresses[?(@.type==\"ExternalIP\")].address}{\"\\n\"}' #获取节点IP： kubectl get node -ocustom-columns=name:.metadata.name,IP:status.addresses[0].address 总结如上命令 kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID,节点内核:.status.nodeInfo.kernelVersion,节点UUID:.status.nodeInfo.machineID,可用区:.metadata.labels.\"failure-domain\\.beta\\.kubernetes\\.io\\/zone\" 查看容量： kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID,节点内核:.status.nodeInfo.kernelVersion,Allocatable:.status.allocatable.\"nvidia\\.com\\/gpu\" kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID,节点内核:.status.nodeInfo.kernelVersion,GPU-Allocatable:.status.allocatable.\"nvidia\\.com\\/gpu\",CPU-Allocatable:.status.allocatable.cpu,MEM-Allocatable:.status.allocatable.memory,eniIP-Allocatable:.status.allocatable.\"tke\\.cloud\\.tencent\\.com\\/eni-ip\" 查看节点当前所在的节点池 kubectl get node -ocustom-columns='Name:.metadata.name,节点池:.metadata.labels.tke\\.cloud\\.tencent\\.com\\/nodepool-id,伸缩组:.metadata.labels.cloud\\.tencent\\.com\\/auto-scaling-group-id,createTime:.metadata.creationTimestamp' 4，查询所有节点eni-ip信息 #查下辅助网卡eniInfo详细信息 kubectl get nec -ocustom-columns=Name:.metadata.name,cvmID:.spec.providerID,eni:.status.eniInfos,ip:.spec.maxIPPerENI #查看集群节点vpc-cni配额 kubectl get node -ocustom-columns=Name:.metadata.name,capcIP:.status.capacity.\"tke\\.cloud\\.tencent\\.com/eni-ip\",allocIP:.status.allocatable.\"tke\\.cloud\\.tencent\\.com/eni-ip\" #查询VIP和VIPC绑定关系 kubectl get vip -ocustom-columns=Name:.metadata.name,vipcNamespace:.spec.claimRef.namespace,type:spec.type,vipcName:.spec.claimRef.name 5，批量删除驱逐状态POD 查寻确认没问题后再做删除，需要把NameSpace替换成用户的命名空间名称 kubectl get pods -n NameSpace |grep Evicted 然后是批量删除 kubectl get pods -n NameSpace | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n NameSpace #所有命名空间 kubectl get pods --all-namespaces |grep Evicted 然后是批量删除 kubectl get pods --all-namespaces | grep Evicted | awk '{print $1}' | xargs kubectl delete pod --all-namespaces 6，批量删除UnexpectedAdmissionError状态pod #指定命名空间去删除 kubectl get pods -n NAMESPACE | grep UnexpectedAdmissionError | awk '{print $1}' | xargs kubectl delete pod -n NAMESPACE 7，批量查看集群中LB类型的service信息 #可以打印的第一行，然后匹配的其他行 kubectl get svc -ocustom-columns=namespace:.metadata.namespace,serviceName:.metadata.name,type:.spec.type,lbID:.metadata.annotations.\"service\\.kubernetes\\.io\\/loadbalance-id\",subnet:.metadata.annotations.\"service\\.kubernetes\\.io\\/qcloud-loadbalancer-internal-subnetid\" -A | grep -E \"serviceName|LoadBalancer\" #查看LBid & VIP信息 kubectl get svc -ocustom-columns=namespace:.metadata.namespace,serviceName:.metadata.name,type:.spec.type,lbID:.metadata.annotations.\"service\\.kubernetes\\.io\\/loadbalance-id\",subnet:.metadata.annotations.\"service\\.kubernetes\\.io\\/qcloud-loadbalancer-internal-subnetid\",vip:.status.loadBalancer.ingress[0].ip -A | grep -E \"serviceName|LoadBalancer\" 8，批量查看集群中ingress信息 kubectl get ingress -ocustom-columns=namespace:.metadata.namespace,ingressName:.metadata.name,ingressType:.metadata.annotations.\"kubernetes\\.io\\/ingress\\.class\",lbID:.metadata.annotations.\"kubernetes\\.io\\/ingress\\.qcloud-loadbalance-id\",vip:.status.loadBalancer.ingress[0].ip,uuid:.metadata.uid -A 9，批量查询PV和PVC 绑定关系 kubectl get pv -ocustom-columns='pvName:.metadata.name,storageClassName:.spec.storageClassName,pvcNamespace:.spec.claimRef.namespace,pvcName:.spec.claimRef.name,volumeHandle:.spec.csi.volumeHandle,driver:.spec.csi.driver' 批量查看CBS盘信息 kubectl get pv -ocustom-columns='pvName:.metadata.name,storageClassName:.spec.storageClassName,pvcNamespace:.spec.claimRef.namespace,pvcName:.spec.claimRef.name,volumeHandle:.spec.csi.volumeHandle,driver:.spec.csi.driver,CBSid:.spec.qcloudCbs.cbsDiskId' 10，显示Pod的容器镜像 kubectl get pods -o custom-columns='NAME:metadata.name,IMAGES:spec.containers[*].image' kubectl get deployments -o custom-columns='DeploymentName:metadata.name,Namespace:.metadata.namespace,image:.spec.template.spec.containers[*].image' --all-namespaces kubectl get statefulset -o custom-columns='DeploymentName:metadata.name,Namespace:.metadata.namespace,image:.spec.template.spec.containers[*].image' --all-namespaces kubectl get pods -o custom-columns='NAME:metadata.name,security-group-id:.metadata.annotations.eks\\.tke\\.cloud\\.tencent\\.com\\/security-group-id' kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{\"\\n\"}{.metadata.name}{\":\\t\"}{range .spec.containers[*]}{.image}{\", \"}{end}{end}' 11，批量查询POD的里面容器ID kubectl get pods -o custom-columns='podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[0].containerID' #kubectl get pods -o custom-columns=podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[*].containerID #kubectl get pods -o custom-columns=podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[0].containerID | awk -F 'docker://' '{print $1,$2}' #kubectl get pods -o custom-columns='podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[0].containerID' | awk -F 'docker://|| containerd://' '{print $1,$2}' ##批量查下POD的 POD_ID 可以对照查看对应日志命令文件 kubectl get pods -o custom-columns='Namespace:..metadata.namespace,podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeIP:.status.hostIP,Pod_ID:.metadata.uid,ContainerName:.spec.containers[*].name' 查看POD创建时间： kubectl get pods -o custom-columns='podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,startTime:.status.startTime' 12，批量查看容器ID docker ps | awk '{print $1}' | xargs -n 1 -I {} -- sh -c \"echo {} && docker inspect {} |grep 27c0e22fc5ffa8aa540b68182672ea61fd2bb701740926025ba77a66b4428341 -C 1\" 13，EKS获取所有EIP列表 kubectl get pods -A -o=jsonpath='{range .items[*]}{\"pod_name:\"}{.metadata.name}{\"\\teip-id:\"}{.metadata.annotations.tke\\.cloud\\.tencent\\.com\\/eip-id}{\"\\teip-ip:\"}{.metadata.annotations.tke\\.cloud\\.tencent\\.com\\/eip\\-public\\-ip}{\"\\n\"}{end}' 或者使用这个命令 kubectl get pod -ocustom-columns=pod-name:.metadata.name,eip-id:.metadata.annotations.\"tke\\.cloud\\.tencent\\.com\\/eip\\-id\",eip-public-ip:.metadata.annotations.\"tke\\.cloud\\.tencent\\.com\\/eip\\-public\\-ip\" 14，批量查看集群里面有那些RS使用了 csi inline模式 kubectl get rs -o custom-columns=ReplicaSetsName:.metadata.name,CurrentPOD:.spec.replicas,CBSid:.spec.template.spec.volumes[*].qcloudCbs.cbsDiskId -A #查看当前有哪些POD在使用csi inline kubectl get pods -o custom-columns=Namespace:.metadata.namespace,PodName:.metadata.name,Status:.status.phase,CBSid:.spec.volumes[*].qcloudCbs.cbsDiskId #通过这个简单查询下 有那些rs使用了 kubectl get rs -o custom-columns=namespace:.metadata.namespace,ReplicaSetsName:.metadata.name,CurrentPOD:.spec.replicas,CBSid:.spec.template.spec.volumes[*].qcloudCbs.cbsDiskId -A | grep disk 查看 StatefulSet类型是否使用csi inline kubectl get sts -o custom-columns=namespace:.metadata.namespace,StatefulSetName:.metadata.name,CBSid:.spec.template.spec.volumes[*].qcloudCbs.cbsDiskId -A | grep disk 15，批量查看pod资源设置情况limit和request #调整显示顺序 kubectl get pod -o custom-columns='namespace:.metadata.namespace,PodName:.metadata.name,ContainerName:.spec.containers[*].name,CPURequest:.spec.containers[*].resources.requests.cpu,CPULimit:.spec.containers[*].resources.limits.cpu,MemRequest:.spec.containers[*].resources.requests.memory,MemLimit:.spec.containers[*].resources.limits.memory,Node:.status.hostIP' kubectl get pod -n kube-system -o=custom-columns='NAME:.metadata.name,NAMESPACE:.metadata.namespace,PHASE:.status.phase,Request-cpu:.spec.containers\\[0\\].resources.requests.cpu,Request-memory:.spec.containers\\[0\\].resources.requests.memory,Limit-cpu:.spec.containers\\[0\\].resources.limits.cpu,Limit-memory:.spec.containers\\[0\\].resources.limits.memory,Node:.status.hostIP' 16，批量查看集群容器网段 kubectl get node -ocustom-columns='Name:.metadata.name,cidr:.spec.podCIDR,annotations:.metadata.annotations.\"tke\\.cloud\\.tencent\\.com\\/pod-cidrs\"' 17，批量查看日志采集配置 kubectl get logconfig -ocustom-columns='Name:.metadata.name,ClsTopic:.spec.clsDetail.topicId,ClsType:.spec.inputDetail.type' 查看采集对象： kubectl get logconfig -ocustom-columns='Name:.metadata.name,ClsTopic:.spec.clsDetail.topicId,ClsType:.spec.inputDetail.type,Namespace:.spec.inputDetail.containerStdout.workloads[*].namespace,Workloads:.spec.inputDetail.containerStdout.workloads[*].name' 18，批量删除多个job #删除成功的作业: kubectl delete jobs --field-selector status.successful=1 #删除失败或长时间运行的作业 kubectl delete jobs --field-selector status.successful=0 19，批量查看集群有没有POD使用GPU kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.nodeName}{\"\\n\"}{end}' | while read line; do pod=$(echo $line | awk '{print $1}'); node=$(echo $line | awk '{print $2}'); echo -n \"${pod} \"; kubectl describe pod $pod | grep -q \"nvidia.com/gpu\" && echo \"is using GPU on ${node}\" || echo \"is not using GPU\"; done "},"docs/Kubernetes/k8s-kubectl-commands.html":{"url":"docs/Kubernetes/k8s-kubectl-commands.html","title":"kubectl常用命令","keywords":"","body":"本文是对k8s，kubectl常用命令的总结。 语法 kubectl [command] [TYPE] [NAME] [flags] 1 command：子命令，用于操作Kubernetes集群资源对象的命令，如create, delete, describe, get, apply等 2 TYPE：资源对象的类型，如pod, service, rc, deployment, node等，可以单数、复数以及简写（pod, pods, po/service, services, svc） 3 NAME：资源对象的名称，不指定则返回所有，如get pod 会返回所有pod， get pod nginx， 只返回nginx这个pod 4 flags：kubectl子命令的可选参数，例如-n 指定namespace，-s 指定apiserver的URL 资源对象类型列表 可以用这个命令获取到： kubectl explain 或者 kubectl api-resources 名称 简写 componentsstatuses cs daemonsets ds deployment deploy events ev endpoints ep horizontalpodautoscalers hpa ingresses ing jobs limitranges limits nodes no namspaces ns pods po persistentvolumes pv persistentvolumeclaims pvc resourcequotas quota replicationcontrollers rc secrets serviceaccounts sa services svc 特殊用法： kubectl子命令 主要包括对资源的创建、删除、查看、修改、配置、运行等 kubectl --help 可以查看所有子命令 kubectl参数 kubectl options 可以查看支持的参数，例如–namespace指定所在namespace kubectl输出格式 kubectl命令可以用多种格式对结果进行显示，输出格式通过-o参数指定： -o支持的格式有 输出格式 说明 custom-columns= 根据自定义列名进行输出，逗号分隔 custom-columns-file= 从文件中获取自定义列名进行输出 json 以JSON格式显示结果 jsonpath= 输出jasonpath表达式定义的字段信息 jasonpath-file= 输出jsonpath表达式定义的字段信息，来源于文件 name 仅输出资源对象的名称 wide 输出更多信息，比如会输出node名 yaml 以yaml格式输出 kubectl命令示例： 1） 创建资源对象 根据yaml文件创建service和deployment,在create和apply之间，我更倾向于apply，这是因为，如果更改了yaml文件，apply不用删除就可应用变更。 kubectl create -f service.yaml -f deploy.yaml kubectl apply -f service.yaml -f deploy.yaml 也可以指定一个目录，这样可以一次性根据该目录下所有yaml或json文件定义资源 kubectl create -f 2） 查看资源对象查看所有pod kubectl get pods 查看deployment和service kubectl get deploy,svc 3） 描述资源对象显示node的详细信息 kubectl describe nodes 显示pod的详细信息 kubectl describe pods/ 显示deployment管理的pod信息 kubectl describe pods 4） 删除资源对象基于yaml文件删除 kubectl delete -f pod.yaml 删除所有包含某个label的pod和service kubectl delete po,svc -l name= 删除所有pod kubectl delete po --all 5） 执行容器的命令在pod中执行某个命令，如date kubectl exec date //pod-name如果不加，默认会选择第一个pod 指定pod的某个容器执行命令 kubectl exec date 进入到pod的容器里 kubectl exec -it bash 6） 查看容器日志 kubectl logs 可以动态查看，类似于tail -f kubectl logs -f -c "},"docs/Kubernetes/dockerfile-best-practices.html":{"url":"docs/Kubernetes/dockerfile-best-practices.html","title":"Dockerfile最佳实践","keywords":"","body":"Dockerfile实践 容器应该是短暂的 通过 Dockerfile 构建的镜像所启动的容器应该尽可能短暂（生命周期短）。「短暂」意味着可以停止和销毁容器，并且创建一个新容器并部署好所需的设置和配置工作量应该是极小的。 使用 .dockerignore 文件 使用 Dockerfile 构建镜像时最好是将 Dockerfile 放置在一个新建的空目录下。然后将构建镜像所需要的文件添加到该目录中。为了提高构建镜像的效率，你可以在目录下新建一个 .dockerignore 文件来指定要忽略的文件和目录。.dockerignore 文件的排除模式语法和 Git 的 .gitignore 文件相似。 使用多阶段构建 在 Docker 17.05 以上版本中，你可以使用 多阶段构建 来减少所构建镜像的大小。 避免安装不必要的包 为了降低复杂性、减少依赖、减小文件大小、节约构建时间，你应该避免安装任何不必要的包。例如，不要在数据库镜像中包含一个文本编辑器。 一个容器只运行一个进程 应该保证在一个容器中只运行一个进程。将多个应用解耦到不同容器中，保证了容器的横向扩展和复用。例如 web 应用应该包含三个容器：web应用、数据库、缓存。 如果容器互相依赖，你可以使用 Docker 自定义网络 来把这些容器连接起来。 镜像层数尽可能少 你需要在 Dockerfile 可读性（也包括长期的可维护性）和减少层数之间做一个平衡。 将多行参数排序 将多行参数按字母顺序排序（比如要安装多个包时）。这可以帮助你避免重复包含同一个包，更新包列表时也更容易。也便于 PRs 阅读和审查。建议在反斜杠符号 \\ 之前添加一个空格，以增加可读性。 下面是来自 buildpack-deps 镜像的例子： RUN apt-get update && apt-get install -y \\ bzr \\ cvs \\ git \\ mercurial \\ subversion 构建缓存 在镜像的构建过程中，Docker 会遍历 Dockerfile 文件中的指令，然后按顺序执行。在执行每条指令之前，Docker 都会在缓存中查找是否已经存在可重用的镜像，如果有就使用现存的镜像，不再重复创建。如果你不想在构建过程中使用缓存，你可以在 docker build 命令中使用 --no-cache=true 选项。 但是，如果你想在构建的过程中使用缓存，你得明白什么时候会，什么时候不会找到匹配的镜像，遵循的基本规则如下： 从一个基础镜像开始（FROM 指令指定），下一条指令将和该基础镜像的所有子镜像进行匹配，检查这些子镜像被创建时使用的指令是否和被检查的指令完全一样。如果不是，则缓存失效。 在大多数情况下，只需要简单地对比 Dockerfile 中的指令和子镜像。然而，有些指令需要更多的检查和解释。 对于 ADD 和 COPY 指令，镜像中对应文件的内容也会被检查，每个文件都会计算出一个校验和。文件的最后修改时间和最后访问时间不会纳入校验。在缓存的查找过程中，会将这些校验和和已存在镜像中的文件校验和进行对比。如果文件有任何改变，比如内容和元数据，则缓存失效。 除了 ADD 和 COPY 指令，缓存匹配过程不会查看临时容器中的文件来决定缓存是否匹配。例如，当执行完 RUN apt-get -y update 指令后，容器中一些文件被更新，但 Docker 不会检查这些文件。这种情况下，只有指令字符串本身被用来匹配缓存。 一旦缓存失效，所有后续的 Dockerfile 指令都将产生新的镜像，缓存不会被使用。 Dockerfile 指令 下面针对 Dockerfile 中各种指令的最佳编写方式给出建议。 FROM 尽可能使用当前官方仓库作为你构建镜像的基础。推荐使用 Alpine (opens new window)镜像，因为它被严格控制并保持最小尺寸（目前小于 5 MB），但它仍然是一个完整的发行版。 LABEL 你可以给镜像添加标签来帮助组织镜像、记录许可信息、辅助自动化构建等。每个标签一行，由 LABEL 开头加上一个或多个标签对。下面的示例展示了各种不同的可能格式。# 开头的行是注释内容。 注意：如果你的字符串中包含空格，必须将字符串放入引号中或者对空格使用转义。如果字符串内容本身就包含引号，必须对引号使用转义。 # Set one or more individual labels LABEL com.example.version=\"0.0.1-beta\" LABEL vendor=\"ACME Incorporated\" LABEL com.example.release-date=\"2015-02-12\" LABEL com.example.version.is-production=\"\" 一个镜像可以包含多个标签，但建议将多个标签放入到一个 LABEL 指令中。 # Set multiple labels at once, using line-continuation characters to break long lines LABEL vendor=ACME\\ Incorporated \\ com.example.is-beta= \\ com.example.is-production=\"\" \\ com.example.version=\"0.0.1-beta\" \\ com.example.release-date=\"2015-02-12\" 关于标签可以接受的键值对，参考 Understanding object labels (opens new window)。关于查询标签信息，参考 Managing labels on objects (opens new window)。 RUN 为了保持 Dockerfile 文件的可读性，可理解性，以及可维护性，建议将长的或复杂的 RUN 指令用反斜杠 \\ 分割成多行。 apt-get RUN 指令最常见的用法是安装包用的 apt-get。因为 RUN apt-get 指令会安装包，所以有几个问题需要注意。 不要使用 RUN apt-get upgrade 或 dist-upgrade，因为许多基础镜像中的「必须」包不会在一个非特权容器中升级。如果基础镜像中的某个包过时了，你应该联系它的维护者。如果你确定某个特定的包，比如 foo，需要升级，使用 apt-get install -y foo 就行，该指令会自动升级 foo 包。 永远将 RUN apt-get update 和 apt-get install 组合成一条 RUN 声明，例如： RUN apt-get update && apt-get install -y \\ package-bar \\ package-baz \\ package-foo 将 apt-get update 放在一条单独的 RUN 声明中会导致缓存问题以及后续的 apt-get install 失败。比如，假设你有一个 Dockerfile 文件： FROM ubuntu:18.04 RUN apt-get update RUN apt-get install -y curl 构建镜像后，所有的层都在 Docker 的缓存中。假设你后来又修改了其中的 apt-get install 添加了一个包： FROM ubuntu:18.04 RUN apt-get update RUN apt-get install -y curl nginx Docker 发现修改后的 RUN apt-get update 指令和之前的完全一样。所以，apt-get update 不会执行，而是使用之前的缓存镜像。因为 apt-get update 没有运行，后面的 apt-get install 可能安装的是过时的 curl 和 nginx 版本。 使用 RUN apt-get update && apt-get install -y 可以确保你的 Dockerfiles 每次安装的都是包的最新的版本，而且这个过程不需要进一步的编码或额外干预。这项技术叫作 cache busting。你也可以显示指定一个包的版本号来达到 cache-busting，这就是所谓的固定版本，例如： RUN apt-get update && apt-get install -y \\ package-bar \\ package-baz \\ package-foo=1.3.* 固定版本会迫使构建过程检索特定的版本，而不管缓存中有什么。这项技术也可以减少因所需包中未预料到的变化而导致的失败。 下面是一个 RUN 指令的示例模板，展示了所有关于 apt-get 的建议。 RUN apt-get update && apt-get install -y \\ aufs-tools \\ automake \\ build-essential \\ curl \\ dpkg-sig \\ libcap-dev \\ libsqlite3-dev \\ mercurial \\ reprepro \\ ruby1.9.1 \\ ruby1.9.1-dev \\ s3cmd=1.1.* \\ && rm -rf /var/lib/apt/lists/* 其中 s3cmd 指令指定了一个版本号 1.1.*。如果之前的镜像使用的是更旧的版本，指定新的版本会导致 apt-get udpate 缓存失效并确保安装的是新版本。 另外，清理掉 apt 缓存 var/lib/apt/lists 可以减小镜像大小。因为 RUN 指令的开头为 apt-get udpate，包缓存总是会在 apt-get install 之前刷新。 注意：官方的 Debian 和 Ubuntu 镜像会自动运行 apt-get clean，所以不需要显式的调用 apt-get clean。 CMD CMD 指令用于执行目标镜像中包含的软件，可以包含参数。CMD 大多数情况下都应该以 CMD [\"executable\", \"param1\", \"param2\"...] 的形式使用。因此，如果创建镜像的目的是为了部署某个服务(比如 Apache)，你可能会执行类似于 CMD [\"apache2\", \"-DFOREGROUND\"] 形式的命令。我们建议任何服务镜像都使用这种形式的命令。 多数情况下，CMD 都需要一个交互式的 shell (bash, Python, perl 等)，例如 CMD [\"perl\", \"-de0\"]，或者 CMD [\"PHP\", \"-a\"]。使用这种形式意味着，当你执行类似 docker run -it python 时，你会进入一个准备好的 shell 中。CMD 应该在极少的情况下才能以 CMD [\"param\", \"param\"] 的形式与 ENTRYPOINT 协同使用，除非你和你的镜像使用者都对 ENTRYPOINT 的工作方式十分熟悉。 EXPOSE EXPOSE 指令用于指定容器将要监听的端口。因此，你应该为你的应用程序使用常见的端口。例如，提供 Apache web 服务的镜像应该使用 EXPOSE 80，而提供 MongoDB 服务的镜像使用 EXPOSE 27017。 对于外部访问，用户可以在执行 docker run 时使用一个标志来指示如何将指定的端口映射到所选择的端口。 ENV 为了方便新程序运行，你可以使用 ENV 来为容器中安装的程序更新 PATH 环境变量。例如使用 ENV PATH /usr/local/nginx/bin:$PATH 来确保 CMD [\"nginx\"] 能正确运行。 ENV 指令也可用于为你想要容器化的服务提供必要的环境变量，比如 Postgres 需要的 PGDATA。 最后，ENV 也能用于设置常见的版本号，比如下面的示例： ENV PG_MAJOR 9.3 ENV PG_VERSION 9.3.4 RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress && … ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH 类似于程序中的常量，这种方法可以让你只需改变 ENV 指令来自动的改变容器中的软件版本。 ADD 和 COPY 虽然 ADD 和 COPY 功能类似，但一般优先使用 COPY。因为它比 ADD 更透明。COPY 只支持简单将本地文件拷贝到容器中，而 ADD 有一些并不明显的功能（比如本地 tar 提取和远程 URL 支持）。因此，ADD 的最佳用例是将本地 tar 文件自动提取到镜像中，例如 ADD rootfs.tar.xz。 如果你的 Dockerfile 有多个步骤需要使用上下文中不同的文件。单独 COPY 每个文件，而不是一次性的 COPY 所有文件，这将保证每个步骤的构建缓存只在特定的文件变化时失效。例如： COPY requirements.txt /tmp/ RUN pip install --requirement /tmp/requirements.txt COPY . /tmp/ 如果将 COPY . /tmp/ 放置在 RUN 指令之前，只要 . 目录中任何一个文件变化，都会导致后续指令的缓存失效。 为了让镜像尽量小，最好不要使用 ADD 指令从远程 URL 获取包，而是使用 curl 和 wget。这样你可以在文件提取完之后删掉不再需要的文件来避免在镜像中额外添加一层。比如尽量避免下面的用法： ADD http://example.com/big.tar.xz /usr/src/things/ RUN tar -xJf /usr/src/things/big.tar.xz -C /usr/src/things RUN make -C /usr/src/things all 而是应该使用下面这种方法： RUN mkdir -p /usr/src/things \\ && curl -SL http://example.com/big.tar.xz \\ | tar -xJC /usr/src/things \\ && make -C /usr/src/things all 上面使用的管道操作，所以没有中间文件需要删除。 对于其他不需要 ADD 的自动提取功能的文件或目录，你应该使用 COPY。 ENTRYPOINT ENTRYPOINT 的最佳用处是设置镜像的主命令，允许将镜像当成命令本身来运行（用 CMD 提供默认选项）。 例如，下面的示例镜像提供了命令行工具 s3cmd: ENTRYPOINT [\"s3cmd\"] CMD [\"--help\"] 现在直接运行该镜像创建的容器会显示命令帮助： $ docker run s3cmd 或者提供正确的参数来执行某个命令： $ docker run s3cmd ls s3://mybucket 这样镜像名可以当成命令行的参考。 ENTRYPOINT 指令也可以结合一个辅助脚本使用，和前面命令行风格类似，即使启动工具需要不止一个步骤。 例如，Postgres 官方镜像使用下面的脚本作为 ENTRYPOINT： #!/bin/bash set -e if [ \"$1\" = 'postgres' ]; then chown -R postgres \"$PGDATA\" if [ -z \"$(ls -A \"$PGDATA\")\" ]; then gosu postgres initdb fi exec gosu postgres \"$@\" fi exec \"$@\" 注意：该脚本使用了 Bash 的内置命令 exec，所以最后运行的进程就是容器的 PID 为 1 的进程。这样，进程就可以接收到任何发送给容器的 Unix 信号了。 该辅助脚本被拷贝到容器，并在容器启动时通过 ENTRYPOINT 执行： COPY ./docker-entrypoint.sh / ENTRYPOINT [\"/docker-entrypoint.sh\"] 该脚本可以让用户用几种不同的方式和 Postgres 交互。 你可以很简单地启动 Postgres： $ docker run postgres 也可以执行 Postgres 并传递参数： $ docker run postgres postgres --help 最后，你还可以启动另外一个完全不同的工具，比如 Bash： $ docker run --rm -it postgres bash VOLUME VOLUME 指令用于暴露任何数据库存储文件，配置文件，或容器创建的文件和目录。强烈建议使用 VOLUME 来管理镜像中的可变部分和用户可以改变的部分。 USER 如果某个服务不需要特权执行，建议使用 USER 指令切换到非 root 用户。先在 Dockerfile 中使用类似 RUN groupadd -r postgres && useradd -r -g postgres postgres 的指令创建用户和用户组。 注意：在镜像中，用户和用户组每次被分配的 UID/GID 都是不确定的，下次重新构建镜像时被分配到的 UID/GID 可能会不一样。如果要依赖确定的 UID/GID，你应该显示的指定一个 UID/GID。 你应该避免使用 sudo，因为它不可预期的 TTY 和信号转发行为可能造成的问题比它能解决的问题还多。如果你真的需要和 sudo 类似的功能（例如，以 root 权限初始化某个守护进程，以非 root 权限执行它），你可以使用 gosu (opens new window)。 最后，为了减少层数和复杂度，避免频繁地使用 USER 来回切换用户。 WORKDIR 为了清晰性和可靠性，你应该总是在 WORKDIR 中使用绝对路径。另外，你应该使用 WORKDIR 来替代类似于 RUN cd ... && do-something 的指令，后者难以阅读、排错和维护。 官方镜像示例 这些官方镜像的 Dockerfile 都是参考典范：https://github.com/docker-library/docs "},"docs/gitbook/gitbook-operation-guide.html":{"url":"docs/gitbook/gitbook-operation-guide.html","title":"Gitbook搭建","keywords":"","body":"环境准备 1，配置 Node.js 环境 使用 Gitbook 需要配置 Node.js 环境，具体的安装步骤，可查看官方文档。 由于目前 Gitbook 项目已经停止维护，Node 过高可能出现不兼容问题，文档后面有常见报错处理方案 安装成功后，执行命令可查看 node 版本和 npm 版本。 # 查看npm版本 npm -v 9.6.5 # 查看node版本 node -v v18.14.2 2，安装 Gitbook 使用下面命令，安装 gitbook 包 npm install -g gitbook-cli 3，初始化项目 3.1，Gitbook 初始化 创建一个文件夹，并进入到该文件夹中，执行下面命令，初始化 gitbook 项目。 gitbook init 执行结果 info: create SUMMARY.md info: initialization is finished 可以看到创建了 SUMMARY.md 文档，这是电子书的目录文档。 然后创建一个 REAMDE.md 文档，用来对这个项目进行介绍。 3.2，npm 初始化 执行下面命令，初始化为 npm 项目。 npm init 命令会提示输入项目信息，可默认不填写，直接回车。 最后，会显示配置信息，输入yes回车即可初始化完毕。 初始化成功后，系统会自动在当前目录创建package.json文件，这是 npm 项目的配置文件。 3.3，章节配置 GitBook 使用文件 SUMMARY.md 来定义书本的章节和子章节的结构。文件 SUMMARY.md 被用来生成书本内容的预览表。 SUMMARY.md 的格式是一个简单的链接列表，链接的名字是章节的名字，链接的指向是章节文件的路径。 子章节被简单的定义为一个内嵌于父章节的列表。 # 概要 - [章节一](chapter1.md) - [章节二](chapter2.md) - [章节三](chapter3.md) # 概要 - [第一章](part1/README.md) - [1.1 第一节](part1/writing.md) - [1.2 第二节](part1/gitbook.md) - [第二章](part2/README.md) - [2.1 第一节](part2/feedback_please.md) - [2.2 第二节](part2/better_tools.md) 4，启动项目 在package.json文件的scripts中配置如下的脚本命令： \"scripts\": { \"serve\": \"gitbook serve\", \"build\": \"gitbook build\" } 分别是 gitbook 在本地启动的命令，和 gitbook 打包成 HTML 静态文件的命令。 对于本地演示，我们可以直接通过下面命令启动。 npm run serve 这条命令其实就是执行了package.json文件的scripts中的serve脚本，即gitbook serve。 启动成功后，就可以在浏览器输入http://localhost:4000/，如图所示。 5，忽略文件 任何在文件夹下的文件，在最后生成电子书时都会被拷贝到输出目录中，如果想要忽略某些文件，和 Git 一样， Gitbook 会依次读取 .gitignore, .bookignore 和 .ignore 文件来将一些文件和目录排除。 6，配置文件 Gitbook 在编译书籍的时候会读取书籍源码顶层目录中的 book.js 或者 book.json，这里以 book.json 为例，参考 gitbook 文档 可以知道，book.json 常用的配置如下。 { // 书籍信息 \"title\": \"学习杂技\", \"description\": \"笔记\", \"isbn\": \"图书编号\", \"author\": \"chen1900s\", \"lang\": \"zh-cn\", // 插件列表 \"plugins\": [], // 插件全局配置 \"pluginsConfig\": { \"fontSettings\": { \"theme\": \"sepia\", \"night\" or \"white\", \"family\": \"serif\" or \"sans\", \"size\": 1 to 4 } }, // 模板变量 \"variables\": { // 自定义 } } gitbook插件使用 Gitbook 最灵活的地方就是有很多插件可以使用，当然如果对插件不满意，也可以自己写插件。所有插件的命名都是以gitbook-plugin-xxx的形式。下面，我们就介绍一些常用的插件。 使用插件前，现在当前项目的根目录中创建一个book.js文件，这是 Gitbook 的配置文件，文件内容可以根据自己来定制，内容格式如上。 1，搜索插件 在命令行输入下面命令安装搜索插件。 npm install gitbook-plugin-search-pro 安装成功后，在book.js中添加插件的配置。 { plugins: ['search-pro']; } 2，代码框插件 在命令行输入下面命令安装代码插件。 npm install gitbook-plugin-code 安装成功后，在book.js中添加插件的配置。 { plugins: ['code']; } 3，自定义主题插件 在命令行输入下面命令安装自定义主题插件。 npm install gitbook-plugin-theme-主题名 安装成功后，在book.js中添加插件的配置。 { plugins: [\"theme-主题名\"] } 4，菜单折叠插件 在命令行输入下面命令安装菜单栏折叠插件。 npm install gitbook-plugin-expandable-chapters 安装成功后，在book.js中添加插件的配置。 { plugins: ['expandable-chapters']; } 5，返回顶部插件 在命令行输入下面命令安装返回顶部插件。 npm install gitbook-plugin-back-to-top-button 安装成功后，在book.js中添加插件的配置。 { plugins: ['back-to-top-button']; } 6，最终效果 下面我们来看看我的运行效果图，比刚开始美观多了。 更多插件可以从 https://plugins.gitbook.com/ 获取。 遇到的问题 1，TypeError [ERR_INVALID_ARG_TYPE]报错 gitbook init 报 TypeError [ERR_INVALID_ARG_TYPE]: The \"data\" argument must be of type string or an instance of Buffer, TypedArray, or DataView. Received an instance of Promise GitBook version: 3.2.3 npm 9.6.5 nodejs v18.14.2 解决方案 将 C:\\Users\\用户\\.gitbook\\versions\\3.2.3\\lib\\ init.js 中第71行附近的 return fs.writeFile(filePath, summary.toText(extension)); 修改为 return summary.toText(extension).then(stx=>{return fs.writeFile(filePath, stx);}) 2，cb.apply is not a function报错 2，高版本Node版本运行gitbook init报错 cb.apply is not a function C:\\Users\\ac_chenjw\\AppData\\Roaming\\npm\\node_modules\\gitbook-cli\\node_modules\\npm\\node_modules\\graceful-fs\\polyfills.js:287 if (cb) cb.apply(this, arguments) 原因是 npm版本的问题 导致 解决方案 查看报错的源码,在node_module/graceful-fs/polyfills.js的285行，对应函数是在 61-63行 ，注释掉就可以 文件路径 C:\\Users\\用户\\AppData\\Roaming\\npm\\node_modules\\gitbook-cli\\node_modules\\npm\\node_modules\\graceful-fs\\polyfills.js:287 fs.stat = statFix(fs.stat) fs.fstat = statFix(fs.fstat) fs.lstat = statFix(fs.lstat) "},"docs/appdeploy/Insatll-crictl-client.html":{"url":"docs/appdeploy/Insatll-crictl-client.html","title":"Crictl客户端安装","keywords":"","body":"安装 CRI 客户端 crictl # https://github.com/kubernetes-sigs/cri-tools/releases/ 选择版本 wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.2/crictl-v1.24.2-linux-amd64.tar.gz sudo tar crictl-v1.24.2-linux-amd64.tar.gz -C /usr/local/bin vi /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false # 验证是否可用 crictl pull centos:latest crictl image crictl rmi centos:latest containerd节点如何使用密码拉取镜像 crictl pull --creds USERNAME[:PASSWORD] 镜像地址 示例： [root@VM-155-12-tlinux ~]#crictl pull --creds 100006305462:chen188289 ccr.ccs.tencentyun.com/chenjingwei/goproxy:latest Image is up to date for sha256:ca30c529755f98c53f660f86fe42f1b38f19ca6127c57aa6025afaf9a016742a "},"docs/appdeploy/Install-harbor.html":{"url":"docs/appdeploy/Install-harbor.html","title":"Harbor镜像仓库部署","keywords":"","body":"一 什么是Harbor Harbor 是由 VMware 公司中国团队为企业用户设计的 Registry server 开源项目，包括了权限管理(RBAC)、LDAP、审计、管理界面、自我注册、HA 等企业必需的功能，同时针对中国用户的特点，设计镜像复制和中文支持等功能 官方文档：https://github.com/goharbor/harbor 部署参考文档：https://my.oschina.net/u/2277632/blog/3095815 二 部署安装 CentOS Linux release 7.8.2003 (Core) Docker version 19.03.13 docker-compose version 1.24.1 1 docker安装 下载地址：https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz 二进制安装，所有节点操作 1.1，下载并解压二进制包 wget https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz tar zxvf docker-19.03.9.tgz mv docker/* /usr/bin 1.2，systemd管理docker cat > /usr/lib/systemd/system/docker.service 1.3，创建配置文件 mkdir /etc/docker cat > /etc/docker/daemon.json 1.4，启动并设置开机启动 systemctl daemon-reload systemctl start docker systemctl enable docker systemctl status docker 2 docker-compose安装 官网文档介绍：https://docs.docker.com/compose/install/ 2.1，下载安装包 或者离线下载安装包我们可以从 Github 上下载它的二进制包来使用，最新发行的版本地址：https://github.com/docker/compose/releases 或通过命令行下载 sudo curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-(uname -s)−(uname -m)\" -o /usr/local/bin/docker-compose 2.2，安装docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 3 安装Harbor wget https://github.com/goharbor/harbor/releases/download/v2.1.4/harbor-offline-installer-v2.1.4.tgz tar xvf harbor-offline-installer-v2.1.4.tgz 根据需要修改相关参数 [root@chen harbor]# cp harbor.yml.tmpl harbor.yml #默认是harbor.yml.tmpl需要将这个文件重命名一下 [root@chen harbor]# vi harbor.yml [root@chen harbor]# ./install.sh 有如上提示表示安装成功 三 基本使用 然后我们访问一下这个地址，账号是admin123，密码就是配置文件里面那个harbor.yml文件里面 1 仓库管理 可以添加其他仓管，以腾讯云TCR镜像仓库为例 2 复制管理 复制管理 可以将本地镜像复制到其他镜像仓库，也可以将其他镜像仓库复制到本地 3 项目管理 4 仓库管理 "},"docs/appdeploy/Install-docker.html":{"url":"docs/appdeploy/Install-docker.html","title":"Docker安装和配置","keywords":"","body":"CentOS Docker 安装 1，使用 Docker 仓库进行安装 设置仓库 安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2 sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 阿里源： sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 清华源： sudo yum-config-manager \\ --add-repo \\ https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 安装 Docker Engine-Community： sudo yum install docker-ce docker-ce-cli containerd.io 2，安装docker 二进制安装 二级制包下载地址：https://download.docker.com/linux/static/stable/x86_64/ tar -zxvf docker-18.09.6.tgz mv docker/* /usr/bin/ vi /usr/lib/systemd/system/docker.service [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFLE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target 3，配置镜像加速源 vi /etc/docker/daemon.json { \"registry-mirrors\": [ \"https://mirror.ccs.tencentyun.com\" ] } 更多镜像加速源可以参考：https://www.runoob.com/docker/docker-mirror-acceleration.html 启动 systemctl start docker systemctl enable docker systemctl status docker "},"docs/appdeploy/Install-gitlab.html":{"url":"docs/appdeploy/Install-gitlab.html","title":"GitLab的安装及使用教程","keywords":"","body":"GitLab的安装及使用 安装 1、配置yum源 vim /etc/yum.repos.d/gitlab-ce.repo 复制以下内容： [root@chen ~]# cat /etc/yum.repos.d/gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7 repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key 2、更新本地yum缓存 sudo yum makecache [root@chen ~]# yum makecache yum install -y postfix systemctl enable postfix vim /etc/postfix/main.cf #删除 inet_interfaces = all 前的 #，在 inet_interfaces = localhost 前加上 systemctl start postfix 3、安装GitLab社区版 sudo yum install gitlab-ce #自动安装最新版 [root@chen ~]# sudo EXTERNAL_URL=\"实例公网 IP 地址\" yum install -y gitlab-ce 4，安装完 需要修改下配置文件，将指的域名替换成公网IP vim /etc/gitlab/gitlab.rb #将external_url 变量的地址修改为gitlab所在centos的ip地址。 external_url ‘http://git.home.com’ gitlab-ctl reconfigure //让配置生效，重新执行此命令时间也比较长 gitlab-ctl restart 5，获得用户数据，修改用户密码 [root@VM-3-9-tlinux /opt/gitlab/bin]# gitlab-rails console irb(main):007:0> User.where(id: 1).first => # irb(main):008:0> user = User.where(id: 1).first => # irb(main):009:0> user.password=12345678 => 12345678 irb(main):010:0> user.password_confirmation=12345678 => 12345678 irb(main):011:0> user.save! Enqueued ActionMailer::MailDeliveryJob (Job ID: 4977da90-a2bf-4687-b39b-bb65430f8530) to Sidekiq(mailers) with arguments: \"DeviseMailer\", \"password_change\", \"deliver_now\", {:args=>[#>]} => true irb(main):012:0> quit GitLab常用命令 udo gitlab-ctl start # 启动所有 gitlab 组件； sudo gitlab-ctl stop # 停止所有 gitlab 组件； sudo gitlab-ctl restart # 重启所有 gitlab 组件； sudo gitlab-ctl status # 查看服务状态； sudo gitlab-ctl reconfigure # 启动服务； sudo vim /etc/gitlab/gitlab.rb # 修改默认的配置文件； gitlab-rake gitlab:check SANITIZE=true --trace # 检查gitlab； sudo gitlab-ctl tail # 查看日志； GitLab使用 登录GitLab 1、在浏览器的地址栏中输入公网IP即可登录GitLab的界面，使用上面修改的的用户名和密码为 root 和 xxxxxxx 2、首次登录会强制用户修改密码。密码修改成功后，输入新密码进行登录。 创建Project 1，安装Git工具linux：安装Git，使用自带的源安装。或者Windows 安装git [root@VM-3-9-tlinux ~]# yum install git 2，生成密钥文件：使用ssh-keygen生成密钥文件.ssh/id_rsa.pub。 [root@VM-3-9-tlinux ~]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:8O/MfCKmOcz6SfDk8adEhgXfFQbXH44Rn4meKZs8i8Y root@VM-3-9-tlinux The key's randomart image is: +---[RSA 2048]----+ | . ..=+. | | o . + .+.o| | . o . .+=.| | = ..o..| | . + S . + | | = = .. + | | o+ + o= | | .++oEo + | | .o=+oo=+ | +----[SHA256]-----+ 3，在GitLab的主页中新建一个Project 4，添加ssh key导入步骤2中生成的密钥文件内容： 5， ssh key添加完成： 可以通过命令验证 ssh -T git@github.com 6，项目地址，该地址在进行clone操作时需要用到: 克隆项目 在已纳入管理的 PC 上执行以下命令，配置使用 Git 仓库的人员姓名。 git config --global user.name \"username\" 执行以下命令，配置使用 Git 仓库的人员邮箱。 git config --global user.email \"xxx@example.com\" 执行以下命令，克隆项目。其中“项目地址”请替换为项目地址。 git clone “项目地址” 克隆项目成功后，会在本地生成同名目录且包含项目中所有文件。 初始化本地项目 PS F:\\容器wiki> git init Reinitialized existing Git repository in F:/容器wiki/.git/ PS F:\\容器wiki> git remote add origin git@1.116.17.152:root/kubernetes.git PS F:\\容器wiki> git add . PS F:\\容器wiki> git commit -m \"first\" PS F:\\容器wiki> git push -u origin master 上传文件 执行以下命令，进入项目目录。 cd test/ 执行以下命令，创建需上传至 GitLab 的目标文件。本文以 test.sh 为例。 echo \"test\" > test.sh 执行以下命令，将 test.sh 文件加入索引中。 git add test.sh 执行以下命令，将 test.sh 提交至本地仓库。 git commit -m \"test.sh\" 执行以下命令，将 test.sh 同步至 GitLab 服务器。 git push -u origin master "},"docs/appdeploy/Install-helm.html":{"url":"docs/appdeploy/Install-helm.html","title":"Helm的安装和使用","keywords":"","body":"一 安装 Helm 客户端 Helm项目提供了两种获取和安装Helm的方式。这是官方提供的获取Helm发布版本的方法。另外， Helm社区提供了通过不同包管理器安装Helm的方法。这些方法可以在下面的官方方法之后看到。 用二进制版本安装 每个Helm 版本都提供了各种操作系统的二进制版本，这些版本可以手动下载和安装。 下载 需要的版本 解压(tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) 在解压目中找到helm程序，移动到需要的目录中(mv linux-amd64/helm /usr/local/bin/helm) 具体可以参考： https://helm.sh/zh/docs/intro/install/ 二 添加helm仓库 以腾讯云TCR镜像仓库为例 helm repo add $instance-$namespace https://$instance.tencentcloudcr.com/chartrepo/$namespace --username $username --password $instance-token #helm repo add tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm --username 10002438xxxx --password 密码xxxxxxxx $instance-$namespace：为 helm repo 名称，建议使用实例名称+命名空间名称组合的方式命名，以便于区分各个实例及命名空间。 https://$instance.tencentcloudcr.com/chartrepo/$namespace ：为 helm repo 的远端地址。 $username：已获取的用户名。 $instance-token：已获取的登录密码。 如添加成功将提示以下信息。 \"tcr-chen-helm\" has been added to your repositories 使用该命令可以查看当前的helm 仓库信息 # helm repo list NAME URL nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ tcr-chen-helm https://tcr-chen.tencentcloudcr.com/chartrepo/helm 三 推送 Helm Chart 安装 Helm Push 插件 注意： 请安装 0.9.0 及以上版本的 helm-push 插件，避免因版本不兼容等问题造成无法正常推送 helm chart。 使用 Helm CLI 上传 Chart 包需要安装 helm-push 插件，该插件支持使用helm push 指令推送 helm chart 至指定 repo，同时支持上传目录及压缩包。 helm plugin install https://github.com/chartmuseum/helm-push 在节点上执行以下命令，创建一个 Chart。 helm create tcr-chart-demo 执行以下命令，可直接推送指定目录至 Chart 仓库（可选）。 helm push tcr-chart-demo $instance-$namespace #helm cm-push tcr-chart-demo $instance-$namespace #高级版本使用的是cm-push命令 其中 $instance-$namespace 为已添加的本地仓库名称。 执行以下命令，可压缩指定目录，并推送至 Chart 仓库。 tar zcvf tcr-chart-demo-1.0.0.tgz tcr-chart-demo/ helm push tcr-chart-demo-1.0.0.tgz $instance-$namespace 其中$instance-$namespace为已添加的本地仓库名称。 四 拉取 Helm Chart 在节点上执行以下命令，获取最新的 Chart 信息。 helm repo update 执行以下命令，拉取指定版本 Helm Chart。 helm fetch / --version 以从企业版实例 tcr-demo 中拉取命名空间 project-a 内 tcr-chart-demo 1.0.0 版本为例： helm fetch tcr-chen-heml/tcr-chart-demo --version 1.0.0 五 Harbor 启用 helmchart 服务 1，安装 harbor 的 helmchart repository 默认新版 harbor 不会启用 chart repository service，如果需要管理 helm，我们需要在安装时添加额外的参数，例如：启用 chart repository service 服务的安装方式要添加一个参数 --with-chartmuseum [root@VM-55-9-tlinux ~/docker-compose/harbor]# ./install.sh --with-chartmuseum 安装完成后，会有这个提示 说明是安装成功： ⠿ Container chartmuseum Started 2，发布 helm charts 方式一、基于dashboard 的可视化上传 使用浏览器登录 harbor 后，在对应的管理界面操作即可，如下图： 方式二、基于命令行的 CLI 推送 更多时候基于第1种UI界面的上传并不能满足我们的实际需求，大部分情况我们都是要通过脚本发布helmchart 的。 1、安装插件 为了能使用命令推送，我们需要安装并使用 helm push 插件包，地址： https://github.com/chartmuseum/helm-push/releases a) 在线安装插件： helm plugin install https://github.com/chartmuseum/helm-pus b) 离线安装插件： 下载安装包 helm-push_0.10.1_linux_amd64.tar.gz，再使用命令 helm env 获取 HELM_PLUGINS 路径，然后放置和解压安装包，最后使用 helm plugin list 查看结果，如下： [root@VM-55-9-tlinux ~/harbor]# helm env | grep HELM_PLUGINS HELM_PLUGINS=\"/root/.local/share/helm/plugins [root@VM-55-9-tlinux ~/harbor]# mkdir -p /root/.local/share/helm/plugins/helm-push [root@VM-55-9-tlinux ~/harbor]# mv helm-push_0.10.1_linux_amd64.tar.gz /root/.local/share/helm/plugins/helm-push/ [root@VM-55-9-tlinux ~/harbor]# cd /root/.local/share/helm/plugins/helm-push/ [root@VM-55-9-tlinux helm-push]# tar -xzvf helm-push_0.10.1_linux_amd64.tar.gz [root@VM-55-9-tlinux helm-push]# helm plugin list NAME VERSION DESCRIPTION cm-push 0.10.1 Push chart package to ChartMuseum 2、添加 harbor 仓库到本地 helm 仓库列表 查看本地仓库列表(列出的是我已经添加其他仓库) [root@VM-55-9-tlinux ~/helm]# helm repo list NAME URL tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm # 添加仓库地址到本地列表(其中 myharbor-helm 为这个仓库地址在 helm 本地的名称，连接是仓库URL，后面是登录 harbor 的用户名和密码) # URL格式：http(s)://{harbor域名或iP:端口(如果默认443或80可不加)}/chartrepo/{yourHarborProjectName} [root@VM-55-9-tlinux ~/helm]# helm repo add myharbor-helm http://101.35.6.116:88/chartrepo/charts --username admin --password admin123 \"myharbor-helm\" has been added to your repositories # 再查看(发现已添加成功) [root@VM-55-9-tlinux ~/helm]# helm repo list NAME URL tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm myharbor-helm http://101.35.6.116:88/chartrepo/charts ##更新本地仓库缓存内容 [root@VM-55-9-tlinux ~/helm]# helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"myharbor-helm\" chart repository ...Successfully got an update from the \"tke-pass-helm\" chart repository Update Complete. ⎈Happy Helming!⎈ 注意点 1.harbor 仓库 URL 中的 chartrepo 是固定值。 2.在操作之前，请务必先在 harbor 中创建好项目，例如 charts即为先创建好的项目名称。 3.如果你还是搞不清这个URL，可以在harbor界面中上传一个外面下着的 chart 包，上次成功后进入这个 chart 详细页面，在 “概要这个Tab” 的最底部区域，harbor会告诉你在本地添加仓库的URL和命令。 4.推送 chart 以及 chart 的更多操作 推送 chart 示例 # 推送chart文件夹方式 helm push mychartdemo myharbor-helm # 推送chart压缩包方式 helm push mychartdemo-1.0.1.tgz myharbor-helm "},"docs/appdeploy/Install-jenkins.html":{"url":"docs/appdeploy/Install-jenkins.html","title":"Jenkins安装和配置","keywords":"","body":"Jenkins 安装和配置（CVM方式） 1，安装jdk以及配置环境变量 下载 JDK，输入如下命令： mkdir /usr/java # 创建 java 文件夹 cd /usr/java # 进入 java 文件夹 上传 JDK 安装包（推荐） rz jdk-8u151-linux-x64.tar.gz tar -xvf jdk-8u151-linux-x64.tar.gz #解压 设置环境变量 vi /etc/profile #添加如下环境变量================= #############2020-10-10 jdk env################ export JAVA_HOME=/usr/java/jdk1.8.0_151 export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib export PATH=$JAVA_HOME/bin:$PATH 加载环境变量 source /etc/profile 查看 JDK 是否安装成功 [root@VM-2-42-tlinux ~]# java -version java version \"1.8.0_151\" Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) 2，安装maven以及配置环境变量（可选） mkdir /usr/maven # 创建 maven 文件夹 cd /usr/maven # 进入 maven 文件夹 上传 maven安装包并解压 tar -xvf apache-maven-3.5.3-bin.tar.gz -C /usr/maven/ 设置环境变量 vi /etc/profile #############2021-10-10 maven env ################## export MAVEN_HOME=/usr/maven/apache-maven-3.5.3 export PATH=$MAVEN_HOME/bin:$PATH 检查maven是否安装成功 [root@chen ~]# mvn -v Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-25T03:49:05+08:00) Maven home: /usr/maven/apache-maven-3.5.3 Java version: 1.8.0_151, vendor: Oracle Corporation Java home: /usr/java/jdk1.8.0_151/jre Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"3.10.0-1127.13.1.el7.x86_64\", arch: \"amd64\", family: \"unix\" 3，安装tomcat 上传或者下载安装包 [root@VM-2-42-tlinux ~]# cd /opt/tomcat/ [root@VM-2-42-tlinux /opt/tomcat]# ls apache-tomcat-8.5.39.tar.gz #### # 镜像地址会改变，Tomcat 版本也会不断升级。如果下载链接失效，请您到 [Tomcat 官网](https://tomcat.apache.org/download-80.cgi)选择合适的安装包地址。 wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.39/bin/apache-tomcat-8.5.39.tar.gz tar -xzvf apache-tomcat-8.5.39.tar.gz 解压安装包 [root@VM-2-42-tlinux /opt/tomcat]# tar -xvf apache-tomcat-8.5.39.tar.gz 4，下载Jenkins安装的war包 Jenkins War Packages https://get.jenkins.io/war-stable/ # wget http://mirrors.jenkins.io/war-stable/2.107.1/jenkins.war mv jenkins.war /opt/tomcat/apache-tomcat-8.5.39/webapps/ 5，启动tomcat服务 进入 Tomcat 服务器的 bin 目录，然后执行./startup.sh命令启动 Tomcat 服务器。 cd /opt/tomcat/apache-tomcat-8.5.39/bin ./startup.sh 6，登陆 在浏览器地址栏中输入 http://公网IP:端口（端口为 server.xml 中设置的 connector port）进行访问，登陆服务控制台http://139.186.160.196:8080/jenkins #密码获取 [root@VM-2-42-tlinux ~]# cat /root/.jenkins/secrets/initialAdminPassword 用户名：admin 密码：admin123 "},"docs/appdeploy/Install-nfs.html":{"url":"docs/appdeploy/Install-nfs.html","title":"Linux服务之NFS服务器","keywords":"","body":"Linux服务之NFS服务器 1. NFS 协议 NFS服务工作在TCP的2049端口，UDP的2049端口。 NFS是Network File System的缩写，即网络文件系统，是一种使用于分散式文件系统的协定。功能是通过网络让不同的机器、不同的操作系统能够彼此分享个别的数据，让应用程序在客户端通过网络访问位于服务器磁盘中的数据，是在类Unix系统间实现磁盘文件共享的一种方法。 这个NFS服务器可以让你的PC来将网络远程的NFS服务器分享的目录，挂载到本地端的机器当中， 在本地端的机器看起来，那个远程主机的目录就好像是自己的一个磁盘分区槽一样。 1.1 工作原理 因为 NFS 支持的功能相当的多，而不同的功能都会使用不同的程序来启动， 每启动一个功能就会启用一些端口来传输数据，因此， NFS 的功能所对应的端口才没有固定住， 而是随机取用一些未被使用的小于 1024 的端口来作为传输之用。但如此一来又造成客户端想要连上服务器时的困扰， 因为客户端得要知道服务器端的相关端口才能够联机吧！ NFS在文件传送或信息传送过程中依赖于RPC协议。RPC，即远程过程调用的缩写，是能使客户端执行其他系统中程序的一种机制。RPC 最主要的功能就是在指定每个 NFS 功能所对应的端口号，并且回报给客户端，让客户端可以连结到正确的端口上去。 NFS本身是没有提供信息传输的协议和功能的，但NFS却能让我们通过网络进行资料的分享，这是因为NFS使用了一些其它的传输协议。而这些传输协议用到这个RPC功能的。可以说NFS本身就是使用RPC的一个程序，或者说NFS也是一个RPC SERVER。所以只要用到NFS的地方都要启动RPC服务，不论是NFS SERVER或者NFS CLIENT。这样SERVER和CLIENT才能通过RPC来实现PROGRAM PORT的对应。可以这么理解RPC和NFS的关系：NFS是一个文件系统，而RPC是负责负责信息的传输。 事实上，有很多这样的服务器都是向 RPC 注册的，举例来说，NIS (Network Information Service) 也是 RPC server 的一种。 Linux服务之NFS服务器 那RPC又是如何知道每个NFS的端口呢？ 这是因为当服务器在启动 NFS 时会随机取用数个端口，并主动的向 RPC 注册，因此 RPC 可以知道每个埠口对应的 NFS 功能，然后 RPC 又是固定使用 111 端口来监听客户端的需求并报客户端正确的埠口， 所以当然可以让 NFS 的启动更为轻松愉快了。 所以你要注意，要启动 NFS 之前，RPC 就要先启动了，否则 NFS 会无法向 RPC 注册。 另外，RPC 若重新启动时，原本注册的数据会不见，因此 RPC 重新启动后，它管理的所有服务都要重新启动来重新向 RPC 注册。 那客户端如何向NFS服务端交换数据数据呢？ (1) 客户端会向服务器端的 RPC 的111端口发出 NFS 档案存取功能的询问要求 (2) 服务器端找到对应的已注册的 NFS 守护进程端口后，会回报给客户端 (3) 客户端了解正确的端口后，就可以直接与 NFS 守护进程来联机 1.2 激活 NFS 服务 NFS 服务需要激活几个重要的 RPC 守护进程 工作流程 nfs—client => portmapper => mountd => nfs-server(nfsd) Linux服务之NFS服务器 (1) rpc.nfsd 这个守护进程主要的功能，则是在管理客户端是否能够登入主机的权限，其中还包含这个登入者的 ID 的判别。 (2) rpc.mountd 主要功能 这个守护进程主要的功能，则是在管理 NFS 的档案系统，用于给用户提供访问令牌。 访问的令牌，由本地的RPC提供随机端口。本地的RPC叫做portmapper，可以使用rpcinfo -P查看。 RPC的portmapper服务工作在1111端口。 请求过程 当客户端顺利的通过 rpc.nfsd 而登入主机之后，在它可以使用 NFS server 提供的档案之前，还会经过档案使用权限 的认证程序，就是那个-rwxrwxrwx、owner、group那几个权限啦。 然后它会去读 NFS 的设定档 /etc/exports 来比对客户端的权限，当通过这一关之后，客户端就可以取得使用 NFS 档案的权限啦。 注释：NFS需要有两个套件 nfs-utils NFS服务的主要套件 提供rpc.nfsd和rpc.mountd两个NFS守护进程和与其它相关文档与说明文件、执行档等的套件 portmap 主要负责RPC端口和守护进程的映射关系，即portmapper 在激活任何一个RPC server之前，我们都需要激活portmapper才行 1.3 各个版本之间的比较 NFS是一种网络文件系统，从1985年推出至今，共发布了3个版本：NFSv2、NFSv3、NFSv4，NFSv4包含两个次版本NFSv4.0和NFSv4.1。经过20多年发展，NFS发生了非常大的变化，最大的变化就是推动者从Sun变成了NetApp，NFSv2和NFSv3基本上是Sun起草的，NetApp从NFSv4.0参与进来，并且主导了NFSv4.1标准的制定过程，而Sun已经被Oracle收购了。 编号 协议版本 RFC 时间 页数 1 NFSv2 rfc1094 1989 年 3 月 27 页 2 NFSv3 rfc1813 1995 年 6 月 126 页 3 NFSv4.0 rfc3530 2003 年 4 月 275 页 4 NFSv4.1 rfc5661 2010 年 1 月 617 页 1. NFSv2 NFSv2是第一个以RFC形式发布的版本，实现了基本的功能。 2. NFSv3 协议特点 NFSv3修正了NFSv2的一些bug，两者有如下一些差别，但是感觉没有本质的差别。 区别差别 (1) NFSv2只支持同步写，如果客户端向服务器端写入数据，服务器必须将数据写入磁盘中才能发送应答消息。NFSv3支持异步写操作，服务器只需要将数据写入缓存中就可以发送应答信息了。 (2) NFSv3增加了ACCESS请求，ACCESS用来检查用户的访问权限。因为服务器端可能进行uid映射，因此客户端的uid和gid不能正确反映用户的访问权限。 (3) 一些请求调整了参数和返回信息，毕竟NFSv3和NFSv2发布的间隔有6年，经过长期运行可能觉得NFSv2某些请求参数和返回信息需要改进。 3. NFSv4.0 协议特点 相比NFSv3，NFSv4发生了比较大的变化，最大的变化是NFSv4有状态了。NFSv2和NFSv3都是无状态协议，服务区端不需要维护客户端的状态信息。 无状态协议的一个优点在于灾难恢复，当服务器出现问题后，客户端只需要重复发送失败请求就可以了，直到收到服务器的响应信息。 区别差别 (1) NFSv4增加了安全性，支持RPCSEC-GSS身份认证。 (2) NFSv4设计成了一种有状态的协议，自身实现了文件锁功能和获取文件系统根节点功能。 (3) NFSv4只提供了两个请求NULL和COMPOUND，所有的操作都整合进了COMPOUND中，客户端可以根据实际请求将多个操作封装到一个COMPOUND请求中，增加了灵活性。 (4) NFSv4文件系统的命令空间发生了变化，服务器端必须设置一个根文件系统(fsid=0)，其他文件系统挂载在根文件系统上导出。 (5) NFSv4支持delegation。由于多个客户端可以挂载同一个文件系统，为了保持文件同步，NFSv3中客户端需要经常向服务器发起请求，请求文件属性信息，判断其他客户端是否修改了文件。如果文件系统是只读的，或者客户端对文件的修改不频繁，频繁向服务器请求文件属性信息会降低系统性能。NFSv4可以依靠delegation实现文件同步。 (6) NFSv4修改了文件属性的表示方法。由于NFS是Sun开发的一套文件系统，设计之出NFS文件属性参考了UNIX中的文件属性，可能Windows中不具备某些属性，因此NFS对操作系统的兼容性不太好。 4. NFSv4.1 与NFSv4.0相比，NFSv4.1最大的变化是支持并行存储了。在以前的协议中，客户端直接与服务器连接，客户端直接将数据传输到服务器中。 当客户端数量较少时这种方式没有问题，但是如果大量的客户端要访问数据时，NFS服务器很快就会成为一个瓶颈，抑制了系统的性能。NFSv4.1支持并行存储，服务器由一台元数据服务器(MDS)和多台数据服务器(DS)构成，元数据服务器只管理文件在磁盘中的布局，数据传输在客户端和数据服务器之间直接进行。由于系统中包含多台数据服务器，因此数据可以以并行方式访问，系统吞吐量迅速提升。 2. NFS 服务搭建 CentOS7以NFSv4作为默认版本，NFSv4使用TCP协议（端口号是2049）和NFS服务器建立连接。 # 系统环境 系统平台：CentOS Linux release 7.9 (Final) NFS Server IP：192.168.0.17 防火墙已关闭/iptables: Firewall is not running. SELINUX=disabled 2.1 安装 NFS 服务 服务端 服务端，程序包名nfs-utils、rpcbind，默认都已经安装了 可以通过rpm -ql nfs-utils查看帮助文档等信息 客户端 客户端，需要安装程序包名nfs-utils，提供基本的客户端命令工具 [root@VM-0-17-tlinux ~/nfs]# yum install nfs-utils -y 查看NFS服务端口 NFS启动时会随机启动多个端口并向RPC注册，为了方便配置防火墙，需要固定NFS服务端口。 这样如果使用iptables对NFS端口进行限制就会有点麻烦，可以更改配置文件固定NFS服务相关端口 分配端口，编辑配置文件/etc/sysconfig/nfs # 使用rpcinfo -P会发现rpc启动了很多监听端口 [root@VM-0-17-tlinux ~/nfs]# rpcinfo -p localhost:q1 # 添加如下 [root@VM-0-17-tlinux ~/nfs]# vim /etc/sysconfig/nfs RQUOTAD_PORT=30001 LOCKD_TCPPORT=30002 LOCKD_UDPPORT=30002 MOUNTD_PORT=30003 STATD_PORT=30004 启动服务： [root@VM-0-17-tlinux ~/nfs]# systemctl start nfs.service 2.2 服务文件配置 相关文件和命令 文件名 说明 /etc/exports NFS 服务器端需要设定的内容，其作用是设定谁拥有什么样的权限去访问此机器的哪个目录 /var/lib/nfs/etab 无需设定，用于纪录 NFS 服务器完整的权限设定，exports 中没有设定的缺省值也会被列出 /var/lib/nfs/xtab 纪录 NFS 连接的相关信息 /usr/sbin/exportfs NFS 设定管理命令，用于 Server 侧设定，通过此条命令使得 exports 的设定变得有效或者无效 /usr/sbin/showmount 用于显示 NFS 设定的相关信息，Server 端和 Client 端均可 配置文件/etc/exports 我们可以按照“共享目录的路径 允许访问的 NFS 客户端(共享权限参数)”的格式，定义要共享的目录与相应的权限 常用参数 作用 ro 只读 rw 读写 sync 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据 root_squash 当 NFS 客户端以 root 管理员访问时，映射为 NFS 服务器的匿名用户 all_squash 无论 NFS 客户端使用什么账户访问，均映射为 NFS 服务器的匿名用户 no_root_squash 当 NFS 客户端以 root 管理员访问时，映射为 NFS 服务器的 root 管理员 secure 这个选项是缺省选项，它使用了 1024 以下的 TCP/IP 端口实现 NFS 的连接 insecure 禁止使用了 1024 以下的 TCP/IP 端口实现 NFS 的连接 no_wdelay 这个选项关闭写延时，如果设置了 async，那么 NFS 就会忽略这个选项 nohide 如果将一个目录挂载到另外一个目录之上，那么原来的目录通常就被隐藏起来或看起来像空的一样。要禁用这种行为，需启用 hide 选项 no_subtree_check 这个选项关闭子树检查，子树检查会执行一些不想忽略的安全性检查，缺省选项是启用子树检查 no_auth_nlm 这个选项也可以作为insecure_locks指定，它告诉 NFS 守护进程不要对加锁请求进行认证。如果关心安全性问题，就要避免使用这个选项。缺省选项是auth_nlm或secure_locks。 mp(mountpoint=path) 通过显式地声明这个选项，NFS 要求挂载所导出的目录 fsid=num 这个选项通常都在 NFS 故障恢复的情况中使用，如果希望实现 NFS 的故障恢复，请参考 NFS 文 [root@VM-0-17-tlinux ~/nfs]# cat /etc/exports /nfs 192.168.*.*(rw,sync,root_squash) #修改完配置后 重新启动nfs服务 root@VM-0-17-tlinux ~/nfs]# systemctl restart nfs [root@VM-0-17-tlinux ~/nfs]# systemctl status nfs 3. 实战演示 将NFS挂在到K8S容器服务的POD里面 案例一 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt name: nfs volumes: - name: nfs nfs: path: /nfs server: 192.168.0.17 [root@VM-0-17-tlinux ~/nfs]# cd /nfs/ [root@VM-0-17-tlinux /nfs]# echo hello worload > hello.txt [root@VM-0-17-tlinux /nfs]# cat hello.txt hello worload #登录容器查看挂着情况 [root@VM-0-17-tlinux /nfs]# kubectl exec -it centos-54db87ccc9-nkx86 -- /bin/bash [root@centos-54db87ccc9-nkx86 /]# [root@centos-54db87ccc9-nkx86 /]# df -h Filesystem Size Used Avail Use% Mounted on overlay 50G 11G 37G 23% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup 192.168.0.17:/nfs 50G 32G 16G 67% /mnt /dev/vda1 50G 11G 37G 23% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 12K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware [root@centos-54db87ccc9-nkx86 /]# cd /mnt/ [root@centos-54db87ccc9-nkx86 mnt]# cat hello.txt hello worload [root@centos-54db87ccc9-nkx86 mnt]# echo 1111>> hello.txt bash: hello.txt: Permission denied [root@centos-54db87ccc9-nkx86 mnt]ls -lrt total 4 -rw-r--r-- 1 root root 14 Oct 17 13:14 hello.txt 可以看到 上面我们的 /etc/exports /nfs 192.168.*.*(rw,sync,root_squash) #是没有权限修改文件 下面我们将配置文件修改成如下配置进行测试 [root@VM-0-17-tlinux /nfs]# cat /etc/exports /nfs 192.168.*.*(rw,sync,no_root_squash) #重启NFS 服务 [root@VM-0-17-tlinux /nfs]# systemctl restart nfs [root@VM-0-17-tlinux /nfs]# systemctl status nfs 将POD销毁重建后登录POD里测试。可以修改文件 并且可以创建文件 [root@centos-54db87ccc9-rgnk8 mnt]# touch 222 [root@centos-54db87ccc9-rgnk8 mnt]# ls 222 hello.txt [root@centos-54db87ccc9-rgnk8 mnt]# echo \"1111\">> hello.txt [root@centos-54db87ccc9-rgnk8 mnt]# cat hello.txt hello worloada 1111 [root@centos-54db87ccc9-rgnk8 mnt]# ls -lrt total 4 -rw-r--r-- 1 root root 0 Oct 17 13:24 222 -rw-r--r-- 1 root root 20 Oct 17 13:28 hello.txt 案例二 使用PVC和PV方式创建并挂载 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv-operationdata spec: accessModes: - ReadWriteMany capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 192.168.0.17 path: /nfs volumeHandle: cfs-pv2 persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-operationdata namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: \"\" volumeMode: Filesystem volumeName: pv-operationdata --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt name: nfs volumes: - name: nfs persistentVolumeClaim: claimName: pvc-operationdata 案例三 服务器端挂载 使用showmount命令查询NFS服务器的远程共享信息 showmount命令输出格式为“共享的目录名称 允许使用客户端地址” showmount命令 参数 作用 -e 显示 NFS 服务器的共享列表 -a 显示本机挂载的文件资源的情况 NFS 资源的情况 -v 显示版本号 exportfs命令 维护exports文件导出的文件系统表的专用工具，可以修改配置之后不重启NFS服务 export -ar：重新导出所有的文件系统 export -au：关闭导出的所有文件系统 export -u FS:：关闭指定的导出的文件系统 # 查看NFS服务器端共享的文件系统 # showmount -e NFSSERVER_IP [root@VM-0-17-tlinux ~/nfs]# showmount -e 192.168.0.17 Export list for 192.168.0.17: /nfs 192.168.*.* # NFS客户端创建一个挂载目录，挂载服务端NFS文件系统到本地 # mount -t nfs SERVER:/path/to/sharedfs /path/to/mount_point [root@VM-0-11-tlinux ~]# mkdir /nfsfile [root@VM-0-11-tlinux ~]# mount -t nfs 192.168.0.17:/nfs /nfsfile [root@VM-0-11-tlinux ~]# df -h | grep nfsfile 192.168.0.17:/nfs 50G 32G 16G 67% /nfsfile # 挂载成功后就应该能够顺利地看到在执行前面的操作时写入的文件内容了 [root@VM-0-11-tlinux ~]# cat /nfsfile/hello.txt hello worloada 1111 # 如果希望NFS文件共享服务能一直有效，则需要将其写入到fstab文件中 # SERVER:/PATH/TO/EXPORTED_FS /mount_point nfs defaults,_netdev 0 0 [root@VM-0-11-tlinux ~]# vim /etc/fstab 192.168.0.17:/nfs /nfsfile nfs defaults 0 0 参考文档：https://www.escapelife.site/posts/c49dfbab.html apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: mysql qcloud-app: mysql name: mysql spec: podManagementPolicy: OrderedReady replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: mysql qcloud-app: mysql serviceName: \"\" template: metadata: labels: k8s-app: mysql qcloud-app: mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"123456\" image: mysql:5.7 imagePullPolicy: IfNotPresent name: mysql volumeMounts: - mountPath: /var/lib/mysql name: nfs subPath: mysql_docker/data dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - name: nfs nfs: path: / server: 172.16.3.7 "}}