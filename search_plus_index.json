{"./":{"url":"./","title":"Introduction","keywords":"","body":"关于本书 本书为电子书形式，内容为本人多年的 Kubernetes运维过程中实战经验进行系统性记录 ，以备快速查找，仅记录 在实践中学习，在学习中实践 "},"docs/gitbook/gitbook-operation-guide.html":{"url":"docs/gitbook/gitbook-operation-guide.html","title":"Gitbook搭建","keywords":"","body":"环境准备 1，配置 Node.js 环境 使用 Gitbook 需要配置 Node.js 环境，具体的安装步骤，可查看官方文档。 由于目前 Gitbook 项目已经停止维护，Node 过高可能出现不兼容问题，文档后面有常见报错处理方案 安装成功后，执行命令可查看 node 版本和 npm 版本。 # 查看npm版本 npm -v 9.6.5 # 查看node版本 node -v v18.14.2 2，安装 Gitbook 使用下面命令，安装 gitbook 包 npm install -g gitbook-cli 3，初始化项目 3.1，Gitbook 初始化 创建一个文件夹，并进入到该文件夹中，执行下面命令，初始化 gitbook 项目。 gitbook init 执行结果 info: create SUMMARY.md info: initialization is finished 可以看到创建了 SUMMARY.md 文档，这是电子书的目录文档。 然后创建一个 REAMDE.md 文档，用来对这个项目进行介绍。 3.2，npm 初始化 执行下面命令，初始化为 npm 项目。 npm init 命令会提示输入项目信息，可默认不填写，直接回车。 最后，会显示配置信息，输入yes回车即可初始化完毕。 初始化成功后，系统会自动在当前目录创建package.json文件，这是 npm 项目的配置文件。 3.3，章节配置 GitBook 使用文件 SUMMARY.md 来定义书本的章节和子章节的结构。文件 SUMMARY.md 被用来生成书本内容的预览表。 SUMMARY.md 的格式是一个简单的链接列表，链接的名字是章节的名字，链接的指向是章节文件的路径。 子章节被简单的定义为一个内嵌于父章节的列表。 # 概要 - [章节一](chapter1.md) - [章节二](chapter2.md) - [章节三](chapter3.md) # 概要 - [第一章](part1/README.md) - [1.1 第一节](part1/writing.md) - [1.2 第二节](part1/gitbook.md) - [第二章](part2/README.md) - [2.1 第一节](part2/feedback_please.md) - [2.2 第二节](part2/better_tools.md) 4，启动项目 在package.json文件的scripts中配置如下的脚本命令： \"scripts\": { \"serve\": \"gitbook serve\", \"build\": \"gitbook build\" } 分别是 gitbook 在本地启动的命令，和 gitbook 打包成 HTML 静态文件的命令。 对于本地演示，我们可以直接通过下面命令启动。 npm run serve 这条命令其实就是执行了package.json文件的scripts中的serve脚本，即gitbook serve。 启动成功后，就可以在浏览器输入http://localhost:4000/，如图所示。 5，忽略文件 任何在文件夹下的文件，在最后生成电子书时都会被拷贝到输出目录中，如果想要忽略某些文件，和 Git 一样， Gitbook 会依次读取 .gitignore, .bookignore 和 .ignore 文件来将一些文件和目录排除。 6，配置文件 Gitbook 在编译书籍的时候会读取书籍源码顶层目录中的 book.js 或者 book.json，这里以 book.json 为例，参考 gitbook 文档 可以知道，book.json 常用的配置如下。 { // 书籍信息 \"title\": \"学习杂技\", \"description\": \"笔记\", \"isbn\": \"图书编号\", \"author\": \"chen1900s\", \"lang\": \"zh-cn\", // 插件列表 \"plugins\": [], // 插件全局配置 \"pluginsConfig\": { \"fontSettings\": { \"theme\": \"sepia\", \"night\" or \"white\", \"family\": \"serif\" or \"sans\", \"size\": 1 to 4 } }, // 模板变量 \"variables\": { // 自定义 } } gitbook插件使用 Gitbook 最灵活的地方就是有很多插件可以使用，当然如果对插件不满意，也可以自己写插件。所有插件的命名都是以gitbook-plugin-xxx的形式。下面，我们就介绍一些常用的插件。 使用插件前，现在当前项目的根目录中创建一个book.js文件，这是 Gitbook 的配置文件，文件内容可以根据自己来定制，内容格式如上。 1，搜索插件 在命令行输入下面命令安装搜索插件。 npm install gitbook-plugin-search-pro 安装成功后，在book.js中添加插件的配置。 { plugins: ['search-pro']; } 2，代码框插件 在命令行输入下面命令安装代码插件。 npm install gitbook-plugin-code 安装成功后，在book.js中添加插件的配置。 { plugins: ['code']; } 3，自定义主题插件 在命令行输入下面命令安装自定义主题插件。 npm install gitbook-plugin-theme-主题名 安装成功后，在book.js中添加插件的配置。 { plugins: [\"theme-主题名\"] } 4，菜单折叠插件 在命令行输入下面命令安装菜单栏折叠插件。 npm install gitbook-plugin-expandable-chapters 安装成功后，在book.js中添加插件的配置。 { plugins: ['expandable-chapters']; } 5，返回顶部插件 在命令行输入下面命令安装返回顶部插件。 npm install gitbook-plugin-back-to-top-button 安装成功后，在book.js中添加插件的配置。 { plugins: ['back-to-top-button']; } 6，最终效果 下面我们来看看我的运行效果图，比刚开始美观多了。 更多插件可以从 https://plugins.gitbook.com/ 获取。 遇到的问题 1，TypeError [ERR_INVALID_ARG_TYPE]报错 gitbook init 报 TypeError [ERR_INVALID_ARG_TYPE]: The \"data\" argument must be of type string or an instance of Buffer, TypedArray, or DataView. Received an instance of Promise GitBook version: 3.2.3 npm 9.6.5 nodejs v18.14.2 解决方案 将 C:\\Users\\用户\\.gitbook\\versions\\3.2.3\\lib\\ init.js 中第71行附近的 return fs.writeFile(filePath, summary.toText(extension)); 修改为 return summary.toText(extension).then(stx=>{return fs.writeFile(filePath, stx);}) 2，cb.apply is not a function报错 2，高版本Node版本运行gitbook init报错 cb.apply is not a function C:\\Users\\ac_chenjw\\AppData\\Roaming\\npm\\node_modules\\gitbook-cli\\node_modules\\npm\\node_modules\\graceful-fs\\polyfills.js:287 if (cb) cb.apply(this, arguments) 原因是 npm版本的问题 导致 解决方案 查看报错的源码,在node_module/graceful-fs/polyfills.js的285行，对应函数是在 61-63行 ，注释掉就可以 文件路径 C:\\Users\\用户\\AppData\\Roaming\\npm\\node_modules\\gitbook-cli\\node_modules\\npm\\node_modules\\graceful-fs\\polyfills.js:287 fs.stat = statFix(fs.stat) fs.fstat = statFix(fs.fstat) fs.lstat = statFix(fs.lstat) "},"docs/appdeploy/Insatll-crictl-client.html":{"url":"docs/appdeploy/Insatll-crictl-client.html","title":"Crictl客户端安装","keywords":"","body":"安装 CRI 客户端 crictl # https://github.com/kubernetes-sigs/cri-tools/releases/ 选择版本 wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.2/crictl-v1.24.2-linux-amd64.tar.gz sudo tar crictl-v1.24.2-linux-amd64.tar.gz -C /usr/local/bin vi /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false # 验证是否可用 crictl pull centos:latest crictl image crictl rmi centos:latest containerd节点如何使用密码拉取镜像 crictl pull --creds USERNAME[:PASSWORD] 镜像地址 示例： [root@VM-155-12-tlinux ~]#crictl pull --creds 100006305462:chen188289 ccr.ccs.tencentyun.com/chenjingwei/goproxy:latest Image is up to date for sha256:ca30c529755f98c53f660f86fe42f1b38f19ca6127c57aa6025afaf9a016742a "},"docs/appdeploy/Install-harbor.html":{"url":"docs/appdeploy/Install-harbor.html","title":"Harbor镜像仓库部署","keywords":"","body":"一 什么是Harbor Harbor 是由 VMware 公司中国团队为企业用户设计的 Registry server 开源项目，包括了权限管理(RBAC)、LDAP、审计、管理界面、自我注册、HA 等企业必需的功能，同时针对中国用户的特点，设计镜像复制和中文支持等功能 官方文档：https://github.com/goharbor/harbor 部署参考文档：https://my.oschina.net/u/2277632/blog/3095815 二 部署安装 CentOS Linux release 7.8.2003 (Core) Docker version 19.03.13 docker-compose version 1.24.1 1 docker安装 下载地址：https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz 二进制安装，所有节点操作 1.1，下载并解压二进制包 wget https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz tar zxvf docker-19.03.9.tgz mv docker/* /usr/bin 1.2，systemd管理docker cat > /usr/lib/systemd/system/docker.service 1.3，创建配置文件 mkdir /etc/docker cat > /etc/docker/daemon.json 1.4，启动并设置开机启动 systemctl daemon-reload systemctl start docker systemctl enable docker systemctl status docker 2 docker-compose安装 官网文档介绍：https://docs.docker.com/compose/install/ 2.1，下载安装包 或者离线下载安装包我们可以从 Github 上下载它的二进制包来使用，最新发行的版本地址：https://github.com/docker/compose/releases 或通过命令行下载 sudo curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-(uname -s)−(uname -m)\" -o /usr/local/bin/docker-compose 2.2，安装docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 3 安装Harbor wget https://github.com/goharbor/harbor/releases/download/v2.1.4/harbor-offline-installer-v2.1.4.tgz tar xvf harbor-offline-installer-v2.1.4.tgz 根据需要修改相关参数 [root@chen harbor]# cp harbor.yml.tmpl harbor.yml #默认是harbor.yml.tmpl需要将这个文件重命名一下 [root@chen harbor]# vi harbor.yml [root@chen harbor]# ./install.sh 有如上提示表示安装成功 三 基本使用 然后我们访问一下这个地址，账号是admin123，密码就是配置文件里面那个harbor.yml文件里面 1 仓库管理 可以添加其他仓管，以腾讯云TCR镜像仓库为例 2 复制管理 复制管理 可以将本地镜像复制到其他镜像仓库，也可以将其他镜像仓库复制到本地 3 项目管理 4 仓库管理 "},"docs/appdeploy/Install-docker.html":{"url":"docs/appdeploy/Install-docker.html","title":"Docker安装和配置","keywords":"","body":"CentOS Docker 安装 1，使用 Docker 仓库进行安装 设置仓库 安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2 sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 阿里源： sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 清华源： sudo yum-config-manager \\ --add-repo \\ https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 安装 Docker Engine-Community： sudo yum install docker-ce docker-ce-cli containerd.io 2，安装docker 二进制安装 二级制包下载地址：https://download.docker.com/linux/static/stable/x86_64/ tar -zxvf docker-18.09.6.tgz mv docker/* /usr/bin/ vi /usr/lib/systemd/system/docker.service [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFLE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target 3，配置镜像加速源 vi /etc/docker/daemon.json { \"registry-mirrors\": [ \"https://mirror.ccs.tencentyun.com\" ] } 更多镜像加速源可以参考：https://www.runoob.com/docker/docker-mirror-acceleration.html 启动 systemctl start docker systemctl enable docker systemctl status docker "},"docs/appdeploy/Install-gitlab.html":{"url":"docs/appdeploy/Install-gitlab.html","title":"GitLab的安装及使用教程","keywords":"","body":"GitLab的安装及使用 安装 1、配置yum源 vim /etc/yum.repos.d/gitlab-ce.repo 复制以下内容： [root@chen ~]# cat /etc/yum.repos.d/gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7 repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key 2、更新本地yum缓存 sudo yum makecache [root@chen ~]# yum makecache yum install -y postfix systemctl enable postfix vim /etc/postfix/main.cf #删除 inet_interfaces = all 前的 #，在 inet_interfaces = localhost 前加上 systemctl start postfix 3、安装GitLab社区版 sudo yum install gitlab-ce #自动安装最新版 [root@chen ~]# sudo EXTERNAL_URL=\"实例公网 IP 地址\" yum install -y gitlab-ce 4，安装完 需要修改下配置文件，将指的域名替换成公网IP vim /etc/gitlab/gitlab.rb #将external_url 变量的地址修改为gitlab所在centos的ip地址。 external_url ‘http://git.home.com’ gitlab-ctl reconfigure //让配置生效，重新执行此命令时间也比较长 gitlab-ctl restart 5，获得用户数据，修改用户密码 [root@VM-3-9-tlinux /opt/gitlab/bin]# gitlab-rails console irb(main):007:0> User.where(id: 1).first => # irb(main):008:0> user = User.where(id: 1).first => # irb(main):009:0> user.password=12345678 => 12345678 irb(main):010:0> user.password_confirmation=12345678 => 12345678 irb(main):011:0> user.save! Enqueued ActionMailer::MailDeliveryJob (Job ID: 4977da90-a2bf-4687-b39b-bb65430f8530) to Sidekiq(mailers) with arguments: \"DeviseMailer\", \"password_change\", \"deliver_now\", {:args=>[#>]} => true irb(main):012:0> quit GitLab常用命令 udo gitlab-ctl start # 启动所有 gitlab 组件； sudo gitlab-ctl stop # 停止所有 gitlab 组件； sudo gitlab-ctl restart # 重启所有 gitlab 组件； sudo gitlab-ctl status # 查看服务状态； sudo gitlab-ctl reconfigure # 启动服务； sudo vim /etc/gitlab/gitlab.rb # 修改默认的配置文件； gitlab-rake gitlab:check SANITIZE=true --trace # 检查gitlab； sudo gitlab-ctl tail # 查看日志； GitLab使用 登录GitLab 1、在浏览器的地址栏中输入公网IP即可登录GitLab的界面，使用上面修改的的用户名和密码为 root 和 xxxxxxx 2、首次登录会强制用户修改密码。密码修改成功后，输入新密码进行登录。 创建Project 1，安装Git工具linux：安装Git，使用自带的源安装。或者Windows 安装git [root@VM-3-9-tlinux ~]# yum install git 2，生成密钥文件：使用ssh-keygen生成密钥文件.ssh/id_rsa.pub。 [root@VM-3-9-tlinux ~]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:8O/MfCKmOcz6SfDk8adEhgXfFQbXH44Rn4meKZs8i8Y root@VM-3-9-tlinux The key's randomart image is: +---[RSA 2048]----+ | . ..=+. | | o . + .+.o| | . o . .+=.| | = ..o..| | . + S . + | | = = .. + | | o+ + o= | | .++oEo + | | .o=+oo=+ | +----[SHA256]-----+ 3，在GitLab的主页中新建一个Project 4，添加ssh key导入步骤2中生成的密钥文件内容： 5， ssh key添加完成： 可以通过命令验证 ssh -T git@github.com 6，项目地址，该地址在进行clone操作时需要用到: 克隆项目 在已纳入管理的 PC 上执行以下命令，配置使用 Git 仓库的人员姓名。 git config --global user.name \"username\" 执行以下命令，配置使用 Git 仓库的人员邮箱。 git config --global user.email \"xxx@example.com\" 执行以下命令，克隆项目。其中“项目地址”请替换为项目地址。 git clone “项目地址” 克隆项目成功后，会在本地生成同名目录且包含项目中所有文件。 初始化本地项目 PS F:\\容器wiki> git init Reinitialized existing Git repository in F:/容器wiki/.git/ PS F:\\容器wiki> git remote add origin git@1.116.17.152:root/kubernetes.git PS F:\\容器wiki> git add . PS F:\\容器wiki> git commit -m \"first\" PS F:\\容器wiki> git push -u origin master 上传文件 执行以下命令，进入项目目录。 cd test/ 执行以下命令，创建需上传至 GitLab 的目标文件。本文以 test.sh 为例。 echo \"test\" > test.sh 执行以下命令，将 test.sh 文件加入索引中。 git add test.sh 执行以下命令，将 test.sh 提交至本地仓库。 git commit -m \"test.sh\" 执行以下命令，将 test.sh 同步至 GitLab 服务器。 git push -u origin master "},"docs/appdeploy/Install-helm.html":{"url":"docs/appdeploy/Install-helm.html","title":"Helm的安装和使用","keywords":"","body":"一 安装 Helm 客户端 Helm项目提供了两种获取和安装Helm的方式。这是官方提供的获取Helm发布版本的方法。另外， Helm社区提供了通过不同包管理器安装Helm的方法。这些方法可以在下面的官方方法之后看到。 用二进制版本安装 每个Helm 版本都提供了各种操作系统的二进制版本，这些版本可以手动下载和安装。 下载 需要的版本 解压(tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) 在解压目中找到helm程序，移动到需要的目录中(mv linux-amd64/helm /usr/local/bin/helm) 具体可以参考： https://helm.sh/zh/docs/intro/install/ 二 添加helm仓库 以腾讯云TCR镜像仓库为例 helm repo add $instance-$namespace https://$instance.tencentcloudcr.com/chartrepo/$namespace --username $username --password $instance-token #helm repo add tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm --username 10002438xxxx --password 密码xxxxxxxx $instance-$namespace：为 helm repo 名称，建议使用实例名称+命名空间名称组合的方式命名，以便于区分各个实例及命名空间。 https://$instance.tencentcloudcr.com/chartrepo/$namespace ：为 helm repo 的远端地址。 $username：已获取的用户名。 $instance-token：已获取的登录密码。 如添加成功将提示以下信息。 \"tcr-chen-helm\" has been added to your repositories 使用该命令可以查看当前的helm 仓库信息 # helm repo list NAME URL nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ tcr-chen-helm https://tcr-chen.tencentcloudcr.com/chartrepo/helm 三 推送 Helm Chart 安装 Helm Push 插件 注意： 请安装 0.9.0 及以上版本的 helm-push 插件，避免因版本不兼容等问题造成无法正常推送 helm chart。 使用 Helm CLI 上传 Chart 包需要安装 helm-push 插件，该插件支持使用helm push 指令推送 helm chart 至指定 repo，同时支持上传目录及压缩包。 helm plugin install https://github.com/chartmuseum/helm-push 在节点上执行以下命令，创建一个 Chart。 helm create tcr-chart-demo 执行以下命令，可直接推送指定目录至 Chart 仓库（可选）。 helm push tcr-chart-demo $instance-$namespace #helm cm-push tcr-chart-demo $instance-$namespace #高级版本使用的是cm-push命令 其中 $instance-$namespace 为已添加的本地仓库名称。 执行以下命令，可压缩指定目录，并推送至 Chart 仓库。 tar zcvf tcr-chart-demo-1.0.0.tgz tcr-chart-demo/ helm push tcr-chart-demo-1.0.0.tgz $instance-$namespace 其中$instance-$namespace为已添加的本地仓库名称。 四 拉取 Helm Chart 在节点上执行以下命令，获取最新的 Chart 信息。 helm repo update 执行以下命令，拉取指定版本 Helm Chart。 helm fetch / --version 以从企业版实例 tcr-demo 中拉取命名空间 project-a 内 tcr-chart-demo 1.0.0 版本为例： helm fetch tcr-chen-heml/tcr-chart-demo --version 1.0.0 五 Harbor 启用 helmchart 服务 1，安装 harbor 的 helmchart repository 默认新版 harbor 不会启用 chart repository service，如果需要管理 helm，我们需要在安装时添加额外的参数，例如：启用 chart repository service 服务的安装方式要添加一个参数 --with-chartmuseum [root@VM-55-9-tlinux ~/docker-compose/harbor]# ./install.sh --with-chartmuseum 安装完成后，会有这个提示 说明是安装成功： ⠿ Container chartmuseum Started 2，发布 helm charts 方式一、基于dashboard 的可视化上传 使用浏览器登录 harbor 后，在对应的管理界面操作即可，如下图： 方式二、基于命令行的 CLI 推送 更多时候基于第1种UI界面的上传并不能满足我们的实际需求，大部分情况我们都是要通过脚本发布helmchart 的。 1、安装插件 为了能使用命令推送，我们需要安装并使用 helm push 插件包，地址： https://github.com/chartmuseum/helm-push/releases a) 在线安装插件： helm plugin install https://github.com/chartmuseum/helm-pus b) 离线安装插件： 下载安装包 helm-push_0.10.1_linux_amd64.tar.gz，再使用命令 helm env 获取 HELM_PLUGINS 路径，然后放置和解压安装包，最后使用 helm plugin list 查看结果，如下： [root@VM-55-9-tlinux ~/harbor]# helm env | grep HELM_PLUGINS HELM_PLUGINS=\"/root/.local/share/helm/plugins [root@VM-55-9-tlinux ~/harbor]# mkdir -p /root/.local/share/helm/plugins/helm-push [root@VM-55-9-tlinux ~/harbor]# mv helm-push_0.10.1_linux_amd64.tar.gz /root/.local/share/helm/plugins/helm-push/ [root@VM-55-9-tlinux ~/harbor]# cd /root/.local/share/helm/plugins/helm-push/ [root@VM-55-9-tlinux helm-push]# tar -xzvf helm-push_0.10.1_linux_amd64.tar.gz [root@VM-55-9-tlinux helm-push]# helm plugin list NAME VERSION DESCRIPTION cm-push 0.10.1 Push chart package to ChartMuseum 2、添加 harbor 仓库到本地 helm 仓库列表 查看本地仓库列表(列出的是我已经添加其他仓库) [root@VM-55-9-tlinux ~/helm]# helm repo list NAME URL tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm # 添加仓库地址到本地列表(其中 myharbor-helm 为这个仓库地址在 helm 本地的名称，连接是仓库URL，后面是登录 harbor 的用户名和密码) # URL格式：http(s)://{harbor域名或iP:端口(如果默认443或80可不加)}/chartrepo/{yourHarborProjectName} [root@VM-55-9-tlinux ~/helm]# helm repo add myharbor-helm http://101.35.6.116:88/chartrepo/charts --username admin --password admin123 \"myharbor-helm\" has been added to your repositories # 再查看(发现已添加成功) [root@VM-55-9-tlinux ~/helm]# helm repo list NAME URL tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm myharbor-helm http://101.35.6.116:88/chartrepo/charts ##更新本地仓库缓存内容 [root@VM-55-9-tlinux ~/helm]# helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"myharbor-helm\" chart repository ...Successfully got an update from the \"tke-pass-helm\" chart repository Update Complete. ⎈Happy Helming!⎈ 注意点 1.harbor 仓库 URL 中的 chartrepo 是固定值。 2.在操作之前，请务必先在 harbor 中创建好项目，例如 charts即为先创建好的项目名称。 3.如果你还是搞不清这个URL，可以在harbor界面中上传一个外面下着的 chart 包，上次成功后进入这个 chart 详细页面，在 “概要这个Tab” 的最底部区域，harbor会告诉你在本地添加仓库的URL和命令。 4.推送 chart 以及 chart 的更多操作 推送 chart 示例 # 推送chart文件夹方式 helm push mychartdemo myharbor-helm # 推送chart压缩包方式 helm push mychartdemo-1.0.1.tgz myharbor-helm "},"docs/appdeploy/Install-jenkins.html":{"url":"docs/appdeploy/Install-jenkins.html","title":"Jenkins安装和配置","keywords":"","body":"Jenkins 安装和配置（CVM方式） 1，安装jdk以及配置环境变量 下载 JDK，输入如下命令： mkdir /usr/java # 创建 java 文件夹 cd /usr/java # 进入 java 文件夹 上传 JDK 安装包（推荐） rz jdk-8u151-linux-x64.tar.gz tar -xvf jdk-8u151-linux-x64.tar.gz #解压 设置环境变量 vi /etc/profile #添加如下环境变量================= #############2020-10-10 jdk env################ export JAVA_HOME=/usr/java/jdk1.8.0_151 export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib export PATH=$JAVA_HOME/bin:$PATH 加载环境变量 source /etc/profile 查看 JDK 是否安装成功 [root@VM-2-42-tlinux ~]# java -version java version \"1.8.0_151\" Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) 2，安装maven以及配置环境变量（可选） mkdir /usr/maven # 创建 maven 文件夹 cd /usr/maven # 进入 maven 文件夹 上传 maven安装包并解压 tar -xvf apache-maven-3.5.3-bin.tar.gz -C /usr/maven/ 设置环境变量 vi /etc/profile #############2021-10-10 maven env ################## export MAVEN_HOME=/usr/maven/apache-maven-3.5.3 export PATH=$MAVEN_HOME/bin:$PATH 检查maven是否安装成功 [root@chen ~]# mvn -v Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-25T03:49:05+08:00) Maven home: /usr/maven/apache-maven-3.5.3 Java version: 1.8.0_151, vendor: Oracle Corporation Java home: /usr/java/jdk1.8.0_151/jre Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"3.10.0-1127.13.1.el7.x86_64\", arch: \"amd64\", family: \"unix\" 3，安装tomcat 上传或者下载安装包 [root@VM-2-42-tlinux ~]# cd /opt/tomcat/ [root@VM-2-42-tlinux /opt/tomcat]# ls apache-tomcat-8.5.39.tar.gz #### # 镜像地址会改变，Tomcat 版本也会不断升级。如果下载链接失效，请您到 [Tomcat 官网](https://tomcat.apache.org/download-80.cgi)选择合适的安装包地址。 wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.39/bin/apache-tomcat-8.5.39.tar.gz tar -xzvf apache-tomcat-8.5.39.tar.gz 解压安装包 [root@VM-2-42-tlinux /opt/tomcat]# tar -xvf apache-tomcat-8.5.39.tar.gz 4，下载Jenkins安装的war包 Jenkins War Packages https://get.jenkins.io/war-stable/ # wget http://mirrors.jenkins.io/war-stable/2.107.1/jenkins.war mv jenkins.war /opt/tomcat/apache-tomcat-8.5.39/webapps/ 5，启动tomcat服务 进入 Tomcat 服务器的 bin 目录，然后执行./startup.sh命令启动 Tomcat 服务器。 cd /opt/tomcat/apache-tomcat-8.5.39/bin ./startup.sh 6，登陆 在浏览器地址栏中输入 http://公网IP:端口（端口为 server.xml 中设置的 connector port）进行访问，登陆服务控制台http://139.186.160.196:8080/jenkins #密码获取 [root@VM-2-42-tlinux ~]# cat /root/.jenkins/secrets/initialAdminPassword 用户名：admin 密码：admin123 "},"docs/appdeploy/Install-nfs.html":{"url":"docs/appdeploy/Install-nfs.html","title":"Linux服务之NFS服务器","keywords":"","body":"Linux服务之NFS服务器 1. NFS 协议 NFS服务工作在TCP的2049端口，UDP的2049端口。 NFS是Network File System的缩写，即网络文件系统，是一种使用于分散式文件系统的协定。功能是通过网络让不同的机器、不同的操作系统能够彼此分享个别的数据，让应用程序在客户端通过网络访问位于服务器磁盘中的数据，是在类Unix系统间实现磁盘文件共享的一种方法。 这个NFS服务器可以让你的PC来将网络远程的NFS服务器分享的目录，挂载到本地端的机器当中， 在本地端的机器看起来，那个远程主机的目录就好像是自己的一个磁盘分区槽一样。 1.1 工作原理 因为 NFS 支持的功能相当的多，而不同的功能都会使用不同的程序来启动， 每启动一个功能就会启用一些端口来传输数据，因此， NFS 的功能所对应的端口才没有固定住， 而是随机取用一些未被使用的小于 1024 的端口来作为传输之用。但如此一来又造成客户端想要连上服务器时的困扰， 因为客户端得要知道服务器端的相关端口才能够联机吧！ NFS在文件传送或信息传送过程中依赖于RPC协议。RPC，即远程过程调用的缩写，是能使客户端执行其他系统中程序的一种机制。RPC 最主要的功能就是在指定每个 NFS 功能所对应的端口号，并且回报给客户端，让客户端可以连结到正确的端口上去。 NFS本身是没有提供信息传输的协议和功能的，但NFS却能让我们通过网络进行资料的分享，这是因为NFS使用了一些其它的传输协议。而这些传输协议用到这个RPC功能的。可以说NFS本身就是使用RPC的一个程序，或者说NFS也是一个RPC SERVER。所以只要用到NFS的地方都要启动RPC服务，不论是NFS SERVER或者NFS CLIENT。这样SERVER和CLIENT才能通过RPC来实现PROGRAM PORT的对应。可以这么理解RPC和NFS的关系：NFS是一个文件系统，而RPC是负责负责信息的传输。 事实上，有很多这样的服务器都是向 RPC 注册的，举例来说，NIS (Network Information Service) 也是 RPC server 的一种。 Linux服务之NFS服务器 那RPC又是如何知道每个NFS的端口呢？ 这是因为当服务器在启动 NFS 时会随机取用数个端口，并主动的向 RPC 注册，因此 RPC 可以知道每个埠口对应的 NFS 功能，然后 RPC 又是固定使用 111 端口来监听客户端的需求并报客户端正确的埠口， 所以当然可以让 NFS 的启动更为轻松愉快了。 所以你要注意，要启动 NFS 之前，RPC 就要先启动了，否则 NFS 会无法向 RPC 注册。 另外，RPC 若重新启动时，原本注册的数据会不见，因此 RPC 重新启动后，它管理的所有服务都要重新启动来重新向 RPC 注册。 那客户端如何向NFS服务端交换数据数据呢？ (1) 客户端会向服务器端的 RPC 的111端口发出 NFS 档案存取功能的询问要求 (2) 服务器端找到对应的已注册的 NFS 守护进程端口后，会回报给客户端 (3) 客户端了解正确的端口后，就可以直接与 NFS 守护进程来联机 1.2 激活 NFS 服务 NFS 服务需要激活几个重要的 RPC 守护进程 工作流程 nfs—client => portmapper => mountd => nfs-server(nfsd) Linux服务之NFS服务器 (1) rpc.nfsd 这个守护进程主要的功能，则是在管理客户端是否能够登入主机的权限，其中还包含这个登入者的 ID 的判别。 (2) rpc.mountd 主要功能 这个守护进程主要的功能，则是在管理 NFS 的档案系统，用于给用户提供访问令牌。 访问的令牌，由本地的RPC提供随机端口。本地的RPC叫做portmapper，可以使用rpcinfo -P查看。 RPC的portmapper服务工作在1111端口。 请求过程 当客户端顺利的通过 rpc.nfsd 而登入主机之后，在它可以使用 NFS server 提供的档案之前，还会经过档案使用权限 的认证程序，就是那个-rwxrwxrwx、owner、group那几个权限啦。 然后它会去读 NFS 的设定档 /etc/exports 来比对客户端的权限，当通过这一关之后，客户端就可以取得使用 NFS 档案的权限啦。 注释：NFS需要有两个套件 nfs-utils NFS服务的主要套件 提供rpc.nfsd和rpc.mountd两个NFS守护进程和与其它相关文档与说明文件、执行档等的套件 portmap 主要负责RPC端口和守护进程的映射关系，即portmapper 在激活任何一个RPC server之前，我们都需要激活portmapper才行 1.3 各个版本之间的比较 NFS是一种网络文件系统，从1985年推出至今，共发布了3个版本：NFSv2、NFSv3、NFSv4，NFSv4包含两个次版本NFSv4.0和NFSv4.1。经过20多年发展，NFS发生了非常大的变化，最大的变化就是推动者从Sun变成了NetApp，NFSv2和NFSv3基本上是Sun起草的，NetApp从NFSv4.0参与进来，并且主导了NFSv4.1标准的制定过程，而Sun已经被Oracle收购了。 编号 协议版本 RFC 时间 页数 1 NFSv2 rfc1094 1989 年 3 月 27 页 2 NFSv3 rfc1813 1995 年 6 月 126 页 3 NFSv4.0 rfc3530 2003 年 4 月 275 页 4 NFSv4.1 rfc5661 2010 年 1 月 617 页 1. NFSv2 NFSv2是第一个以RFC形式发布的版本，实现了基本的功能。 2. NFSv3 协议特点 NFSv3修正了NFSv2的一些bug，两者有如下一些差别，但是感觉没有本质的差别。 区别差别 (1) NFSv2只支持同步写，如果客户端向服务器端写入数据，服务器必须将数据写入磁盘中才能发送应答消息。NFSv3支持异步写操作，服务器只需要将数据写入缓存中就可以发送应答信息了。 (2) NFSv3增加了ACCESS请求，ACCESS用来检查用户的访问权限。因为服务器端可能进行uid映射，因此客户端的uid和gid不能正确反映用户的访问权限。 (3) 一些请求调整了参数和返回信息，毕竟NFSv3和NFSv2发布的间隔有6年，经过长期运行可能觉得NFSv2某些请求参数和返回信息需要改进。 3. NFSv4.0 协议特点 相比NFSv3，NFSv4发生了比较大的变化，最大的变化是NFSv4有状态了。NFSv2和NFSv3都是无状态协议，服务区端不需要维护客户端的状态信息。 无状态协议的一个优点在于灾难恢复，当服务器出现问题后，客户端只需要重复发送失败请求就可以了，直到收到服务器的响应信息。 区别差别 (1) NFSv4增加了安全性，支持RPCSEC-GSS身份认证。 (2) NFSv4设计成了一种有状态的协议，自身实现了文件锁功能和获取文件系统根节点功能。 (3) NFSv4只提供了两个请求NULL和COMPOUND，所有的操作都整合进了COMPOUND中，客户端可以根据实际请求将多个操作封装到一个COMPOUND请求中，增加了灵活性。 (4) NFSv4文件系统的命令空间发生了变化，服务器端必须设置一个根文件系统(fsid=0)，其他文件系统挂载在根文件系统上导出。 (5) NFSv4支持delegation。由于多个客户端可以挂载同一个文件系统，为了保持文件同步，NFSv3中客户端需要经常向服务器发起请求，请求文件属性信息，判断其他客户端是否修改了文件。如果文件系统是只读的，或者客户端对文件的修改不频繁，频繁向服务器请求文件属性信息会降低系统性能。NFSv4可以依靠delegation实现文件同步。 (6) NFSv4修改了文件属性的表示方法。由于NFS是Sun开发的一套文件系统，设计之出NFS文件属性参考了UNIX中的文件属性，可能Windows中不具备某些属性，因此NFS对操作系统的兼容性不太好。 4. NFSv4.1 与NFSv4.0相比，NFSv4.1最大的变化是支持并行存储了。在以前的协议中，客户端直接与服务器连接，客户端直接将数据传输到服务器中。 当客户端数量较少时这种方式没有问题，但是如果大量的客户端要访问数据时，NFS服务器很快就会成为一个瓶颈，抑制了系统的性能。NFSv4.1支持并行存储，服务器由一台元数据服务器(MDS)和多台数据服务器(DS)构成，元数据服务器只管理文件在磁盘中的布局，数据传输在客户端和数据服务器之间直接进行。由于系统中包含多台数据服务器，因此数据可以以并行方式访问，系统吞吐量迅速提升。 2. NFS 服务搭建 CentOS7以NFSv4作为默认版本，NFSv4使用TCP协议（端口号是2049）和NFS服务器建立连接。 # 系统环境 系统平台：CentOS Linux release 7.9 (Final) NFS Server IP：192.168.0.17 防火墙已关闭/iptables: Firewall is not running. SELINUX=disabled 2.1 安装 NFS 服务 服务端 服务端，程序包名nfs-utils、rpcbind，默认都已经安装了 可以通过rpm -ql nfs-utils查看帮助文档等信息 客户端 客户端，需要安装程序包名nfs-utils，提供基本的客户端命令工具 [root@VM-0-17-tlinux ~/nfs]# yum install nfs-utils -y 查看NFS服务端口 NFS启动时会随机启动多个端口并向RPC注册，为了方便配置防火墙，需要固定NFS服务端口。 这样如果使用iptables对NFS端口进行限制就会有点麻烦，可以更改配置文件固定NFS服务相关端口 分配端口，编辑配置文件/etc/sysconfig/nfs # 使用rpcinfo -P会发现rpc启动了很多监听端口 [root@VM-0-17-tlinux ~/nfs]# rpcinfo -p localhost:q1 # 添加如下 [root@VM-0-17-tlinux ~/nfs]# vim /etc/sysconfig/nfs RQUOTAD_PORT=30001 LOCKD_TCPPORT=30002 LOCKD_UDPPORT=30002 MOUNTD_PORT=30003 STATD_PORT=30004 启动服务： [root@VM-0-17-tlinux ~/nfs]# systemctl start nfs.service 2.2 服务文件配置 相关文件和命令 文件名 说明 /etc/exports NFS 服务器端需要设定的内容，其作用是设定谁拥有什么样的权限去访问此机器的哪个目录 /var/lib/nfs/etab 无需设定，用于纪录 NFS 服务器完整的权限设定，exports 中没有设定的缺省值也会被列出 /var/lib/nfs/xtab 纪录 NFS 连接的相关信息 /usr/sbin/exportfs NFS 设定管理命令，用于 Server 侧设定，通过此条命令使得 exports 的设定变得有效或者无效 /usr/sbin/showmount 用于显示 NFS 设定的相关信息，Server 端和 Client 端均可 配置文件/etc/exports 我们可以按照“共享目录的路径 允许访问的 NFS 客户端(共享权限参数)”的格式，定义要共享的目录与相应的权限 常用参数 作用 ro 只读 rw 读写 sync 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据 root_squash 当 NFS 客户端以 root 管理员访问时，映射为 NFS 服务器的匿名用户 all_squash 无论 NFS 客户端使用什么账户访问，均映射为 NFS 服务器的匿名用户 no_root_squash 当 NFS 客户端以 root 管理员访问时，映射为 NFS 服务器的 root 管理员 secure 这个选项是缺省选项，它使用了 1024 以下的 TCP/IP 端口实现 NFS 的连接 insecure 禁止使用了 1024 以下的 TCP/IP 端口实现 NFS 的连接 no_wdelay 这个选项关闭写延时，如果设置了 async，那么 NFS 就会忽略这个选项 nohide 如果将一个目录挂载到另外一个目录之上，那么原来的目录通常就被隐藏起来或看起来像空的一样。要禁用这种行为，需启用 hide 选项 no_subtree_check 这个选项关闭子树检查，子树检查会执行一些不想忽略的安全性检查，缺省选项是启用子树检查 no_auth_nlm 这个选项也可以作为insecure_locks指定，它告诉 NFS 守护进程不要对加锁请求进行认证。如果关心安全性问题，就要避免使用这个选项。缺省选项是auth_nlm或secure_locks。 mp(mountpoint=path) 通过显式地声明这个选项，NFS 要求挂载所导出的目录 fsid=num 这个选项通常都在 NFS 故障恢复的情况中使用，如果希望实现 NFS 的故障恢复，请参考 NFS 文 [root@VM-0-17-tlinux ~/nfs]# cat /etc/exports /nfs 192.168.*.*(rw,sync,root_squash) #修改完配置后 重新启动nfs服务 root@VM-0-17-tlinux ~/nfs]# systemctl restart nfs [root@VM-0-17-tlinux ~/nfs]# systemctl status nfs 3. 实战演示 将NFS挂在到K8S容器服务的POD里面 案例一 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt name: nfs volumes: - name: nfs nfs: path: /nfs server: 192.168.0.17 [root@VM-0-17-tlinux ~/nfs]# cd /nfs/ [root@VM-0-17-tlinux /nfs]# echo hello worload > hello.txt [root@VM-0-17-tlinux /nfs]# cat hello.txt hello worload #登录容器查看挂着情况 [root@VM-0-17-tlinux /nfs]# kubectl exec -it centos-54db87ccc9-nkx86 -- /bin/bash [root@centos-54db87ccc9-nkx86 /]# [root@centos-54db87ccc9-nkx86 /]# df -h Filesystem Size Used Avail Use% Mounted on overlay 50G 11G 37G 23% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup 192.168.0.17:/nfs 50G 32G 16G 67% /mnt /dev/vda1 50G 11G 37G 23% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 12K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware [root@centos-54db87ccc9-nkx86 /]# cd /mnt/ [root@centos-54db87ccc9-nkx86 mnt]# cat hello.txt hello worload [root@centos-54db87ccc9-nkx86 mnt]# echo 1111>> hello.txt bash: hello.txt: Permission denied [root@centos-54db87ccc9-nkx86 mnt]ls -lrt total 4 -rw-r--r-- 1 root root 14 Oct 17 13:14 hello.txt 可以看到 上面我们的 /etc/exports /nfs 192.168.*.*(rw,sync,root_squash) #是没有权限修改文件 下面我们将配置文件修改成如下配置进行测试 [root@VM-0-17-tlinux /nfs]# cat /etc/exports /nfs 192.168.*.*(rw,sync,no_root_squash) #重启NFS 服务 [root@VM-0-17-tlinux /nfs]# systemctl restart nfs [root@VM-0-17-tlinux /nfs]# systemctl status nfs 将POD销毁重建后登录POD里测试。可以修改文件 并且可以创建文件 [root@centos-54db87ccc9-rgnk8 mnt]# touch 222 [root@centos-54db87ccc9-rgnk8 mnt]# ls 222 hello.txt [root@centos-54db87ccc9-rgnk8 mnt]# echo \"1111\">> hello.txt [root@centos-54db87ccc9-rgnk8 mnt]# cat hello.txt hello worloada 1111 [root@centos-54db87ccc9-rgnk8 mnt]# ls -lrt total 4 -rw-r--r-- 1 root root 0 Oct 17 13:24 222 -rw-r--r-- 1 root root 20 Oct 17 13:28 hello.txt 案例二 使用PVC和PV方式创建并挂载 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv-operationdata spec: accessModes: - ReadWriteMany capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 192.168.0.17 path: /nfs volumeHandle: cfs-pv2 persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-operationdata namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: \"\" volumeMode: Filesystem volumeName: pv-operationdata --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt name: nfs volumes: - name: nfs persistentVolumeClaim: claimName: pvc-operationdata 案例三 服务器端挂载 使用showmount命令查询NFS服务器的远程共享信息 showmount命令输出格式为“共享的目录名称 允许使用客户端地址” showmount命令 参数 作用 -e 显示 NFS 服务器的共享列表 -a 显示本机挂载的文件资源的情况 NFS 资源的情况 -v 显示版本号 exportfs命令 维护exports文件导出的文件系统表的专用工具，可以修改配置之后不重启NFS服务 export -ar：重新导出所有的文件系统 export -au：关闭导出的所有文件系统 export -u FS:：关闭指定的导出的文件系统 # 查看NFS服务器端共享的文件系统 # showmount -e NFSSERVER_IP [root@VM-0-17-tlinux ~/nfs]# showmount -e 192.168.0.17 Export list for 192.168.0.17: /nfs 192.168.*.* # NFS客户端创建一个挂载目录，挂载服务端NFS文件系统到本地 # mount -t nfs SERVER:/path/to/sharedfs /path/to/mount_point [root@VM-0-11-tlinux ~]# mkdir /nfsfile [root@VM-0-11-tlinux ~]# mount -t nfs 192.168.0.17:/nfs /nfsfile [root@VM-0-11-tlinux ~]# df -h | grep nfsfile 192.168.0.17:/nfs 50G 32G 16G 67% /nfsfile # 挂载成功后就应该能够顺利地看到在执行前面的操作时写入的文件内容了 [root@VM-0-11-tlinux ~]# cat /nfsfile/hello.txt hello worloada 1111 # 如果希望NFS文件共享服务能一直有效，则需要将其写入到fstab文件中 # SERVER:/PATH/TO/EXPORTED_FS /mount_point nfs defaults,_netdev 0 0 [root@VM-0-11-tlinux ~]# vim /etc/fstab 192.168.0.17:/nfs /nfsfile nfs defaults 0 0 参考文档：https://www.escapelife.site/posts/c49dfbab.html apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: mysql qcloud-app: mysql name: mysql spec: podManagementPolicy: OrderedReady replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: mysql qcloud-app: mysql serviceName: \"\" template: metadata: labels: k8s-app: mysql qcloud-app: mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"123456\" image: mysql:5.7 imagePullPolicy: IfNotPresent name: mysql volumeMounts: - mountPath: /var/lib/mysql name: nfs subPath: mysql_docker/data dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - name: nfs nfs: path: / server: 172.16.3.7 "}}