{"./":{"url":"./","title":"Introduction","keywords":"","body":"关于本书 本书为电子书形式，内容为本人多年的 Kubernetes运维过程中实战经验进行系统性记录 ，以备快速查找，仅记录 在实践中学习，在学习中实践 常用命令 git init git remote add origin git@github.com:chen1900s/book.git git add . git commit -m \"update\" git checkout -b gh-pages1 git push origin gh-pages1 "},"docs/Kubernetes/k8s-common-commands.html":{"url":"docs/Kubernetes/k8s-common-commands.html","title":"k8s常用批量命令","keywords":"","body":" 仅限腾讯云TKE集群中使用，其他容器集群或者自建仅参考，依赖于环境 1，节点相关 表格输出各节点占用的 podCIDR kubectl get no -o=custom-columns=INTERNAL-IP:.metadata.name,EXTERNAL-IP:.status.addresses[1].address,CIDR:.spec.podCIDR INTERNAL-IP EXTERNAL-IP CIDR 172.30.2.5 139.186.202.9 172.16.0.0/26 表格输出各节点总可用资源 (Allocatable) kubectl get no -o=custom-columns=\"NODE:.metadata.name,ALLOCATABLE CPU:.status.allocatable.cpu,ALLOCATABLE MEMORY:.status.allocatable.memory\" NODE ALLOCATABLE CPU ALLOCATABLE MEMORY 172.30.2.5 1930m 1347064Ki 输出各节点已分配资源的情况 kubectl get nodes --no-headers | awk '{print $1}' | xargs -I {} sh -c \"echo {} ; kubectl describe node {} | grep Allocated -A 5 | grep -ve Event -ve Allocated -ve percent -ve --;\" 2，查询所有节点挂载卷信息 kubectl get node -ocustom-columns='节点名称:.metadata.name,容器网段:.spec.podCIDR,eni-ip配额:.status.capacity.\"tke\\.cloud\\.tencent\\.com\\/eni-ip\",挂卷:.status.volumesInUse' 3，查询所有节点IP和实例ID ## kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:节点IP.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID ##查看节点的内核版本 kubectl get node -ocustom-columns=节点名称:.metadata.name,节点内核:.status.nodeInfo.kernelVersion,节点UUID:.status.nodeInfo.machineID,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\" ##全部信息 kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID,节点内核:.status.nodeInfo.kernelVersion,节点UUID:.status.nodeInfo.machineID,可用区:.metadata.labels.\"failure-domain\\.beta\\.kubernetes\\.io\\/zone\" kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name} {.status.addresses[?(@.type==\"ExternalIP\")].address}{\"\\n\"}' #获取节点IP： kubectl get node -ocustom-columns=name:.metadata.name,IP:status.addresses[0].address 总结如上命令 kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID,节点内核:.status.nodeInfo.kernelVersion,节点UUID:.status.nodeInfo.machineID,可用区:.metadata.labels.\"failure-domain\\.beta\\.kubernetes\\.io\\/zone\" 查看容量： kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID,节点内核:.status.nodeInfo.kernelVersion,Allocatable:.status.allocatable.\"nvidia\\.com\\/gpu\" kubectl get node -ocustom-columns=节点名称:.metadata.name,节点IP:.status.addresses[0].address,实例ID:.metadata.labels.\"cloud\\.tencent\\.com\\/node-instance-id\",providerID:.spec.providerID,节点内核:.status.nodeInfo.kernelVersion,GPU-Allocatable:.status.allocatable.\"nvidia\\.com\\/gpu\",CPU-Allocatable:.status.allocatable.cpu,MEM-Allocatable:.status.allocatable.memory,eniIP-Allocatable:.status.allocatable.\"tke\\.cloud\\.tencent\\.com\\/eni-ip\" 查看节点当前所在的节点池 kubectl get node -ocustom-columns='Name:.metadata.name,节点池:.metadata.labels.tke\\.cloud\\.tencent\\.com\\/nodepool-id,伸缩组:.metadata.labels.cloud\\.tencent\\.com\\/auto-scaling-group-id,createTime:.metadata.creationTimestamp' 4，查询所有节点eni-ip信息 #查下辅助网卡eniInfo详细信息 kubectl get nec -ocustom-columns=Name:.metadata.name,cvmID:.spec.providerID,eni:.status.eniInfos,ip:.spec.maxIPPerENI #查看集群节点vpc-cni配额 kubectl get node -ocustom-columns=Name:.metadata.name,capcIP:.status.capacity.\"tke\\.cloud\\.tencent\\.com/eni-ip\",allocIP:.status.allocatable.\"tke\\.cloud\\.tencent\\.com/eni-ip\" #查询VIP和VIPC绑定关系 kubectl get vip -ocustom-columns=Name:.metadata.name,vipcNamespace:.spec.claimRef.namespace,type:spec.type,vipcName:.spec.claimRef.name 5，批量删除驱逐状态POD 查寻确认没问题后再做删除，需要把NameSpace替换成用户的命名空间名称 kubectl get pods -n NameSpace |grep Evicted 然后是批量删除 kubectl get pods -n NameSpace | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n NameSpace #所有命名空间 kubectl get pods --all-namespaces |grep Evicted 然后是批量删除 kubectl get pods --all-namespaces | grep Evicted | awk '{print $1}' | xargs kubectl delete pod --all-namespaces 6，批量删除UnexpectedAdmissionError状态pod #指定命名空间去删除 kubectl get pods -n NAMESPACE | grep UnexpectedAdmissionError | awk '{print $1}' | xargs kubectl delete pod -n NAMESPACE 7，批量查看集群中LB类型的service信息 #可以打印的第一行，然后匹配的其他行 kubectl get svc -ocustom-columns=namespace:.metadata.namespace,serviceName:.metadata.name,type:.spec.type,lbID:.metadata.annotations.\"service\\.kubernetes\\.io\\/loadbalance-id\",subnet:.metadata.annotations.\"service\\.kubernetes\\.io\\/qcloud-loadbalancer-internal-subnetid\" -A | grep -E \"serviceName|LoadBalancer\" #查看LBid & VIP信息 kubectl get svc -ocustom-columns=namespace:.metadata.namespace,serviceName:.metadata.name,type:.spec.type,lbID:.metadata.annotations.\"service\\.kubernetes\\.io\\/loadbalance-id\",subnet:.metadata.annotations.\"service\\.kubernetes\\.io\\/qcloud-loadbalancer-internal-subnetid\",vip:.status.loadBalancer.ingress[0].ip -A | grep -E \"serviceName|LoadBalancer\" 8，批量查看集群中ingress信息 kubectl get ingress -ocustom-columns=namespace:.metadata.namespace,ingressName:.metadata.name,ingressType:.metadata.annotations.\"kubernetes\\.io\\/ingress\\.class\",lbID:.metadata.annotations.\"kubernetes\\.io\\/ingress\\.qcloud-loadbalance-id\",vip:.status.loadBalancer.ingress[0].ip,uuid:.metadata.uid -A 9，批量查询PV和PVC 绑定关系 kubectl get pv -ocustom-columns='pvName:.metadata.name,storageClassName:.spec.storageClassName,pvcNamespace:.spec.claimRef.namespace,pvcName:.spec.claimRef.name,volumeHandle:.spec.csi.volumeHandle,driver:.spec.csi.driver' 批量查看CBS盘信息 kubectl get pv -ocustom-columns='pvName:.metadata.name,storageClassName:.spec.storageClassName,pvcNamespace:.spec.claimRef.namespace,pvcName:.spec.claimRef.name,volumeHandle:.spec.csi.volumeHandle,driver:.spec.csi.driver,CBSid:.spec.qcloudCbs.cbsDiskId' 10，显示Pod的容器镜像 kubectl get pods -o custom-columns='NAME:metadata.name,IMAGES:spec.containers[*].image' kubectl get deployments -o custom-columns='DeploymentName:metadata.name,Namespace:.metadata.namespace,image:.spec.template.spec.containers[*].image' --all-namespaces kubectl get statefulset -o custom-columns='DeploymentName:metadata.name,Namespace:.metadata.namespace,image:.spec.template.spec.containers[*].image' --all-namespaces kubectl get pods -o custom-columns='NAME:metadata.name,security-group-id:.metadata.annotations.eks\\.tke\\.cloud\\.tencent\\.com\\/security-group-id' kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{\"\\n\"}{.metadata.name}{\":\\t\"}{range .spec.containers[*]}{.image}{\", \"}{end}{end}' 11，批量查询POD的里面容器ID kubectl get pods -o custom-columns='podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[0].containerID' #kubectl get pods -o custom-columns=podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[*].containerID #kubectl get pods -o custom-columns=podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[0].containerID | awk -F 'docker://' '{print $1,$2}' #kubectl get pods -o custom-columns='podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[0].containerID' | awk -F 'docker://|| containerd://' '{print $1,$2}' ##批量查下POD的 POD_ID 可以对照查看对应日志命令文件 kubectl get pods -o custom-columns='Namespace:..metadata.namespace,podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeIP:.status.hostIP,Pod_ID:.metadata.uid,ContainerName:.spec.containers[*].name' 查看POD创建时间： kubectl get pods -o custom-columns='podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,startTime:.status.startTime' 12，批量查看容器ID docker ps | awk '{print $1}' | xargs -n 1 -I {} -- sh -c \"echo {} && docker inspect {} |grep 27c0e22fc5ffa8aa540b68182672ea61fd2bb701740926025ba77a66b4428341 -C 1\" 13，EKS获取所有EIP列表 kubectl get pods -A -o=jsonpath='{range .items[*]}{\"pod_name:\"}{.metadata.name}{\"\\teip-id:\"}{.metadata.annotations.tke\\.cloud\\.tencent\\.com\\/eip-id}{\"\\teip-ip:\"}{.metadata.annotations.tke\\.cloud\\.tencent\\.com\\/eip\\-public\\-ip}{\"\\n\"}{end}' 或者使用这个命令 kubectl get pod -ocustom-columns=pod-name:.metadata.name,eip-id:.metadata.annotations.\"tke\\.cloud\\.tencent\\.com\\/eip\\-id\",eip-public-ip:.metadata.annotations.\"tke\\.cloud\\.tencent\\.com\\/eip\\-public\\-ip\" 14，批量查看集群里面有那些RS使用了 csi inline模式 kubectl get rs -o custom-columns=ReplicaSetsName:.metadata.name,CurrentPOD:.spec.replicas,CBSid:.spec.template.spec.volumes[*].qcloudCbs.cbsDiskId -A #查看当前有哪些POD在使用csi inline kubectl get pods -o custom-columns=Namespace:.metadata.namespace,PodName:.metadata.name,Status:.status.phase,CBSid:.spec.volumes[*].qcloudCbs.cbsDiskId #通过这个简单查询下 有那些rs使用了 kubectl get rs -o custom-columns=namespace:.metadata.namespace,ReplicaSetsName:.metadata.name,CurrentPOD:.spec.replicas,CBSid:.spec.template.spec.volumes[*].qcloudCbs.cbsDiskId -A | grep disk 查看 StatefulSet类型是否使用csi inline kubectl get sts -o custom-columns=namespace:.metadata.namespace,StatefulSetName:.metadata.name,CBSid:.spec.template.spec.volumes[*].qcloudCbs.cbsDiskId -A | grep disk 15，批量查看pod资源设置情况limit和request #调整显示顺序 kubectl get pod -o custom-columns='namespace:.metadata.namespace,PodName:.metadata.name,ContainerName:.spec.containers[*].name,CPURequest:.spec.containers[*].resources.requests.cpu,CPULimit:.spec.containers[*].resources.limits.cpu,MemRequest:.spec.containers[*].resources.requests.memory,MemLimit:.spec.containers[*].resources.limits.memory,Node:.status.hostIP' kubectl get pod -n kube-system -o=custom-columns='NAME:.metadata.name,NAMESPACE:.metadata.namespace,PHASE:.status.phase,Request-cpu:.spec.containers\\[0\\].resources.requests.cpu,Request-memory:.spec.containers\\[0\\].resources.requests.memory,Limit-cpu:.spec.containers\\[0\\].resources.limits.cpu,Limit-memory:.spec.containers\\[0\\].resources.limits.memory,Node:.status.hostIP' 16，批量查看集群容器网段 kubectl get node -ocustom-columns='Name:.metadata.name,cidr:.spec.podCIDR,annotations:.metadata.annotations.\"tke\\.cloud\\.tencent\\.com\\/pod-cidrs\"' 17，批量查看日志采集配置 kubectl get logconfig -ocustom-columns='Name:.metadata.name,ClsTopic:.spec.clsDetail.topicId,ClsType:.spec.inputDetail.type' 查看采集对象： kubectl get logconfig -ocustom-columns='Name:.metadata.name,ClsTopic:.spec.clsDetail.topicId,ClsType:.spec.inputDetail.type,Namespace:.spec.inputDetail.containerStdout.workloads[*].namespace,Workloads:.spec.inputDetail.containerStdout.workloads[*].name' 18，批量删除多个job #删除成功的作业: kubectl delete jobs --field-selector status.successful=1 #删除失败或长时间运行的作业 kubectl delete jobs --field-selector status.successful=0 19，批量查看集群有没有POD使用GPU kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.nodeName}{\"\\n\"}{end}' | while read line; do pod=$(echo $line | awk '{print $1}'); node=$(echo $line | awk '{print $2}'); echo -n \"${pod} \"; kubectl describe pod $pod | grep -q \"nvidia.com/gpu\" && echo \"is using GPU on ${node}\" || echo \"is not using GPU\"; done "},"docs/Kubernetes/k8s-kubectl-commands.html":{"url":"docs/Kubernetes/k8s-kubectl-commands.html","title":"kubectl常用命令","keywords":"","body":"本文是对k8s，kubectl常用命令的总结。 语法 kubectl [command] [TYPE] [NAME] [flags] 1 command：子命令，用于操作Kubernetes集群资源对象的命令，如create, delete, describe, get, apply等 2 TYPE：资源对象的类型，如pod, service, rc, deployment, node等，可以单数、复数以及简写（pod, pods, po/service, services, svc） 3 NAME：资源对象的名称，不指定则返回所有，如get pod 会返回所有pod， get pod nginx， 只返回nginx这个pod 4 flags：kubectl子命令的可选参数，例如-n 指定namespace，-s 指定apiserver的URL 资源对象类型列表 可以用这个命令获取到： kubectl explain 或者 kubectl api-resources 名称 简写 componentsstatuses cs daemonsets ds deployment deploy events ev endpoints ep horizontalpodautoscalers hpa ingresses ing jobs limitranges limits nodes no namspaces ns pods po persistentvolumes pv persistentvolumeclaims pvc resourcequotas quota replicationcontrollers rc secrets serviceaccounts sa services svc 特殊用法： kubectl子命令 主要包括对资源的创建、删除、查看、修改、配置、运行等 kubectl --help 可以查看所有子命令 kubectl参数 kubectl options 可以查看支持的参数，例如–namespace指定所在namespace kubectl输出格式 kubectl命令可以用多种格式对结果进行显示，输出格式通过-o参数指定： -o支持的格式有 输出格式 说明 custom-columns= 根据自定义列名进行输出，逗号分隔 custom-columns-file= 从文件中获取自定义列名进行输出 json 以JSON格式显示结果 jsonpath= 输出jasonpath表达式定义的字段信息 jasonpath-file= 输出jsonpath表达式定义的字段信息，来源于文件 name 仅输出资源对象的名称 wide 输出更多信息，比如会输出node名 yaml 以yaml格式输出 kubectl命令示例： 1） 创建资源对象 根据yaml文件创建service和deployment,在create和apply之间，我更倾向于apply，这是因为，如果更改了yaml文件，apply不用删除就可应用变更。 kubectl create -f service.yaml -f deploy.yaml kubectl apply -f service.yaml -f deploy.yaml 也可以指定一个目录，这样可以一次性根据该目录下所有yaml或json文件定义资源 kubectl create -f 2） 查看资源对象查看所有pod kubectl get pods 查看deployment和service kubectl get deploy,svc 3） 描述资源对象显示node的详细信息 kubectl describe nodes 显示pod的详细信息 kubectl describe pods/ 显示deployment管理的pod信息 kubectl describe pods 4） 删除资源对象基于yaml文件删除 kubectl delete -f pod.yaml 删除所有包含某个label的pod和service kubectl delete po,svc -l name= 删除所有pod kubectl delete po --all 5） 执行容器的命令在pod中执行某个命令，如date kubectl exec date //pod-name如果不加，默认会选择第一个pod 指定pod的某个容器执行命令 kubectl exec date 进入到pod的容器里 kubectl exec -it bash 6） 查看容器日志 kubectl logs 可以动态查看，类似于tail -f kubectl logs -f -c "},"docs/Kubernetes/dockerfile-best-practices.html":{"url":"docs/Kubernetes/dockerfile-best-practices.html","title":"Dockerfile最佳实践","keywords":"","body":"Dockerfile实践 容器应该是短暂的 通过 Dockerfile 构建的镜像所启动的容器应该尽可能短暂（生命周期短）。「短暂」意味着可以停止和销毁容器，并且创建一个新容器并部署好所需的设置和配置工作量应该是极小的。 使用 .dockerignore 文件 使用 Dockerfile 构建镜像时最好是将 Dockerfile 放置在一个新建的空目录下。然后将构建镜像所需要的文件添加到该目录中。为了提高构建镜像的效率，你可以在目录下新建一个 .dockerignore 文件来指定要忽略的文件和目录。.dockerignore 文件的排除模式语法和 Git 的 .gitignore 文件相似。 使用多阶段构建 在 Docker 17.05 以上版本中，你可以使用 多阶段构建 来减少所构建镜像的大小。 避免安装不必要的包 为了降低复杂性、减少依赖、减小文件大小、节约构建时间，你应该避免安装任何不必要的包。例如，不要在数据库镜像中包含一个文本编辑器。 一个容器只运行一个进程 应该保证在一个容器中只运行一个进程。将多个应用解耦到不同容器中，保证了容器的横向扩展和复用。例如 web 应用应该包含三个容器：web应用、数据库、缓存。 如果容器互相依赖，你可以使用 Docker 自定义网络 来把这些容器连接起来。 镜像层数尽可能少 你需要在 Dockerfile 可读性（也包括长期的可维护性）和减少层数之间做一个平衡。 将多行参数排序 将多行参数按字母顺序排序（比如要安装多个包时）。这可以帮助你避免重复包含同一个包，更新包列表时也更容易。也便于 PRs 阅读和审查。建议在反斜杠符号 \\ 之前添加一个空格，以增加可读性。 下面是来自 buildpack-deps 镜像的例子： RUN apt-get update && apt-get install -y \\ bzr \\ cvs \\ git \\ mercurial \\ subversion 构建缓存 在镜像的构建过程中，Docker 会遍历 Dockerfile 文件中的指令，然后按顺序执行。在执行每条指令之前，Docker 都会在缓存中查找是否已经存在可重用的镜像，如果有就使用现存的镜像，不再重复创建。如果你不想在构建过程中使用缓存，你可以在 docker build 命令中使用 --no-cache=true 选项。 但是，如果你想在构建的过程中使用缓存，你得明白什么时候会，什么时候不会找到匹配的镜像，遵循的基本规则如下： 从一个基础镜像开始（FROM 指令指定），下一条指令将和该基础镜像的所有子镜像进行匹配，检查这些子镜像被创建时使用的指令是否和被检查的指令完全一样。如果不是，则缓存失效。 在大多数情况下，只需要简单地对比 Dockerfile 中的指令和子镜像。然而，有些指令需要更多的检查和解释。 对于 ADD 和 COPY 指令，镜像中对应文件的内容也会被检查，每个文件都会计算出一个校验和。文件的最后修改时间和最后访问时间不会纳入校验。在缓存的查找过程中，会将这些校验和和已存在镜像中的文件校验和进行对比。如果文件有任何改变，比如内容和元数据，则缓存失效。 除了 ADD 和 COPY 指令，缓存匹配过程不会查看临时容器中的文件来决定缓存是否匹配。例如，当执行完 RUN apt-get -y update 指令后，容器中一些文件被更新，但 Docker 不会检查这些文件。这种情况下，只有指令字符串本身被用来匹配缓存。 一旦缓存失效，所有后续的 Dockerfile 指令都将产生新的镜像，缓存不会被使用。 Dockerfile 指令 下面针对 Dockerfile 中各种指令的最佳编写方式给出建议。 FROM 尽可能使用当前官方仓库作为你构建镜像的基础。推荐使用 Alpine (opens new window)镜像，因为它被严格控制并保持最小尺寸（目前小于 5 MB），但它仍然是一个完整的发行版。 LABEL 你可以给镜像添加标签来帮助组织镜像、记录许可信息、辅助自动化构建等。每个标签一行，由 LABEL 开头加上一个或多个标签对。下面的示例展示了各种不同的可能格式。# 开头的行是注释内容。 注意：如果你的字符串中包含空格，必须将字符串放入引号中或者对空格使用转义。如果字符串内容本身就包含引号，必须对引号使用转义。 # Set one or more individual labels LABEL com.example.version=\"0.0.1-beta\" LABEL vendor=\"ACME Incorporated\" LABEL com.example.release-date=\"2015-02-12\" LABEL com.example.version.is-production=\"\" 一个镜像可以包含多个标签，但建议将多个标签放入到一个 LABEL 指令中。 # Set multiple labels at once, using line-continuation characters to break long lines LABEL vendor=ACME\\ Incorporated \\ com.example.is-beta= \\ com.example.is-production=\"\" \\ com.example.version=\"0.0.1-beta\" \\ com.example.release-date=\"2015-02-12\" 关于标签可以接受的键值对，参考 Understanding object labels (opens new window)。关于查询标签信息，参考 Managing labels on objects (opens new window)。 RUN 为了保持 Dockerfile 文件的可读性，可理解性，以及可维护性，建议将长的或复杂的 RUN 指令用反斜杠 \\ 分割成多行。 apt-get RUN 指令最常见的用法是安装包用的 apt-get。因为 RUN apt-get 指令会安装包，所以有几个问题需要注意。 不要使用 RUN apt-get upgrade 或 dist-upgrade，因为许多基础镜像中的「必须」包不会在一个非特权容器中升级。如果基础镜像中的某个包过时了，你应该联系它的维护者。如果你确定某个特定的包，比如 foo，需要升级，使用 apt-get install -y foo 就行，该指令会自动升级 foo 包。 永远将 RUN apt-get update 和 apt-get install 组合成一条 RUN 声明，例如： RUN apt-get update && apt-get install -y \\ package-bar \\ package-baz \\ package-foo 将 apt-get update 放在一条单独的 RUN 声明中会导致缓存问题以及后续的 apt-get install 失败。比如，假设你有一个 Dockerfile 文件： FROM ubuntu:18.04 RUN apt-get update RUN apt-get install -y curl 构建镜像后，所有的层都在 Docker 的缓存中。假设你后来又修改了其中的 apt-get install 添加了一个包： FROM ubuntu:18.04 RUN apt-get update RUN apt-get install -y curl nginx Docker 发现修改后的 RUN apt-get update 指令和之前的完全一样。所以，apt-get update 不会执行，而是使用之前的缓存镜像。因为 apt-get update 没有运行，后面的 apt-get install 可能安装的是过时的 curl 和 nginx 版本。 使用 RUN apt-get update && apt-get install -y 可以确保你的 Dockerfiles 每次安装的都是包的最新的版本，而且这个过程不需要进一步的编码或额外干预。这项技术叫作 cache busting。你也可以显示指定一个包的版本号来达到 cache-busting，这就是所谓的固定版本，例如： RUN apt-get update && apt-get install -y \\ package-bar \\ package-baz \\ package-foo=1.3.* 固定版本会迫使构建过程检索特定的版本，而不管缓存中有什么。这项技术也可以减少因所需包中未预料到的变化而导致的失败。 下面是一个 RUN 指令的示例模板，展示了所有关于 apt-get 的建议。 RUN apt-get update && apt-get install -y \\ aufs-tools \\ automake \\ build-essential \\ curl \\ dpkg-sig \\ libcap-dev \\ libsqlite3-dev \\ mercurial \\ reprepro \\ ruby1.9.1 \\ ruby1.9.1-dev \\ s3cmd=1.1.* \\ && rm -rf /var/lib/apt/lists/* 其中 s3cmd 指令指定了一个版本号 1.1.*。如果之前的镜像使用的是更旧的版本，指定新的版本会导致 apt-get udpate 缓存失效并确保安装的是新版本。 另外，清理掉 apt 缓存 var/lib/apt/lists 可以减小镜像大小。因为 RUN 指令的开头为 apt-get udpate，包缓存总是会在 apt-get install 之前刷新。 注意：官方的 Debian 和 Ubuntu 镜像会自动运行 apt-get clean，所以不需要显式的调用 apt-get clean。 CMD CMD 指令用于执行目标镜像中包含的软件，可以包含参数。CMD 大多数情况下都应该以 CMD [\"executable\", \"param1\", \"param2\"...] 的形式使用。因此，如果创建镜像的目的是为了部署某个服务(比如 Apache)，你可能会执行类似于 CMD [\"apache2\", \"-DFOREGROUND\"] 形式的命令。我们建议任何服务镜像都使用这种形式的命令。 多数情况下，CMD 都需要一个交互式的 shell (bash, Python, perl 等)，例如 CMD [\"perl\", \"-de0\"]，或者 CMD [\"PHP\", \"-a\"]。使用这种形式意味着，当你执行类似 docker run -it python 时，你会进入一个准备好的 shell 中。CMD 应该在极少的情况下才能以 CMD [\"param\", \"param\"] 的形式与 ENTRYPOINT 协同使用，除非你和你的镜像使用者都对 ENTRYPOINT 的工作方式十分熟悉。 EXPOSE EXPOSE 指令用于指定容器将要监听的端口。因此，你应该为你的应用程序使用常见的端口。例如，提供 Apache web 服务的镜像应该使用 EXPOSE 80，而提供 MongoDB 服务的镜像使用 EXPOSE 27017。 对于外部访问，用户可以在执行 docker run 时使用一个标志来指示如何将指定的端口映射到所选择的端口。 ENV 为了方便新程序运行，你可以使用 ENV 来为容器中安装的程序更新 PATH 环境变量。例如使用 ENV PATH /usr/local/nginx/bin:$PATH 来确保 CMD [\"nginx\"] 能正确运行。 ENV 指令也可用于为你想要容器化的服务提供必要的环境变量，比如 Postgres 需要的 PGDATA。 最后，ENV 也能用于设置常见的版本号，比如下面的示例： ENV PG_MAJOR 9.3 ENV PG_VERSION 9.3.4 RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress && … ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH 类似于程序中的常量，这种方法可以让你只需改变 ENV 指令来自动的改变容器中的软件版本。 ADD 和 COPY 虽然 ADD 和 COPY 功能类似，但一般优先使用 COPY。因为它比 ADD 更透明。COPY 只支持简单将本地文件拷贝到容器中，而 ADD 有一些并不明显的功能（比如本地 tar 提取和远程 URL 支持）。因此，ADD 的最佳用例是将本地 tar 文件自动提取到镜像中，例如 ADD rootfs.tar.xz。 如果你的 Dockerfile 有多个步骤需要使用上下文中不同的文件。单独 COPY 每个文件，而不是一次性的 COPY 所有文件，这将保证每个步骤的构建缓存只在特定的文件变化时失效。例如： COPY requirements.txt /tmp/ RUN pip install --requirement /tmp/requirements.txt COPY . /tmp/ 如果将 COPY . /tmp/ 放置在 RUN 指令之前，只要 . 目录中任何一个文件变化，都会导致后续指令的缓存失效。 为了让镜像尽量小，最好不要使用 ADD 指令从远程 URL 获取包，而是使用 curl 和 wget。这样你可以在文件提取完之后删掉不再需要的文件来避免在镜像中额外添加一层。比如尽量避免下面的用法： ADD http://example.com/big.tar.xz /usr/src/things/ RUN tar -xJf /usr/src/things/big.tar.xz -C /usr/src/things RUN make -C /usr/src/things all 而是应该使用下面这种方法： RUN mkdir -p /usr/src/things \\ && curl -SL http://example.com/big.tar.xz \\ | tar -xJC /usr/src/things \\ && make -C /usr/src/things all 上面使用的管道操作，所以没有中间文件需要删除。 对于其他不需要 ADD 的自动提取功能的文件或目录，你应该使用 COPY。 ENTRYPOINT ENTRYPOINT 的最佳用处是设置镜像的主命令，允许将镜像当成命令本身来运行（用 CMD 提供默认选项）。 例如，下面的示例镜像提供了命令行工具 s3cmd: ENTRYPOINT [\"s3cmd\"] CMD [\"--help\"] 现在直接运行该镜像创建的容器会显示命令帮助： $ docker run s3cmd 或者提供正确的参数来执行某个命令： $ docker run s3cmd ls s3://mybucket 这样镜像名可以当成命令行的参考。 ENTRYPOINT 指令也可以结合一个辅助脚本使用，和前面命令行风格类似，即使启动工具需要不止一个步骤。 例如，Postgres 官方镜像使用下面的脚本作为 ENTRYPOINT： #!/bin/bash set -e if [ \"$1\" = 'postgres' ]; then chown -R postgres \"$PGDATA\" if [ -z \"$(ls -A \"$PGDATA\")\" ]; then gosu postgres initdb fi exec gosu postgres \"$@\" fi exec \"$@\" 注意：该脚本使用了 Bash 的内置命令 exec，所以最后运行的进程就是容器的 PID 为 1 的进程。这样，进程就可以接收到任何发送给容器的 Unix 信号了。 该辅助脚本被拷贝到容器，并在容器启动时通过 ENTRYPOINT 执行： COPY ./docker-entrypoint.sh / ENTRYPOINT [\"/docker-entrypoint.sh\"] 该脚本可以让用户用几种不同的方式和 Postgres 交互。 你可以很简单地启动 Postgres： $ docker run postgres 也可以执行 Postgres 并传递参数： $ docker run postgres postgres --help 最后，你还可以启动另外一个完全不同的工具，比如 Bash： $ docker run --rm -it postgres bash VOLUME VOLUME 指令用于暴露任何数据库存储文件，配置文件，或容器创建的文件和目录。强烈建议使用 VOLUME 来管理镜像中的可变部分和用户可以改变的部分。 USER 如果某个服务不需要特权执行，建议使用 USER 指令切换到非 root 用户。先在 Dockerfile 中使用类似 RUN groupadd -r postgres && useradd -r -g postgres postgres 的指令创建用户和用户组。 注意：在镜像中，用户和用户组每次被分配的 UID/GID 都是不确定的，下次重新构建镜像时被分配到的 UID/GID 可能会不一样。如果要依赖确定的 UID/GID，你应该显示的指定一个 UID/GID。 你应该避免使用 sudo，因为它不可预期的 TTY 和信号转发行为可能造成的问题比它能解决的问题还多。如果你真的需要和 sudo 类似的功能（例如，以 root 权限初始化某个守护进程，以非 root 权限执行它），你可以使用 gosu (opens new window)。 最后，为了减少层数和复杂度，避免频繁地使用 USER 来回切换用户。 WORKDIR 为了清晰性和可靠性，你应该总是在 WORKDIR 中使用绝对路径。另外，你应该使用 WORKDIR 来替代类似于 RUN cd ... && do-something 的指令，后者难以阅读、排错和维护。 官方镜像示例 这些官方镜像的 Dockerfile 都是参考典范：https://github.com/docker-library/docs "},"docs/Kubernetes/k8s-configmap-secret.html":{"url":"docs/Kubernetes/k8s-configmap-secret.html","title":"ConfigMap&Secret最佳实践","keywords":"","body":"ConfigMap 管理 基本概念 Kubernetes原生提供的 ConfigMap(配置集)，可以将配置数据，与业务镜像解耦分离， 以使容器化的应用程序具有可移植性 ，ConfigMap 包含配置数据，以供pod运行时使用，其核心数据结构，可以理解为一个 key-value 映射 说明： 配置集的key，value 均为string 类型。其中，key创建时有语法要求。 根据业务需要，业务Pod可以引用 ConfigMap 对象，作为容器的环境变量；或者以数据卷方式（后续表现为一个文件），挂载到 Pod 内部，作为配置文件使用。 使用场景 使用ConfigMap数据定义容器环境变 configmap作为环境变量使用时，Pod 可以挂载一个 ConfigMap key，也可以挂载多个 ConfigMap key: 说明： 此时 ConfigMap 的 key 为环境变量名称，对应的 Value 为环境变量值。 以数据卷方式挂载到Pod 同理，作为数据卷使用时，Pod 可以挂载多个 ConfigMap key 也可以挂载一个ConfigMap key 说明： 此时 ConfigMap Key 映射为容器内部的配置文件名，对应的 ConfigMap Value 表现为配置文件内容。 文件覆盖： 数据卷形式挂载，会覆盖对应挂载目录，使用时需注意路径信息。 实时更新： 本地小规模集群测试发现，ConfigMap 对象更新后，Pod 内部，ConfigMap 挂载对象几乎实时更新。 部分业务应用如果支持热加载，可以考虑使用该特性 reload 进程。社区开源的 nginx-ingress-controller，就包含类似机制（cm 对象更新，nginx 进程 reload）。 kubelet 在每次周期性同步时都会检查已挂载的 ConfigMap 是否是最新的。 但是，它使用其本地的基于 TTL 的缓存来获取 ConfigMap 的当前值。 因此，从更新 ConfigMap 到将新键映射到 Pod 的总延迟可能等于 kubelet 同步周期 （默认 1 分钟） + ConfigMap 在 kubelet 中缓存的 TTL（默认 1 分钟）。 Subpath volume： 业务容器，如果使用ConfigMap作为subPath 卷，将不会感知到ConfigMap 更新。 详情请参考：https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath 同理secret也是一样 使用单个 ConfigMap 中的数据定义容器环境变量 1，在 ConfigMap 中将环境变量定义为键值对: kubectl create configmap special-config --from-literal=special.how=very -n m-configmap 2，将 ConfigMap 中定义的 special.how 值分配给 Pod 规范中的 SPECIAL_LEVEL_KEY 环境变量。 apiVersion: apps/v1 kind: Deployment metadata: annotations: description: 将 ConfigMap 中定义的 special.how 值分配给 Pod 规范中的 SPECIAL_LEVEL_KEY 环境变量 labels: k8s-app: configmap-env-key qcloud-app: configmap-env-key name: configmap-env-key namespace: m-configmap spec: selector: matchLabels: k8s-app: configmap-env-key qcloud-app: configmap-env-key template: metadata: labels: k8s-app: configmap-env-key qcloud-app: configmap-env-key spec: containers: - env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: key: special.how name: special-config optional: false image: ccr.ccs.tencentyun.com/v_cjweichen/centos:latest imagePullPolicy: IfNotPresent name: centos 使用来自多个 ConfigMap 的数据定义容器环境变量 使用 ConfigMap 作为 subPath 卷挂载的容器将不会收到 ConfigMap 的更新。 ConfigMap或Secret挂载到特定目录的特定路径 有时候挂载希望将setenv.sh这样的一个初始化配置环境变量的脚本挂载到tomcat的bin目录: /usr/local/tomcat/bin下. 如果不使用subpath, 直接将该ConfigMap 挂载到/usr/local/tomcat/bin目录下, 那么该目录下已有的文件全部被覆盖. 所以正确的做法是使用Subpath进行挂载: 特别注意mountPath和subPath的写法, 最后的path要保持一致. 如mountPath是: /usr/local/tomcat/bin/setenv.sh;subPath是:setenv.sh`. mountPath`不要漏写为: `/usr/local/tomcat/bin/ volumeMounts: - mountPath: /usr/local/tomcat/bin/setenv.sh subPath: setenv.sh #name: config #readOnly: true "},"docs/Kubernetes/coredns-configmap.html":{"url":"docs/Kubernetes/coredns-configmap.html","title":"CoreDNS-ConfigMap选项","keywords":"","body":"CoreDns配置以及外部dns使用 CoreDNS ConfigMap选项 先来看看默认的CoreDns的配置文件 Corefile: |2- .:53 { errors health kubernetes cluster.local. in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 reload loadbalance } Corefile 配置包括以下 CoreDNS 插件： errors：错误记录到标准输出。 health：在 http://localhost:8080/health 处提供 CoreDNS 的健康报告。 ready：在端口 8181 上提供的一个 HTTP 末端，当所有能够 表达自身就绪的插件都已就绪时，在此末端返回 200 OK。 kubernetes：CoreDNS 将基于 Kubernetes 的服务和 Pod 的 IP 答复 DNS 查询。你可以在 CoreDNS 网站阅读更多细节。 你可以使用 ttl 来定制响应的 TTL。默认值是 5 秒钟。TTL 的最小值可以是 0 秒钟， 最大值为 3600 秒。将 TTL 设置为 0 可以禁止对 DNS 记录进行缓存， 此行内的cluster.local处理custer.local区域中的所有查询（以及in-addr.arpa中的反向dns查找） pods insecure 选项是为了与 kube-dns 向后兼容。你可以使用 pods verified 选项，该选项使得 仅在相同名称空间中存在具有匹配 IP 的 Pod 时才返回 A 记录。如果你不使用 Pod 记录，则可以使用 pods disabled 选项。 prometheus：CoreDNS 的度量指标值以 Prometheus 格式在 http://localhost:9153/metrics 上提供。 forward: 不在 Kubernetes 集群域内的任何查询都将转发到 预定义的解析器 (/etc/resolv.conf). 解析外部域名 coredns 默认会请求上游 DNS 来查询，这里的上游 DNS 默认是 coredns pod 所在宿主机的 resolv.conf 里面的 nameserver (coredns pod 的 dnsPolicy 为 “Default”，也就是会将宿主机里的 resolv.conf 里的 nameserver 加到容器里的 resolv.conf, coredns 默认配置 proxy . /etc/resolv.conf, 意思是非 service 域名会使用 coredns 容器中 resolv.conf 文件里的 nameserver 来解析) POD里面访问集群外的域名 走的是上游DNS服务器，也就是coredns所在的CVM节点DNS去做解析， 会先在coredns 的search域里面查找，查找不到走的是上游DNS服务器 外部域名请求的一个流程 路由请求流程 未配置存根域：没有匹配上配置的集群域名后缀的任何请求，例如 “www.kubernetes.io”，将会被转发到继承自节点的上游域名服务器。 已配置存根域：如果配置了存根域和上游DNS服务器，DNS查询将基于下面的流程对请求进行路由： \\1. 查询首先被发送到coredns中的DNS缓存层。 \\2. 从缓存层，检查请求的后缀，并根据下面的情况转发到对应的DNS上： - 具有集群后缀的名字（例如“.cluster.local”）：请求被发送到coredns。 - 具有存根域后缀的名字（例如“.acme.local”）：请求被发送到配置的自定义DNS解析器（例如：监听在 1.2.3.4）。 - 未能匹配上后缀的名字（例如“widget.com”）：请求被转发到上游DNS。 上游DNS解析是 随机的策略 https://github.com/coredns/coredns/blob/29f6d0a6b20e9f3e7509867d932f11efd443c72b/plugin/forward/README.md 默认是随机策略 cache：启用前端缓存，需要指出的是，上文中的cache 30，表示 pttl 为30s，也就是说，一条域名解析记录在DNS缓存中的存留时间最长为30s。 loop：检测到简单的转发环，如果发现死循环，则中止 CoreDNS 进程。 reload：允许自动重新加载已更改的 Corefile。 编辑 ConfigMap 配置后，请等待两分钟，以使更改生效。 loadbalance：这是一个轮转式 DNS 负载均衡器， 它在应答中随机分配 A、AAAA 和 MX 记录的顺序。 CoreDNS内置的两个健康检查插件health和ready的使用方式和适用场景 ：https://tinychen.com/20220728-dns-11-coredns-08-healthcheck/ 路由请求流程 未配置存根域：没有匹配上配置的集群域名后缀的任何请求，例如 “www.kubernetes.io”，将会被转发到继承自节点的上游域名服务器。 已配置存根域：如果配置了存根域和上游DNS服务器，DNS查询将基于下面的流程对请求进行路由： 查询首先被发送到coredns中的DNS缓存层。 从缓存层，检查请求的后缀，并根据下面的情况转发到对应的DNS上： 具有集群后缀的名字（例如“.cluster.local”）：请求被发送到coredns。 具有存根域后缀的名字（例如“.acme.local”）：请求被发送到配置的自定义DNS解析器（例如：监听在 1.2.3.4）。 未能匹配上后缀的名字（例如“widget.com”）：请求被转发到上游DNS。 DNS域名解析原理 节点kubelet的启动参数有--cluster-dns=、--cluster-domain=，这两个参数分别被用来设置集群DNS服务器的IP地址和主域名后缀。 Pod内的DNS域名解析配置文件为/etc/resolv.conf，文件内容如下。 earch default.svc.cluster.local svc.cluster.local cluster.local nameserver 172.18.254.219 options ndots:5 search：设置域名的查找后缀规则，查找配置越多，说明域名解析查找匹配次数越多，集群匹配有default.svc.cluster.local、svc.cluster.local、cluster.local3个后缀，最多进行8次查询才能得到正确解析结果，因为集群里面进行IPV4和IPV6查询各四次 nameserver：定义DNS服务器的IP地址 options：定义域名解析配置文件选项，支持多个KV值。例如该参数设置成ndots:5，说明如果访问的域名字符串内的点字符数量超过ndots值，则认为是完整域名，并被直接解析；如果不足ndots值，则追加search段后缀再进行查询 集群dnsPolicy配置 Kubernetes持通过dnsPolicy字段为每个Pod配置不同的DNS策略。目前支持四种策略： ClusterFirst：通过CoreDNS来做域名解析，Pod内/etc/resolv.conf配置的DNS服务地址是集群DNS服务的kube-dns地址。该策略是集群工作负载的默认策略。 None：忽略集群DNS策略，需要您提供dnsConfig字段来指定DNS配置信息。 Default：Pod直接继承集群节点的域名解析配置。即在ACK集群直接使用ECS的/etc/resolv.conf文件（文件内配置的是阿里云DNS服务）。 ClusterFirstWithHostNet：强制在hostNetWork网络模式下使用ClusterFirst策略（默认使用Default策略） Pod层面自定义DNS配置 apiVersion: v1 kind: Pod metadata: name: alpine namespace: default spec: containers: - image: alpine command: - sleep - \"10000\" imagePullPolicy: Always name: alpine dnsPolicy: None dnsConfig: nameservers: [\"169.254.xx.xx\"] searches: - default.svc.cluster.local - svc.cluster.local - cluster.local options: - name: ndots value: \"2\" 集群CoreDNS扩展配置 场景一：开启日志服务 如果需将CoreDNS每次域名解析的日志打印出来，您可以开启log插件，在Corefile里加上log。示例配置如下： Corefile: |2- .:53 { errors log health kubernetes cluster.local. in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 reload loadbalance } 场景二：配置外部上游dns服务器 有些服务不在kubernetes内部，在内部环境内需要通过dns去访问,名称后缀为carey.com ，强制所有非集群 DNS 查找通过特定的域名服务器（位于10.150.0.1），将 proxy 和 forward 指向域名服务器，而不是 /etc/resolv.conf。 carey.com:53 { errors cache 30 forward . 10.150.0.1 } 要显式强制所有非集群 DNS 查找通过特定的域名服务器（位于 172.16.0.1），可将 forward 指向该域名服务器，而不是 /etc/resolv.conf。 完整的配置文件 Corefile: |2- .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } domain-name:53 { errors cache 30 forward . custom-dns-server reload #前面的域要加上reload } carey.com:53 { errors cache 30 forward . 10.150.0.1 } 场景三：通过coredns实现内外流量分离 场景 旧业务固定了域名，无法通过内部service直接访问服务 需要实现内部流量和外部流量自动拆分 实现 通过coredns的rewrite功能实现以上能力,如以下内部访问tenant.msa.chinamcloud.com域名时，会将流量转发到 tenantapi.yunjiao.svc.cluster.local域名，实现内外域名访问一致。 部分版本nginx配置时候可能遇见无法访问的情况 apiVersion: v1 data: Corefile: |2- .:53 { errors health rewrite name tenant.msa.chinamcloud.com tenantapi.yunjiao.svc.cluster.local rewrite name console.msa.chinamcloud.com console.yunjiao.svc.cluster.local rewrite name user.msa.chinamcloud.com userapi.yunjiao.svc.cluster.local rewrite name lims.msa.chinamcloud.com lims.yunjiao.svc.cluster.local rewrite name labapp.msa.chinamcloud.com limsapp.yunjiao.svc.cluster.local kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } kind: ConfigMap metadata: name: coredns namespace: kube-system 场景四：coredns的hosts特性声明 hosts ：字段部分指明了三个域名的解析地址 ， hosts是 CoreDNS 的一个 plugin，这一节的意思是加载 /etc/hosts文件里面的解析信息。hosts 在最前面，则如果一个域名在 hosts 文件中存在，则优先使用这个信息返回； fallthrough ：如果 hosts 中找不到，则进入下一个 plugin 继续。缺少这一个指令，后面的 plugins 配置就无意义了； 注意 请配置fallthrough，否则会造成非定制hosts域名解析失败。 apiVersion: v1 data: Corefile: | .:53 { errors health hosts { 100.64.139.66 minio.chinamcloud.com 100.64.139.66 registry.chinamcloud.com 100.64.139.66 gitlab.chinamcloud.com fallthrough } kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } kind: ConfigMap metadata: name: coredns namespace: kube-system 场景五：外部域名完全使用自建DNS服务器 如果您需要使用的自建DNS服务的域名没有统一的域名后缀，您可以选择所有集群外部域名走自建DNS服务器，例如，您自建的DNS服务器IP为10.10.0.10和10.10.0.20，可以更改forward参数进行配置。示例配置如下 Corefile: |2- .:53 { errors health kubernetes cluster.local. in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 10.10.0.10 10.10.0.20 ###### cache 30 reload loadbalance } 场景六：集群外部访问集群内服务 如果您希望运行在集群节点上的进程能够访问到集群内的服务，虽然可以通过将节点的/etc/resolv.conf文件内nameserver配置为集群kube-dns的ClusterIP地址来达到目的，但不推荐您直接更改节点的/etc/resolv.conf文件的方式来达到任何目的 场景七：禁止CoreDNS对IPv6类型的AAAA记录查询返回 当业务容器不需要AAAA记录类型时，可以在CoreDNS中将AAAA记录类型拦截，返回域名不存在，以减少不必要的网络通信。示例配置如下： Corefile: | .:53 { errors health { lameduck 15s } #新增以下一行Template插件，其它数据请保持不变。 template IN AAAA . } 场景八：local dns配置外部域名 修改kube-system下的local dns 的configmap kubectl edit cm node-local-dns -nkube-system 增加11行： tencentyun.com:53 { errors cache 30 reload loop bind 169.254.20.10 COREDNS_CLUSTER_IP forward . __PILLAR__CLUSTER__DNS__ { force_tcp } prometheus :9253 } ​ ipvs在使用node-local-dns-dnscache缺陷导致访问链路异常 规避措施： node-local-dns 不要绑定 kube-dns service IP kubelet 增加命令行参数 --cluster-dns=169.254.20.10 参考文档：https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#coredns https://github.com/coredns/coredns/blob/master/plugin/kubernetes/README.md https://kubernetes.io/zh/docs/tasks/administer-cluster/dns-custom-nameservers/ Coredns+Nodelocaldns cache解决Coredns域名解析延迟 https://blog.51cto.com/u_14143894/2515451 CoreDNS 详情 https://blog.51cto.com/u_14299052/3104596 "},"docs/Kubernetes/docker-roor-dir.html":{"url":"docs/Kubernetes/docker-roor-dir.html","title":"Docker目录被写满","keywords":"","body":"容器数据磁盘被写满 造成的危害 不能创建 Pod (一直 ContainerCreating) 不能删除 Pod (一直 Terminating) 判断是否被写满 容器数据目录大多会单独挂数据盘，路径一般是 /var/lib/docker，也可能是 /data/docker 或 /opt/docker，取决于节点被添加时的配置： 可通过 docker info 确定： $ docker info ... Docker Root Dir: /var/lib/docker ... 如果没有单独挂数据盘，则会使用系统盘存储。判断是否被写满： $ df Filesystem 1K-blocks Used Available Use% Mounted on ... /dev/vda1 51474044 4619112 44233548 10% / ... /dev/vdb 20511356 20511356 0 100% /var/lib/docker 解决方法 先恢复业务，清理磁盘空间 重启 dockerd (清理容器日志输出和可写层文件) 重启前需要稍微腾出一点空间，不然重启 docker 会失败，可以手动删除一些docker的log文件或可写层文件，通常删除log: $ cd /var/lib/docker/containers $ du -sh * # 找到比较大的目录 $ cd dda02c9a7491fa797ab730c1568ba06cba74cecd4e4a82e9d90d00fa11de743c $ cat /dev/null > dda02c9a7491fa797ab730c1568ba06cba74cecd4e4a82e9d90d00fa11de743c-json.log.9 # 删除log文件 注意: 使用 cat /dev/null > 式删除而不用 rm 因为用 rm 删除的文件，docker 进程可能不会释放文件，空间也就不会释放；log 的后缀数字越大表示越久远，先删除旧日志。 将该 node 标记不可调度，并将其已有的 pod 驱逐到其它节点，这样重启dockerd就会让该节点的pod对应的容器删掉，容器相关的日志(标准输出)与容器内产生的数据文件(可写层)也会被清理： kubectl drain 10.179.80.31 重启 dockerd: systemctl restart dockerd 取消不可调度的标记: kubectl uncordon 10.179.80.31 定位根因，彻底解决 问题定位方法见附录，这里列举根因对应的解决方法： 日志输出量大导致磁盘写满: 减少日志输出 增大磁盘空间 减小单机可调度的pod数量 可写层量大导致磁盘写满: 优化程序逻辑，不写文件到容器内或控制写入文件的大小与数量 镜像占用空间大导致磁盘写满: 增大磁盘空间 删除不需要的镜像 附录 查看docker的磁盘空间占用情况 $ docker system df -v 定位容器写满磁盘的原因 进入容器数据目录(假设是 /var/lib/docker，并且存储驱动是 aufs): $ cd /var/lib/docker $ du -sh * 20K builder 72K buildkit 1.1G containers 12M image 100K network 12G overlay2 20K plugins 4.0K runtimes 4.0K swarm 4.0K tmp 4.0K trust 28K volumes containers 目录: 体积大说明日志标准输出量大 [root@v-cjweichen-mesh /var/lib/docker/containers]# du -sh * 17M 01d8566ea348bd333b0d971a1751335b32f0d97396352e63a2dc25fdc7360a19 32K 07533d0f9e45ee03f6f22117326de17e2c9e0712a1e265d3a778dec1329f25e7 120K 076f585dc0f7532ab567743b7066d2803e2fbc698fa33f769e7f61002f450a3a 28K 0777cff7c65a4594df7c612eca7dac0ec6ddea2ffe70a8b118830150b062a468 28K 44565163715c8cdfe355892d7400f1836a86f9df27fb32adf32f5e41d7ea0531 目录名即为容器id，可以根据前12位 进行模糊匹配，就等查到对应的POD 信息 如： #docker ps | grep 01d8566ea348 01d8566ea348 ccr.ccs.tencentyun.com/tkeimages/tke-bridge-agent \"/install-cni.sh --c…\" 2 weeks ago Up 2 weeks k8s_tke-bridge-agent_tke-bridge-agent-qbbmn_kube-system_3b66c838-57c7-43a1-9281-8899c8b8b264_1 #docker ps | grep 445651637 44565163715c nginx \"/docker-entrypoint.…\" 2 days ago Up 2 days k8s_nginx_nginx-v1-6dc49fbd58-v2ntt_default_eed1f780-0d02-4788-9f45-8c94a2a58435_0 overlay2 容器里面的文件系统rootfs在宿主机的路径规则 [root@mesh /var/lib/docker/overlay2]# du -sh * 36K 005fb0cee0261e7ab9201986ede339f5e1ea9f4d36efb5535c2451ecdb69488c 91M 02973b91425e166ac4a6f096ab37e4af41fb2fcf7e658e809ddaf7698caa73b7 32K 03cb6d1267a971930cdf50dcef86c69b3f92d16f0fc56a19b637e192c3bb57f7 720K 07e9f8b0c251d3e6d01551622bd2c4994a638d65cba9a7073a61a3273b13a20a 48K 07e9f8b0c251d3e6d01551622bd2c4994a638d65cba9a7073a61a3273b13a20a-init 163M 09ae9b4457a0ed3eec58c129d524345855ab20bdb18560f9c7d506da3c48d121 #目录名即为mount_id, 可以通过/image/overlay2/layerdb/mounts/container_id/mount-id查询 cd 09ae9b4457a0ed3eec58c129d524345855ab20bdb18560f9c7d506da3c48d121 [root@mesh /var/lib/docker/overlay2/09ae9b4457a0ed3eec58c129d524345855ab20bdb18560f9c7d506da3c48d121]# du -sh * 100K diff #diff子目录: 容器可写层，体积大说明可写层数据量大(程序在容器里写入文件) 4.0K link 4.0K lower 163M merged #联合挂载点，内容为容器里看到的内容，即包含镜像本身内容以及可写层内容 8.0K work 找出日志输出量大的 pod TKE 的 pod 中每个容器输出的日志最大存储 1G (日志轮转，最大10个文件，每个文件最大100m，可用 docker inpect 查看): $ docker inspect fef835ebfc88 [ { ... \"HostConfig\": { ... \"LogConfig\": { \"Type\": \"json-file\", \"Config\": { \"max-file\": \"10\", \"max-size\": \"100m\" } }, ... 查看哪些容器日志输出量大： $ cd /var/lib/docker/containers $ du -sh * 目录名即为容器id，使用前几位与 docker ps 结果匹配可找出对应容器，最后就可以推算出是哪些 pod 搞的鬼 找出可写层数据量大的 pod 可写层的数据主要是容器内程序自身写入的，无法控制大小，可写层越大说明容器写入的文件越多或越大，通常是容器内程序将log写到文件里了，查看一下哪个容器的可写层数据量大： $ cd /var/lib/docker/overlay2/diff $ du -sh * 通过可写层目录(diff的子目录)反查容器id: $ grep 834d97500892f56b24c6e63ffd4e520fc29c6c0d809a3472055116f59fb1d2be /var/lib/docker/image/aufs/layerdb/mounts/*/mount-id /var/lib/docker/image/aufs/layerdb/mounts/eb76fcd31dfbe5fc949b67e4ad717e002847d15334791715ff7d96bb2c8785f9/mount-id:834d97500892f56b24c6e63ffd4e520fc29c6c0d809a3472055116f59fb1d2be mounts 后面一级的id即为容器id: eb76fcd31dfbe5fc949b67e4ad717e002847d15334791715ff7d96bb2c8785f9，使用前几位与 docker ps 结果匹配可找出对应容器，最后就可以推算出是哪些 pod 搞的鬼 找出体积大的镜像 Docker 作为 K8S 容器运行时，容器日志的落盘将由 docker 来完成，保存在类似/var/lib/docker/containers/$CONTAINERID 目录下。Kubelet 会在 /var/log/pods 和 /var/log/containers 下面建立软链接，指向 /var/lib/docker/containers/$CONTAINERID 该目录下的容器日志文件 如果 Containerd 作为 K8S 容器运行时， 容器日志的落盘由 Kubelet 来完成，保存至 /var/log/pods/$CONTAINER_NAME 目录下，同时在 /var/log/containers 目录下创建软链接，指向日志文件 "},"docs/Kubernetes/docker-start-cmd.html":{"url":"docs/Kubernetes/docker-start-cmd.html","title":"Docker启动命令和参数","keywords":"","body":"创建 Pod 时设置命令及参数 创建 Pod 时，可以为其下的容器设置启动时要执行的命令及其参数。如果要设置命令，就填写在配置文件的 command 字段下，如果要设置命令的参数，就填写在配置文件的 args 字段下。一旦 Pod 创建完成，该命令及其参数就无法再进行更改了。 如果在配置文件中设置了容器启动时要执行的命令及其参数，那么容器镜像中自带的命令与参数将会被覆盖而不再执行。如果配置文件中只是设置了参数，却没有设置其对应的命令，那么容器镜像中自带的命令会使用该新参数作为其执行时的参数。 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: command-demo name: command-demo namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: command-demo template: metadata: labels: k8s-app: command-demo spec: containers: - args: - HOSTNAME - KUBERNETES_PORT command: - printenv image: debian imagePullPolicy: IfNotPresent name: command-demo-container resources: {} securityContext: privileged: false dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Alway 在 Shell 来执行命令 有时候，你需要在 Shell 脚本中运行命令。 例如，你要执行的命令可能由多个命令组合而成，或者它就是一个 Shell 脚本。 这时，就可以通过如下方式在 Shell 中执行命令： command: [\"/bin/sh\"] args: [\"-c\", \"while true; do echo hello; sleep 10;done\"] --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: command-demo name: command-demo namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: command-demo template: metadata: labels: k8s-app: command-demo spec: containers: - args: [\"-c\", \"while true; do echo hello; sleep 10;done\"] command: [\"/bin/sh\"] image: debian imagePullPolicy: IfNotPresent name: command-demo-container resources: {} securityContext: privileged: false dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always 说明事项 下表给出了 Docker 与 Kubernetes 中对应的字段名称。 描述 Docker 字段名称 Kubernetes 字段名称 容器执行的命令 Entrypoint command 传给命令的参数 Cmd args 如果要覆盖默认的 Entrypoint 与 Cmd，需要遵循如下规则： 如果在容器配置中没有设置 command 或者 args，那么将使用 Docker 镜像自带的命令及其参数。 如果在容器配置中只设置了 command 但是没有设置 args，那么容器启动时只会执行该命令， Docker 镜像中自带的命令及其参数会被忽略。 如果在容器配置中只设置了 args，那么 Docker 镜像中自带的命令会使用该新参数作为其执行时的参数。 如果在容器配置中同时设置了 command 与 args，那么 Docker 镜像中自带的命令及其参数会被忽略。 容器启动时只会执行配置中设置的命令，并使用配置中设置的参数作为命令的参数。 下面是一些例子： 镜像 Entrypoint 镜像 Cmd 容器 command 容器 args 命令执行 [/ep-1] [foo bar] [ep-1 foo bar] [/ep-1] [foo bar] [/ep-2] [ep-2] [/ep-1] [foo bar] [zoo boo] [ep-1 zoo boo] [/ep-1] [foo bar] [/ep-2] [zoo boo] [ep-2 zoo boo] 参考链接：https://kubernetes.io/zh/docs/tasks/inject-data-application/define-command-argument-container/#notes "},"docs/Kubernetes/hostport-hostnetwork-diff.html":{"url":"docs/Kubernetes/hostport-hostnetwork-diff.html","title":"hostPort与hostNetwork异同","keywords":"","body":"hostPort示例 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: nginx-hostport qcloud-app: nginx-hostport name: nginx-hostport namespace: cjweichen spec: replicas: 1 selector: matchLabels: k8s-app: nginx-hostport qcloud-app: nginx-hostport template: metadata: labels: k8s-app: nginx-hostport qcloud-app: nginx-hostport spec: containers: - image: nginx imagePullPolicy: Always name: nginx ports: - containerPort: 80 hostPort: 8000 name: http protocol: TCP - containerPort: 443 hostPort: 44300 name: https protocol: TCP hostPort 与 hostNetwork 异同 相同点 ​ hostPort 与 hostNetwork 本质上都是暴露 pod 宿主机 IP 给终端用户，因为 pod 生命周期并不固定，随时都有可能异常重建，故 IP 的不确定最终导致用户使用上的不方便；此外宿主机端口占用也导致不能在同一台机子上有多个程序使用同一端口。因此一般情况下，不要使用 hostPort 方式。 不同点 ​ 使用 hostNetwork，pod 实际上用的是 pod 宿主机的网络地址空间：即 pod IP 是宿主机 IP，而非 cni 分配的 pod IP，端口是宿主机网络监听接口。 使用 hostPort，pod IP 并非宿主机 IP，而是 cni 分配的 pod IP，跟其他普通的 pod 使用一样的 ip 分配方式，端口并非宿主机网络监听端口，只是使用了 DNAT 机制将 hostPort 指定的端口映射到了容器的端口之上（可以通过 iptables 命令进行查看）。外部访问此 pod 时，仍然使用宿主机和 hostPort 方式。pod ip 跟宿主机 ip 截图如 有关端口 DNAT 通过 iptables 命令进行查看，如下截图所示： iptables -nvL -t nat | grep -C 5 DNAT 由上图可知，pod 所在宿主机上的 iptables nat 表流向如下： 1 当客户端发起 pod 访问时，比如 curl http://pod_in_host:hostPort 2 网络包会流经 pod 宿主机的 prerouting chain，会命中 CNI-HOSTPORT-DNAT 链 3 网络包会流经 CNI-HOSTPORT_DNAT 链中的第 3 条规则，即 DNAT 目标，此时会将 9998 端口访问的流量路由到 80 端口去 基于此，当客户端访问 pod 所在主机的 8000 端口时，流量会自动被路由到 IP 为 10.55.3.5（也就是 pod ip）的 80 端口上。 ​ 当 pod 同时使用了 hostNetwork 和 hostPort，那么 hostNetwork 将会直接使用宿主机网络命名空间，hostPort 其实就形同虚设了。可以认为 hostNetwork 选项优先级要高于 hostPort。 参考文档： https://blog.51cto.com/u_14625168/2489160 "},"docs/Kubernetes/k8s-deploy-elk-log.html":{"url":"docs/Kubernetes/k8s-deploy-elk-log.html","title":"k8s搭建EFK日志收集","keywords":"","body":"Kubernetes 中比较流行的日志收集解决方案是 Elasticsearch、Fluentd 和 Kibana（EFK）技术栈，也是官方现在比较推荐的一种方案。 Elasticsearch 是一个实时的、分布式的可扩展的搜索引擎，允许进行全文、结构化搜索，它通常用于索引和搜索大量日志数据，也可用于搜索许多不同类型的文档。 Elasticsearch 通常与 Kibana 一起部署，Kibana 是 Elasticsearch 的一个功能强大的数据可视化 Dashboard，Kibana 允许你通过 web 界面来浏览 Elasticsearch 日志数据。 Fluentd是一个流行的开源数据收集器，我们将在 Kubernetes 集群节点上安装 Fluentd，通过获取容器日志文件、过滤和转换日志数据，然后将数据传递到 Elasticsearch 集群，在该集群中对其进行索引和存储。 我们先来配置启动一个可扩展的 Elasticsearch 集群，然后在 Kubernetes 集群中创建一个 Kibana 应用，最后通过 DaemonSet 来运行 Fluentd，以便它在每个 Kubernetes 工作节点上都可以运行一个 Pod。 如果你了解 EFK 的基本原理，只是为了测试可以直接使用 Kubernetes 官方提供的 addon 插件的资源清单，地址：https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/，直接安装即可。 创建 Elasticsearch 集群 集群环境准备： 1，部署组件时候设置有nodeSelector kubectl label nodes node名 es=log kubectl label nodes node名 beta.kubernetes.io/fluentd-ds-ready=true 2，下载镜像上传到自己的镜像仓库 Elasticsearch 镜像：docker.elastic.co/elasticsearch/elasticsearch:7.6.2 Kibana 镜像：docker.elastic.co/kibana/kibana:7.6.2 Fluentd 镜像：quay.io/fluentd_elasticsearch/fluentd:v3.0.1 elastalert 镜像：jertel/elastalert-docker:0.2.4 Rook Ceph 镜像：rook/ceph:v1.2.1 tcr-chen.tencentcloudcr.com/efk/elastalert-docker:0.2.4 tcr-chen.tencentcloudcr.com/efk/fluentd_elasticsearch/fluentd:v3.0.1 tcr-chen.tencentcloudcr.com/efk/kibana/kibana:7.6.2 tcr-chen.tencentcloudcr.com/efk/elasticsearch/elasticsearch:7.6.2 tcr-chen.tencentcloudcr.com/efk/ceph:1.2.1 在创建 Elasticsearch 集群之前，我们先创建一个命名空间，我们将在其中安装所有日志相关的资源对象。 新建一个 logging-namespace.yaml 文件： apiVersion: v1 kind: Namespace metadata: name: logging 然后通过 kubectl 创建该资源清单，创建一个名为 logging 的 namespace： #kubectl apply -f kube-logging.yaml namespace/logging created # kubectl get ns NAME STATUS AGE default Active 21d kube-node-lease Active 21d kube-public Active 21d kube-system Active 21d logging Active 5s 现在创建了一个命名空间来存放我们的日志相关资源，接下来可以部署 EFK 相关组件，首先开始部署一个3节点的 Elasticsearch 集群。 这里我们使用3个 Elasticsearch Pod 来避免高可用下多节点集群中出现的“脑裂”问题，当一个或多个节点无法与其他节点通信时会产生“脑裂”，可能会出现几个主节点。 了解更多 Elasticsearch 集群脑裂问题，可以查看文档https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#split-brain 一个关键点是您应该设置参数discover.zen.minimum_master_nodes=N/2+1，其中N是 Elasticsearch 集群中符合主节点的节点数，比如我们这里3个节点，意味着N应该设置为2。这样，如果一个节点暂时与集群断开连接，则另外两个节点可以选择一个新的主节点，并且集群可以在最后一个节点尝试重新加入时继续运行，在扩展 Elasticsearch 集群时，一定要记住这个参数。 先创建一个名为 elasticsearch 的无头服务，新建文件 elasticsearch-svc.yaml，文件内容如下： kind: Service apiVersion: v1 metadata: name: elasticsearch namespace: logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node 定义了一个名为 elasticsearch 的 Service，指定标签 app=elasticsearch，当我们将 Elasticsearch StatefulSet 与此服务关联时，服务将返回带有标签 app=elasticsearch的 Elasticsearch Pods 的 DNS A 记录，然后设置 clusterIP=None，将该服务设置成无头服务。最后，我们分别定义端口9200、9300，分别用于与 REST API 交互，以及用于节点间通信。 使用 kubectl 直接创建上面的服务资源对象： # kubectl apply -f elasticsearch-svc.yaml service/elasticsearch created # kubectl get svc -n logging NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP None 9200/TCP,9300/TCP 81s 现在我们已经为 Pod 设置了无头服务和一个稳定的域名.elasticsearch.logging.svc.cluster.local，接下来我们通过 StatefulSet 来创建具体的 Elasticsearch 的 Pod 应用。 Kubernetes StatefulSet 允许我们为 Pod 分配一个稳定的标识和持久化存储，Elasticsearch 需要稳定的存储来保证 Pod 在重新调度或者重启后的数据依然不变，所以需要使用 StatefulSet 来管理 Pod。 新建名为 elasticsearch-statefulset.yaml 的资源清单文件，首先粘贴下面内容： apiVersion: apps/v1 kind: StatefulSet metadata: name: es namespace: logging spec: serviceName: elasticsearch replicas: 3 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch 该内容中，我们定义了一个名为 es 的 StatefulSet 对象，然后定义serviceName=elasticsearch和前面创建的 Service 相关联，这可以确保使用以下 DNS 地址访问 StatefulSet 中的每一个 Pod：es-[0,1,2].elasticsearch.logging.svc.cluster.local，其中[0,1,2]对应于已分配的 Pod 序号。 然后指定3个副本，将 matchLabels 设置为app=elasticsearch，所以 Pod 的模板部分.spec.template.metadata.lables也必须包含app=elasticsearch标签。 然后定义 Pod 模板部分内容： ... spec: containers: - name: elasticsearch image: tcr-chen.tencentcloudcr.com/efk/elasticsearch/elasticsearch:7.6.2 resources: limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: rest protocol: TCP - containerPort: 9300 name: inter-node protocol: TCP volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: cluster.initial_master_nodes value: \"es-0,es-1,es-2\" - name: discovery.zen.minimum_master_nodes value: \"2\" - name: discovery.seed_hosts value: \"elasticsearch\" - name: ES_JAVA_OPTS value: \"-Xms512m -Xmx512m\" - name: network.host value: \"0.0.0.0\" 该部分是定义 StatefulSet 中的 Pod，暴露了9200和9300两个端口，注意名称要和上面定义的 Service 保持一致。然后通过 volumeMount 声明了数据持久化目录，下面我们再来定义 VolumeClaims。最后就是我们在容器中设置的一些环境变量了： cluster.name：Elasticsearch 集群的名称，我们这里命名成 k8s-logs。 node.name：节点的名称，通过 metadata.name 来获取。这将解析为 es-[0,1,2]，取决于节点的指定顺序。 discovery.seed_hosts：此字段用于设置在 Elasticsearch 集群中节点相互连接的发现方法。由于我们之前配置的无头服务，我们的 Pod 具有唯一的 DNS 域es-[0,1,2].elasticsearch.logging.svc.cluster.local，因此我们相应地设置此变量。要了解有关 Elasticsearch 发现的更多信息，请参阅 Elasticsearch 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html。 discovery.zen.minimum_master_nodes：我们将其设置为(N/2) + 1，N是我们的群集中符合主节点的节点的数量。我们有3个 Elasticsearch 节点，因此我们将此值设置为2（向下舍入到最接近的整数）。要了解有关此参数的更多信息，请参阅官方 Elasticsearch 文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#split-brain。 ES_JAVA_OPTS：这里我们设置为-Xms512m -Xmx512m，告诉JVM使用512 MB的最小和最大堆。您应该根据群集的资源可用性和需求调整这些参数。要了解更多信息，请参阅设置堆大小的相关文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html。 接下来添加关于 initContainer 的内容： ... initContainers: - name: increase-vm-max-map image: busybox command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\"sh\", \"-c\", \"ulimit -n 65536\"] securityContext: privileged: true 这里我们定义了几个在主应用程序之前运行的 Init 容器，这些初始容器按照定义的顺序依次执行，执行完成后才会启动主应用容器。 第一个名为 increase-vm-max-map 的容器用来增加操作系统对mmap计数的限制，默认情况下该值可能太低，导致内存不足的错误，要了解更多关于该设置的信息，可以查看 Elasticsearch 官方文档说明：https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html。 最后一个初始化容器是用来执行ulimit命令增加打开文件描述符的最大数量的。 此外 Elastisearch Notes for Production Use 文档还提到了由于性能原因最好禁用 swap，当然对于 Kubernetes 集群而言，最好也是禁用 swap 分区的。 现在我们已经定义了主应用容器和它之前运行的 Init Containers 来调整一些必要的系统参数，接下来我们可以添加数据目录的持久化相关的配置，在 StatefulSet 中，使用 volumeClaimTemplates 来定义 volume 模板即可： ... volumeClaimTemplates: - metadata: name: data labels: app: elasticsearch spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: cbs resources: requests: storage: 10Gi 我们这里使用 volumeClaimTemplates 来定义持久化模板，Kubernetes 会使用它为 Pod 创建 PersistentVolume，设置访问模式为ReadWriteOnce，这意味着它只能被 mount 到单个节点上进行读写，然后最重要的是使用了一个 StorageClass 对象，这里我们就直接使用前面创建的 Ceph RBD 类型的名为 rook-ceph-block 的 StorageClass 对象即可。最后，我们指定了每个 PersistentVolume 的大小为 50GB，我们可以根据自己的实际需要进行调整该值。 完整的 Elasticsearch StatefulSet 资源清单文件内容如下：elasticsearch-statefulset.yaml apiVersion: apps/v1beta2 kind: StatefulSet metadata: name: es namespace: logging spec: podManagementPolicy: OrderedReady replicas: 3 revisionHistoryLimit: 10 selector: matchLabels: app: elasticsearch serviceName: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: cluster.initial_master_nodes value: es-0,es-1,es-2 - name: discovery.zen.minimum_master_nodes value: \"2\" - name: discovery.seed_hosts value: elasticsearch - name: ES_JAVA_OPTS value: -Xms512m -Xmx512m - name: network.host value: 0.0.0.0 image: tcr-chen.tencentcloudcr.com/efk/elasticsearch/elasticsearch:7.6.2 imagePullPolicy: IfNotPresent name: elasticsearch ports: - containerPort: 9200 name: rest protocol: TCP - containerPort: 9300 name: inter protocol: TCP resources: limits: cpu: \"1\" requests: cpu: 500m securityContext: privileged: false volumeMounts: - mountPath: /usr/share/elasticsearch/data name: data dnsPolicy: ClusterFirst initContainers: - command: - sysctl - -w - vm.max_map_count=262144 image: busybox:latest imagePullPolicy: Always name: increase-vm-max-map resources: {} securityContext: privileged: true - command: - sh - -c - ulimit -n 65536 image: busybox:latest imagePullPolicy: Always name: increase-fd-ulimit resources: {} securityContext: privileged: true - args: - -c - chmod 777 /mnt/data command: - /bin/sh image: centos:latest imagePullPolicy: Always name: chmod resources: {} securityContext: privileged: true volumeMounts: - mountPath: /mnt/data name: data nodeSelector: es: log volumeClaimTemplates: - metadata: labels: app: elasticsearch name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: cbs-csi 现在直接使用 kubectl 工具部署即可： $ kubectl create -f elasticsearch-statefulset.yaml statefulset.apps/es created 添加成功后，可以看到 logging 命名空间下面的所有的资源对象： $ kubectl get sts -n logging NAME READY AGE es 3/3 83m $ kubectl get pods -n logging NAME READY STATUS RESTARTS AGE es-0 1/1 Running 0 83m es-1 1/1 Running 0 82m es-2 1/1 Running 0 81m $ kubectl get svc -n logging NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP None 9200/TCP,9300/TCP 20h Pods 部署完成后，我们可以通过请求一个 REST API 来检查 Elasticsearch 集群是否正常运行。使用下面的命令将本地端口9200 转发到 Elasticsearch 节点（如es-0）对应的端口： $ kubectl port-forward es-0 9200:9200 -n logging Forwarding from 127.0.0.1:9200 -> 9200 Forwarding from [::1]:9200 -> 9200 然后，在另外的终端窗口中，执行如下请求： $ curl http://localhost:9200/_cluster/state?pretty 正常来说，应该会看到类似于如下的信息： { \"cluster_name\" : \"k8s-logs\", \"cluster_uuid\" : \"YIRfosw0Q_634rqz0vEZdQ\", \"version\" : 21, \"state_uuid\" : \"9kbcE2VzRJKogHyqcPpmrw\", \"master_node\" : \"avxp7IAHRNmDSIL78bIkhA\", \"blocks\" : { }, \"nodes\" : { \"4ccbIsnZTmGqSvnO-zniLg\" : { \"name\" : \"es-2\", \"ephemeral_id\" : \"yF8KNcwVSiqfT0KDVrH9_g\", \"transport_address\" : \"172.30.1.79:9300\", \"attributes\" : { \"ml.machine_memory\" : \"8093675520\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" } }, \"avxp7IAHRNmDSIL78bIkhA\" : { \"name\" : \"es-1\", \"ephemeral_id\" : \"OZ76WUkBSFSo3l1npsdOxQ\", \"transport_address\" : \"172.30.0.136:9300\", \"attributes\" : { \"ml.machine_memory\" : \"8093675520\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" } }, \"UB8YiMnaSGyIHd-giZhGFg\" : { \"name\" : \"es-0\", \"ephemeral_id\" : \"BVq0lZtMS7eCMNsO1qVhow\", \"transport_address\" : \"172.30.1.78:9300\", \"attributes\" : { \"ml.machine_memory\" : \"8093675520\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" } } }, 看到上面的信息就表明我们名为 k8s-logs 的 Elasticsearch 集群成功创建了3个节点：es-0，es-1，和es-2，当前主节点是 es-0。 创建 Kibana 服务 Elasticsearch 集群启动成功了，接下来我们可以来部署 Kibana 服务，新建一个名为 kibana.yaml 的文件，对应的文件内容如下： apiVersion: v1 kind: Service metadata: name: kibana namespace: logging labels: app: kibana spec: ports: - port: 5601 type: NodePort selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: logging labels: app: kibana spec: selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: nodeSelector: es: log containers: - name: kibana image: tcr-chen.tencentcloudcr.com/efk/kibana/kibana:7.6.2 resources: limits: cpu: 1000m requests: cpu: 500m env: - name: ELASTICSEARCH_HOSTS value: http://elasticsearch:9200 ports: - containerPort: 5601 上面我们定义了两个资源对象，一个 Service 和 Deployment，为了测试方便，我们将 Service 设置为了 NodePort 类型，Kibana Pod 中配置都比较简单，唯一需要注意的是我们使用 ELASTICSEARCH_HOSTS 这个环境变量来设置Elasticsearch 集群的端点和端口，直接使用 Kubernetes DNS 即可，此端点对应服务名称为 elasticsearch，由于是一个 headless service，所以该域将解析为3个 Elasticsearch Pod 的 IP 地址列表。 配置完成后，直接使用 kubectl 工具创建： $ kubectl create -f kibana.yaml service/kibana created deployment.apps/kibana created 创建完成后，可以查看 Kibana Pod 的运行状态： # kubectl get pods -n logging NAME READY STATUS RESTARTS AGE es-0 1/1 Running 0 14m es-1 1/1 Running 0 14m es-2 1/1 Running 0 13m kibana-649578fc57-dw9qk 1/1 Running 0 91s # kubectl get svc -n logging NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP None 9200/TCP,9300/TCP 143m kibana NodePort 172.30.254.50 5601:31784/TCP 98s 如果 Pod 已经是 Running 状态了，证明应用已经部署成功了，然后可以通过 NodePort 来访问 Kibana 这个服务，在浏览器中打开http://:31784即可，如果看到如下欢迎界面证明 Kibana 已经成功部署到了 Kubernetes集群之中。 部署 Fluentd Fluentd 是一个高效的日志聚合器，是用 Ruby 编写的，并且可以很好地扩展。对于大部分企业来说，Fluentd 足够高效并且消耗的资源相对较少，另外一个工具Fluent-bit更轻量级，占用资源更少，但是插件相对 Fluentd 来说不够丰富，所以整体来说，Fluentd 更加成熟，使用更加广泛，所以我们这里也同样使用 Fluentd 来作为日志收集工具。 工作原理 Fluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储等等。Fluentd 支持超过300个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下： 首先 Fluentd 从多个日志源获取数据 结构化并且标记这些数据 然后根据匹配的标签将数据发送到多个目标服务去 配置 一般来说我们是通过一个配置文件来告诉 Fluentd 如何采集、处理数据的，下面简单和大家介绍下 Fluentd 的配置方法。 日志源配置 比如我们这里为了收集 Kubernetes 节点上的所有容器日志，就需要做如下的日志源配置： @id fluentd-containers.log @type tail # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志。 path /var/log/containers/*.log # 挂载的服务器Docker容器日志地址 pos_file /var/log/es-containers.log.pos tag raw.kubernetes.* # 设置日志标签 read_from_head true # 多行格式化成JSON @type multi_format # 使用 multi-format-parser 解析器插件 format json # JSON 解析器 time_key time # 指定事件时间的时间字段 time_format %Y-%m-%dT%H:%M:%S.%NZ # 时间格式 format /^(?.+) (?stdout|stderr) [^ ]* (?.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z 上面配置部分参数说明如下： id：表示引用该日志源的唯一标识符，该标识可用于进一步过滤和路由结构化日志数据 type：Fluentd 内置的指令，tail 表示 Fluentd 从上次读取的位置通过 tail 不断获取数据，另外一个是 http 表示通过一个 GET 请求来收集数据。 path：tail 类型下的特定参数，告诉 Fluentd 采集 /var/log/containers 目录下的所有日志，这是 docker 在 Kubernetes 节点上用来存储运行容器 stdout 输出日志数据的目录。 pos_file：检查点，如果 Fluentd 程序重新启动了，它将使用此文件中的位置来恢复日志数据收集。 tag：用来将日志源与目标或者过滤器匹配的自定义字符串，Fluentd 匹配源/目标标签来路由日志数据。 路由配置 上面是日志源的配置，接下来看看如何将日志数据发送到 Elasticsearch： @id elasticsearch @type elasticsearch @log_level info include_tag_key true type_name fluentd host \"#{ENV['OUTPUT_HOST']}\" port \"#{ENV['OUTPUT_PORT']}\" logstash_format true @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size \"#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}\" queue_limit_length \"#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}\" overflow_action block match：标识一个目标标签，后面是一个匹配日志源的正则表达式，我们这里想要捕获所有的日志并将它们发送给 Elasticsearch，所以需要配置成**。 id：目标的一个唯一标识符。 type：支持的输出插件标识符，我们这里要输出到 Elasticsearch，所以配置成 elasticsearch，这是 Fluentd 的一个内置插件。 log_level：指定要捕获的日志级别，我们这里配置成 info，表示任何该级别或者该级别以上（INFO、WARNING、ERROR）的日志都将被路由到 Elsasticsearch。 host/port：定义 Elasticsearch 的地址，也可以配置认证信息，我们的 Elasticsearch 不需要认证，所以这里直接指定 host 和 port 即可。 logstash_format：Elasticsearch 服务对日志数据构建反向索引进行搜索，将 logstash_format 设置为 true，Fluentd 将会以 logstash 格式来转发结构化的日志数据。 Buffer： Fluentd 允许在目标不可用时进行缓存，比如，如果网络出现故障或者 Elasticsearch 不可用的时候。缓冲区配置也有助于降低磁盘的 IO。 过滤 由于 Kubernetes 集群中应用太多，也还有很多历史数据，所以我们可以只将某些应用的日志进行收集，比如我们只采集具有 logging=true 这个 Label 标签的 Pod 日志，这个时候就需要使用 filter，如下所示： # 删除无用的属性 @type record_transformer remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash # 只保留具有logging=true标签的Pod日志 @id filter_log @type grep key $.kubernetes.labels.logging pattern ^true$ 安装 要收集 Kubernetes 集群的日志，直接用 DasemonSet 控制器来部署 Fluentd 应用，这样，它就可以从 Kubernetes 节点上采集日志，确保在集群中的每个节点上始终运行一个 Fluentd 容器。当然可以直接使用 Helm 来进行一键安装，为了能够了解更多实现细节，我们这里还是采用手动方法来进行安装。 首先，我们通过 ConfigMap 对象来指定 Fluentd 配置文件，新建 fluentd-configmap.yaml 文件，文件内容如下： kind: ConfigMap apiVersion: v1 metadata: name: fluentd-config namespace: logging data: system.conf: |- root_dir /tmp/fluentd-buffers/ containers.input.conf: |- @id fluentd-containers.log @type tail # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志。 path /var/log/containers/*.log # 挂载的服务器Docker容器日志地址 pos_file /var/log/es-containers.log.pos tag raw.kubernetes.* # 设置日志标签 read_from_head true # 多行格式化成JSON @type multi_format # 使用 multi-format-parser 解析器插件 format json # JSON解析器 time_key time # 指定事件时间的时间字段 time_format %Y-%m-%dT%H:%M:%S.%NZ # 时间格式 format /^(?.+) (?stdout|stderr) [^ ]* (?.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z # 在日志输出中检测异常，并将其作为一条日志转发 # https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions # 匹配tag为raw.kubernetes.**日志信息 @id raw.kubernetes @type detect_exceptions # 使用detect-exceptions插件处理异常栈信息 remove_tag_prefix raw # 移除 raw 前缀 message log stream stream multiline_flush_interval 5 max_bytes 500000 max_lines 1000 # 拼接日志 @id filter_concat @type concat # Fluentd Filter 插件，用于连接多个事件中分隔的多行日志。 key message multiline_end_regexp /\\n$/ # 以换行符“\\n”拼接 separator \"\" # 添加 Kubernetes metadata 数据 @id filter_kubernetes_metadata @type kubernetes_metadata # 修复 ES 中的 JSON 字段 # 插件地址：https://github.com/repeatedly/fluent-plugin-multi-format-parser @id filter_parser @type parser # multi-format-parser多格式解析器插件 key_name log # 在要解析的记录中指定字段名称。 reserve_data true # 在解析结果中保留原始键值对。 remove_key_name_field true # key_name 解析成功后删除字段。 @type multi_format format json format none # 删除一些多余的属性 @type record_transformer remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash # 只保留具有logging=true标签的Pod日志 @id filter_log @type grep key $.kubernetes.labels.logging pattern ^true$ ###### 监听配置，一般用于日志聚合用 ###### forward.input.conf: |- # 监听通过TCP发送的消息 @id forward @type forward output.conf: |- @id elasticsearch @type elasticsearch @log_level info include_tag_key true host elasticsearch port 9200 logstash_format true logstash_prefix k8s request_timeout 30s @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block 上面配置文件中我们只配置了 docker 容器日志目录，收集到数据经过处理后发送到 elasticsearch:9200 服务。 然后新建一个 fluentd-daemonset.yaml 的文件，文件内容如下： apiVersion: v1 kind: ServiceAccount metadata: name: fluentd-es namespace: logging labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - \"\" resources: - \"namespaces\" - \"pods\" verbs: - \"get\" - \"watch\" - \"list\" --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: fluentd-es namespace: logging apiGroup: \"\" roleRef: kind: ClusterRole name: fluentd-es apiGroup: \"\" --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-es namespace: logging labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: fluentd-es template: metadata: labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \"true\" # 此注释确保如果节点被驱逐，fluentd不会被驱逐，支持关键的基于 pod 注释的优先级方案。 annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: fluentd-es containers: - name: fluentd-es image: tcr-chen.tencentcloudcr.com/efk/fluentd_elasticsearch/fluentd:v3.0.1 env: - name: FLUENTD_ARGS value: --no-supervisor -q resources: limits: memory: 500Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: config-volume mountPath: /etc/fluent/config.d nodeSelector: beta.kubernetes.io/fluentd-ds-ready: \"true\" tolerations: - operator: Exists terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: config-volume configMap: name: fluentd-config 我们将上面创建的 fluentd-config 这个 ConfigMap 对象通过 volumes 挂载到了 Fluentd 容器中，另外为了能够灵活控制哪些节点的日志可以被收集，所以我们这里还添加了一个 nodSelector 属性： nodeSelector: beta.kubernetes.io/fluentd-ds-ready: \"true\" 如果你需要在其他节点上采集日志，则需要给对应节点打上标签，使用如下命令：kubectl label nodes node名 beta.kubernetes.io/fluentd-ds-ready=true。 另外由于我们的集群使用的是 kubeadm 搭建的，默认情况下 master 节点有污点，所以如果要想也收集 master 节点的日志，则需要添加上容忍： tolerations: - operator: Exists 分别创建上面的 ConfigMap 对象和 DaemonSet： $ kubectl create -f fluentd-configmap.yaml configmap \"fluentd-config\" created $ kubectl create -f fluentd-daemonset.yaml serviceaccount \"fluentd-es\" created clusterrole.rbac.authorization.k8s.io \"fluentd-es\" created clusterrolebinding.rbac.authorization.k8s.io \"fluentd-es\" created daemonset.apps \"fluentd-es\" created 创建完成后，查看对应的 Pods 列表，检查是否部署成功： # kubectl get pods -n logging NAME READY STATUS RESTARTS AGE es-0 1/1 Running 0 19h es-1 1/1 Running 0 19h es-2 1/1 Running 0 19h fluentd-es-cnp82 1/1 Running 0 109m fluentd-es-cwt8k 1/1 Running 0 109m fluentd-es-kxx29 1/1 Running 0 109m kibana-649578fc57-4wfrl 1/1 Running 0 5h13m Fluentd 启动成功后，这个时候就可以发送日志到 ES 了，但是我们这里是过滤了只采集具有 logging=true 标签的 Pod 日志，所以现在还没有任何数据会被采集。 下面我们部署一个简单的测试应用， 新建 centos-log.yaml 文件，文件内容如下： apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos-log logging: \"true\" qcloud-app: centos-log name: centos-log namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos-log logging: \"true\" qcloud-app: centos-log template: metadata: labels: k8s-app: centos-log logging: \"true\" # 一定要具有该标签才会被采集 qcloud-app: centos-log spec: containers: - image: ccr.ccs.tencentyun.com/v_cjweichen/centos:latest imagePullPolicy: Always name: centos-log 该 Pod 只是简单将日志信息打印到 stdout，所以正常来说 Fluentd 会收集到这个日志数据，在 Kibana 中也就可以找到对应的日志数据了，使用 kubectl 工具创建该 Pod： $ kubectl create -f centos-log.yaml ]# kubectl get pods -l logging=true NAME READY STATUS RESTARTS AGE centos-log-559f48fcb-brbwp 1/1 Running 0 2m36s Pod 创建并运行后，回到 Kibana Dashboard 页面，点击左侧最下面的 management 图标，然后点击 Kibana 下面的 Index Patterns 开始导入索引数据： 在这里可以配置我们需要的 Elasticsearch 索引，前面 Fluentd 配置文件中我们采集的日志使用的是 logstash 格式，定义了一个 k8s 的前缀，所以这里只需要在文本框中输入k8s-*即可匹配到 Elasticsearch 集群中采集的 Kubernetes 集群日志数据，然后点击下一步，进入以下页面： 在该页面中配置使用哪个字段按时间过滤日志数据，在下拉列表中，选择@timestamp字段，然后点击Create index pattern，创建完成后，点击左侧导航菜单中的Discover，然后就可以看到一些直方图和最近采集到的日志数据了： 日志分析 上面我们已经可以将应用日志收集起来了，下面我们来使用一个应用演示如何分析采集的日志。示例应用会输出如下所示的 JSON 格式的日志信息： {\"LOGLEVEL\":\"WARNING\",\"serviceName\":\"msg-processor\",\"serviceEnvironment\":\"staging\",\"message\":\"WARNING client connection terminated unexpectedly.\"} {\"LOGLEVEL\":\"INFO\",\"serviceName\":\"msg-processor\",\"serviceEnvironment\":\"staging\",\"message\":\"\",\"eventsNumber\":5} {\"LOGLEVEL\":\"INFO\",\"serviceName\":\"msg-receiver-api\":\"msg-receiver-api\",\"serviceEnvironment\":\"staging\",\"volume\":14,\"message\":\"API received messages\"} {\"LOGLEVEL\":\"ERROR\",\"serviceName\":\"msg-receiver-api\",\"serviceEnvironment\":\"staging\",\"message\":\"ERROR Unable to upload files for processing\"} 因为 JSON 格式的日志解析非常容易，当我们将日志结构化传输到 ES 过后，我们可以根据特定的字段值而不是文本搜索日志数据，当然纯文本格式的日志我们也可以进行结构化，但是这样每个应用的日志格式不统一，都需要单独进行结构化，非常麻烦，所以建议将日志格式统一成 JSON 格式输出。 我们这里的示例应用会定期输出不同类型的日志消息，包含不同日志级别（INFO/WARN/ERROR）的日志，一行 JSON 日志就是我们收集的一条日志消息，该消息通过 fluentd 进行采集发送到 Elasticsearch。这里我们会使用到 fluentd 里面的自动 JSON 解析插件，默认情况下，fluentd 会将每个日志文件的一行作为名为 log 的字段进行发送，并自动添加其他字段，比如 tag 标识容器，stream 标识 stdout 或者 stderr。 由于在 fluentd 配置中我们添加了如下所示的过滤器： @id filter_parser @type parser # multi-format-parser多格式解析器插件 key_name log # 在要解析的记录中指定字段名称 reserve_data true # 在解析结果中保留原始键值对 remove_key_name_field true # key_name 解析成功后删除字段。 @type multi_format format json format none 该过滤器使用 json 和 none 两个插件将 JSON 数据进行结构化，这样就会把 JSON 日志里面的属性解析成一个一个的字段，解析生效过后记得刷新 Kibana 的索引字段，否则会识别不了这些字段，通过 管理 -> Index Pattern 点击刷新字段列表即可。 下面我们将示例应用部署到 Kubernetes 集群中：(dummylogs.yaml) apiVersion: apps/v1 kind: Deployment metadata: name: dummylogs namespace: logging spec: replicas: 3 selector: matchLabels: app: dummylogs template: metadata: labels: app: dummylogs logging: \"true\" # 要采集日志需要加上该标签 spec: containers: - name: dummy image: cnych/dummylogs:latest args: - msg-processor --- apiVersion: apps/v1 kind: Deployment metadata: name: dummylogs2 namespace: logging spec: replicas: 3 selector: matchLabels: app: dummylogs2 template: metadata: labels: app: dummylogs2 logging: \"true\" # 要采集日志需要加上该标签 spec: containers: - name: dummy image: cnych/dummylogs:latest args: - msg-receiver-api 直接部署上面的应用即可： # kubectl get pods -l logging=true NAME READY STATUS RESTARTS AGE dummylogs-56987f84-7c4ll 1/1 Running 0 30s dummylogs-56987f84-g5xhg 1/1 Running 0 30s dummylogs-56987f84-m5hln 1/1 Running 0 30s dummylogs2-97864cdcd-4lgqq 1/1 Running 0 30s dummylogs2-97864cdcd-b5vb9 1/1 Running 0 30s dummylogs2-97864cdcd-r5pcr 1/1 Running 0 30s 部署完成后 dummylogs 和 dummylogs2 两个应用就会开始输出不同级别的日志信息了，记得要给应用所在的节点打上 beta.kubernetes.io/fluentd-ds-ready=true 的标签，否则 fluentd 不会在对应的节点上运行也就不会收集日志了。正常情况下日志就已经可以被采集到 Elasticsearch 当中了，我们可以前往 Kibana 的 Dashboard 页面查看: 我们可以看到可用的字段中已经包含我们应用中的一些字段了。找到 serviceName 字段点击我们可以查看已经采集了哪些服务的消息： 可以看到我们收到了来自 msg-processor 和 msg-receiver-api 的日志信息，在最近15分钟之内，api 服务产生的日志更多，点击后面的加号就可以只过滤该服务的日志数据 然后同样我们可以根据自己的需求来筛选需要查看的日志数据： 如果你的 Elasticsearch 的查询语句比较熟悉的话，使用查询语句能实现的筛选功能更加强大，比如我们要查询 mgs-processor 和 msg-receiver-api 两个服务的日志，则可以使用如下所示的查询语句：serviceName : msg-processor OR serviceName : msg-receiver-api 接下来我们来创建一个图表来展示已经处理了多少 msg-processor 服务的日志信息。在 Kibana 中切换到 Visualize 页面，点击 Create new visualization 按钮选择 Area，选择 k8s-* 的索引，首先配置 Y 轴的数据，这里我们使用 eventsNumber 字段的 Sum 函数进行聚合： 然后配置 X 轴数据使用 Date Histogram 类型的 @timestamp 字段： 配置完成后点击右上角的 Apply Changes 按钮则就会在右侧展示出对应的图表信息： 这个图表展示的就是最近15分钟内被处理的事件总数，当然我们也可以自己选择时间范围。我们还可以将 msg-receiver-api 事件的数量和已处理的消息总数进行关联，在该图表上添加另外一层数据，在 Y 轴上添加一个新指标，选择 Add metrics 和 Y-axis，然后同样选择 sum 聚合器，使用 volume 字段： 点击 Apply Changes 按钮就可以同时显示两个服务事件的数据了。最后点击顶部的 save 来保存该图表，并为其添加一个名称 在实际的应用中，我们可能对应用的错误日志更加关心，需要了解应用的运行情况，所以对于错误或者警告级别的日志进行统计也是非常有必要的。现在我们回到 Discover 页面，输入 LOGLEVEL:ERROR OR LOGLEVEL:WARNING 查询语句来过滤所有的错误和告警日志： 错误日志相对较少，实际上我们这里的示例应用会每 15-20 分钟左右就会抛出4个错误信息，其余都是警告信息。同样现在我们还是用可视化的图表来展示下错误日志的情况。 同样切换到 Visualize 页面，点击 Create visualization，选择 Vertical Bar，然后选中 k8s-* 的 Index Pattern。 现在我们忽略 Y 轴，使用默认的 Count 设置来显示消息数量。首先点击 Buckets 下面的 X-axis，然后同样选择 Date histogram，然后点击下方的 Add，添加 Sub-Bueckt，选择 Split series : 然后我们可以通过指定的字段来分割条形图，选择 Terms 作为子聚合方式，然后选择 serviceName.keyword 字段，最后点击 apply 生成图表： 现在上面的图表以不同的颜色来显示每个服务消息，接下来我们在搜索框中输入要查找的内容，因为现在的图表是每个服务的所有消息计数，包括正常和错误的日志，我们要过滤告警和错误的日志，同样输入 LOGLEVEL:ERROR OR LOGLEVEL:WARNING 查询语句进行搜索即可： 从图表上可以看出来 msg-processor 服务问题较多，只有少量的是 msg-receiver-api 服务的，当然我们也可以只查看 ERROR 级别的日志统计信息 从图表上可以看出来基本上出现错误日志的情况下两个服务都会出现，所以这个时候我们就可以猜测两个服务的错误是非常相关的了，这对于我们去排查错误非常有帮助。最后也将该图表进行保存。 最后我们也可以将上面的两个图表添加到 dashboard 中，这样我们就可以在一个页面上组合各种可视化图表。切换到 dashboard 页面，然后点击 Create New Dashboard 按钮： 然后选择上面我们创建的两个图表，添加完成后同样保存该 dashboard 即可： 到这里我们就完成了通过 Fluentd 收集日志到 Elasticsearch，并通过 Kibana 对日志进行了分析可视化操作。 基于日志的报警 在生产环境中我们往往都会使用 Promethus 对应用的各项指标进行监控，但是往往应用的日志中也会产生一些错误日志，这些信息并不是都能够通过 metrics 提供数据的，所以为了避免出现太多的错误，我们还需要对错误日志进行监控报警。在 Elasticsearch 中，我们可以通过使用 elastalert 组件来完成这个工作。 elastalert 是 yelp 使用 python 开发的 elasticsearch 告警工具。elastalert 依照一定频率查询 ES，将查询结果对比告警阈值，超过阈值即进行告警。告警方式包括但不局限于邮箱、微信、钉钉等。 我们这里将 elastalert 部署到 Kubernetes 集群中，对应的资源清单文件如下所示：(elastalert.yaml) apiVersion: v1 kind: ConfigMap metadata: name: elastalert-config namespace: logging labels: app: elastalert data: elastalert_config: |- --- rules_folder: /opt/rules # 指定规则的目录 scan_subdirectories: false run_every: # 多久从 ES 中查询一次 minutes: 1 buffer_time: minutes: 15 es_host: elasticsearch es_port: 9200 writeback_index: elastalert use_ssl: False verify_certs: True alert_time_limit: # 失败重试限制 minutes: 2880 --- apiVersion: v1 kind: ConfigMap metadata: name: elastalert-rules namespace: logging labels: app: elastalert data: rule_config.yaml: |- name: dummylogs error # 规则名字，唯一值 es_host: elasticsearch es_port: 9200 type: any # 报警类型 index: k8s-* # es索引 filter: # 过滤 - query: query_string: query: \"LOGLEVEL:ERROR\" # 报警条件 alert: # 报警类型 - \"email\" smtp_host: smtp.qq.com smtp_port: 587 smtp_auth_file: /opt/auth/smtp_auth_file.yaml email_reply_to: 953562726@qq.com from_addr: 953562726@qq.com email: # 接受邮箱 - \"953562726@qq.com\" --- apiVersion: apps/v1 kind: Deployment metadata: name: elastalert namespace: logging labels: app: elastalert spec: selector: matchLabels: app: elastalert template: metadata: labels: app: elastalert spec: containers: - name: elastalert image: tcr-chen.tencentcloudcr.com/efk/elastalert-docker:0.2.4 imagePullPolicy: IfNotPresent volumeMounts: - name: config mountPath: /opt/config - name: rules mountPath: /opt/rules - name: auth mountPath: /opt/auth resources: limits: cpu: 50m memory: 256Mi requests: cpu: 50m memory: 256Mi volumes: - name: auth secret: secretName: smtp-auth - name: rules configMap: name: elastalert-rules - name: config configMap: name: elastalert-config items: - key: elastalert_config path: elastalert_config.yaml 使用邮件进行报警的时候，需要指定一个 smtp_auth_file 的文件，文件中包含用户名和密码：(smtp_auth_file.yaml) user: \"953562726@qq.com\" password: \"odqhiagwebhmbcfc\" #user 发送的邮箱地址 # password不是qq邮箱的登录密码，是授权码 然后使用上面的文件创建一个对应的 Secret 资源对象： $ kubectl create secret generic smtp-auth --from-file=smtp_auth_file.yaml -n logging 然后直接创建上面的 elastalert 应用： $ kubectl apply -f elastalert.yaml # kubectl get pods -n logging -l app=elastalert NAME READY STATUS RESTARTS AGE elastalert-67984b596d-w4xh5 1/1 Running 0 54m $ kubectl logs -f elastalert-67984b596d-w4xh5 -n logging Elastic Version: 7.6.2 Reading Elastic 6 index mappings: Reading index mapping 'es_mappings/6/silence.json' Reading index mapping 'es_mappings/6/elastalert_status.json' Reading index mapping 'es_mappings/6/elastalert.json' Reading index mapping 'es_mappings/6/past_elastalert.json' Reading index mapping 'es_mappings/6/elastalert_error.json' Deleting index elastalert_status. New index elastalert created Done! 看到上面的日志信息就证明 elastalert 应用部署成功了。在 Elasticsearch 中也可以看到几个相关的 Index ： 由于我们的示例应用会隔一段时间就产生 ERROR 级别的错误日志，所以正常情况下我们就可以收到如下所示的邮件信息了： dummylogs error @timestamp: 2022-01-16T08:58:20.589314Z LOGLEVEL: ERROR _id: bRAbYn4BMhTI7GVb8jUC _index: k8s-2022.01.16 _type: _doc docker: {} kubernetes: { \"container_image\": \"cnych/dummylogs:latest\", \"container_name\": \"dummy\", \"host\": \"v-cjweichen-tke-node1\", \"labels\": { \"app\": \"dummylogs2\", \"logging\": \"true\" }, \"namespace_name\": \"default\", \"pod_name\": \"dummylogs2-97864cdcd-4lgqq\" } message: ERROR Unable to upload files for processing num_hits: 4 num_matches: 4 serviceEnvironment: staging serviceName: msg-receiver-api stream: stdout tag: kubernetes.var.log.containers.dummylogs2-97864cdcd-4lgqq_default_dummy-d51703108fd39b0ff6485b3d3c2b6f7816bd65f2664a75722efaae8a9b0fa237.log 除此之外我们也可以配置将报警信息发往 企业微信 或者 钉钉，还可以安装一个 elastalert 的 Kibana 插件，用于在 Kibana 页面上进行可视化操作。 关于 elastalert 更多的操作和使用说明，大家可以查看官方文档了解更多：https://elastalert.readthedocs.io/en/latest/。 "},"docs/Kubernetes/k8s-commin-pare.html":{"url":"docs/Kubernetes/k8s-commin-pare.html","title":"K8S中常见频率参数","keywords":"","body":"HPA 扩缩容灵敏度 --horizontal-pod-autoscaler-downscale-stabilization-window ：参数控制hpa缩容时间窗口 默认 5 分钟** 备注： 在 K8S 1.18 之前，HPA 扩容是无法调整灵敏度的: 对于缩容，由 kube-controller-manager 的 --horizontal-pod-autoscaler-downscale-stabilization-window 参数控制缩容时间窗口，默认 5 分钟，即负载减小后至少需要等 5 分钟才会缩容。 对于扩容，由 hpa controller 固定的算法、硬编码的常量因子来控制扩容速度，无法自定义。 terminationGracePeriodSeconds 备注：等待容器进程完全停止，如果在 terminationGracePeriodSeconds 内 (默认30s) 还未完全停止，将发送 SIGKILL 信号强制停止进程 --node-monitor-period=5s 在NodeController中同步节点状态的周期。默认5s --node-monitor-grace-period=40s 我们允许运行的节点在标记为不健康之前没有响应的时间。必须是kubelet的nodeStatusUpdateFrequency的N倍，其中N表示允许kubelet发布节点状态的重试次数默认40s。 --node-startup-grace-period=1m0s 我们允许启动节点在标记为不健康之前没有响应的时间。默认1m0s。(这个其实不需要调整因为是启动节点的时间，业务并没有要求启动时间) --pod-eviction-timeout=5m 删除失败节点上的pods的宽限期。默认5m 失败立即删除 --node-status-update-frequency: 10s 指定的频率连续报告节点状态更新，其默认值为 10s。指定kubelet多长时间向master发布一次节点状态。注意: 它必须与kube-controller中的nodeMonitorGracePeriod一起协调工作。(默认 10s) TEK控制台webconsole 默认3分钟不进行任何操作，就会断开 "},"docs/Kubernetes/k8s-deployment-nacos.html":{"url":"docs/Kubernetes/k8s-deployment-nacos.html","title":"Kubernetes部署nacos服务","summary":"kubernetes部署nacos服务","keywords":"","body":"Kubernetes 部署 Nacos 集群 官网文档：https://nacos.io/zh-cn/docs/use-nacos-with-kubernetes.html 本文是基于腾讯云TKE容器服务集群搭建 部署数据库 1，数据库是NFS做数据化存储（或者CBS都可以） 需要注意： 镜像要使用nacos提供的数据库nacos/nacos-mysql:5.7，自带的数据库创建完成后相关库和数据都已经导入，其中NFS替换成自己NFS实例，自建或者云上CFS apiVersion: apps/v1 kind: Deployment metadata: generation: 1 labels: k8s-app: nacos-mysql qcloud-app: nacos-mysql name: nacos-mysql namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: nacos-mysql qcloud-app: nacos-mysql template: metadata: labels: k8s-app: nacos-mysql qcloud-app: nacos-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: root - name: MYSQL_DATABASE value: nacos - name: MYSQL_USER value: nacos - name: MYSQL_PASSWORD value: nacos image: nacos/nacos-mysql:5.7 imagePullPolicy: IfNotPresent name: mysql resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false volumeMounts: - mountPath: /var/lib/mysql name: mysql-data dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - name: mysql-data nfs: path: /nacos server: 192.168.1.6 --- apiVersion: v1 kind: Service metadata: name: nacos-mysql namespace: default labels: name: nacos-mysql spec: ports: - port: 3306 targetPort: 3306 selector: k8s-app: nacos-mysql qcloud-app: nacos-mysql 验证数据库可用性： [root@VM-0-17-tlinux ~/nacos]# mysql -h172.18.250.54 -unacos -pnacos MySQL [(none)]> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | nacos | +--------------------+ 2 rows in set (0.01 sec) MySQL [(none)]> use nacos Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MySQL [nacos]> show tables; +----------------------+ | Tables_in_nacos | +----------------------+ | config_info | | config_info_aggr | | config_info_beta | | config_info_tag | | config_tags_relation | | group_capacity | | his_config_info | | permissions | | roles | | tenant_capacity | | tenant_info | | users | +----------------------+ 12 rows in set (0.00 sec) 部署nacos 1，创建链接mysql的配置文件 apiVersion: v1 kind: ConfigMap metadata: name: nacos-cm namespace: nacos data: mysql.host: \"172.18.253.36\" #如果是K8S集群内数据库，可以使用服务名称 mysql.db.name: \"nacos\" #上面创建数据库是指的的库名称 mysql.port: \"3306\" #端口 mysql.user: \"nacos\" #用户 mysql.password: \"nacos\" #用户密码 2，创建nacos-headless 用于集群之间的链接 apiVersion: v1 kind: Service metadata: name: nacos-headless namespace: nacos labels: app: nacos annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\" spec: ports: - port: 8848 name: server targetPort: 8848 clusterIP: None selector: app: nacos 3，部署nacos服务 apiVersion: apps/v1 kind: StatefulSet metadata: name: nacos namespace: nacos spec: serviceName: nacos-headless replicas: 3 template: metadata: labels: app: nacos annotations: pod.alpha.kubernetes.io/initialized: \"true\" spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \"app\" operator: In values: - nacos topologyKey: \"kubernetes.io/hostname\" containers: - name: k8snacos imagePullPolicy: Always image: nacos/nacos-server:latest resources: requests: memory: \"2Gi\" cpu: \"500m\" ports: - containerPort: 8848 name: client env: - name: NACOS_REPLICAS value: \"3\" - name: MYSQL_SERVICE_HOST valueFrom: configMapKeyRef: name: nacos-cm key: mysql.host - name: MYSQL_SERVICE_DB_NAME valueFrom: configMapKeyRef: name: nacos-cm key: mysql.db.name - name: MYSQL_SERVICE_PORT valueFrom: configMapKeyRef: name: nacos-cm key: mysql.port - name: MYSQL_SERVICE_USER valueFrom: configMapKeyRef: name: nacos-cm key: mysql.user - name: MYSQL_SERVICE_PASSWORD valueFrom: configMapKeyRef: name: nacos-cm key: mysql.password - name: MODE value: \"cluster\" - name: NACOS_SERVER_PORT value: \"8848\" - name: PREFER_HOST_MODE value: \"hostname\" - name: NACOS_SERVERS value: \"nacos-0.nacos-headless.nacos.svc.cluster.local:8848 nacos-1.nacos-headless.nacos.svc.cluster.local:8848 nacos-2.nacos-headless.nacos.svc.cluster.local:8848\" selector: matchLabels: app: nacos 4，创建公网类型CLB的service（可选） apiVersion: v1 kind: Service metadata: annotations: service.kubernetes.io/service.extensiveParameters: '{\"AddressIPVersion\":\"IPV4\",\"ZoneId\":\"ap-chongqing-1\"}' name: nacos namespace: nacos spec: externalTrafficPolicy: Cluster ports: - name: 8848-8848-tcp port: 8848 protocol: TCP targetPort: 8848 selector: app: nacos sessionAffinity: None type: LoadBalancer 5，创建公网访问的ingress（可选） apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: ingress.cloud.tencent.com/direct-access: \"false\" name: nacos-ingress namespace: nacos spec: rules: - host: nacos.dev.cc http: paths: - backend: serviceName: nacos servicePort: 8848 path: /nacos nacos使用 1，使用service访问 或者ingress进行访问 【附录事项】： 附1，如果对应数据库配置没有问题，但是nacos启动一直报如下错，找不到数据源 nacos server did not start because dumpservice bean construction failure. errMsg:102, dataSource or tableName is null 导致的原因是，需要额外加一下这个环境变量 SPRING_DATASOURCE_PLATFORM 值为：mysql - name: SPRING_DATASOURCE_PLATFORM #最新的版本一定要这个，nacos官方的yaml漏了这个（2.2版本，2.0版本是不需要加这个） value: \"mysql\" 附2，使用CBS做数据库的持久化存储，实例yaml: 仅参考 apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: nacos-mysql qcloud-app: nacos-mysql name: nacos-mysql namespace: default spec: podManagementPolicy: OrderedReady replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: nacos-mysql qcloud-app: nacos-mysql serviceName: \"\" template: metadata: annotations: eks.tke.cloud.tencent.com/retain-ip: \"true\" #EKS集群需要，TKE集群不需要这个 eks.tke.cloud.tencent.com/root-cbs-size: \"20\" #EKS集群需要，TKE集群不需要这个 labels: k8s-app: nacos-mysql qcloud-app: nacos-mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: root - name: MYSQL_DATABASE value: nacos - name: MYSQL_USER value: nacos - name: MYSQL_PASSWORD value: nacos image: nacos/nacos-mysql:5.7 imagePullPolicy: IfNotPresent name: nacos-mysql resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false volumeMounts: - mountPath: /var/lib/mysql name: nacos-mysql subPath: mysql dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always updateStrategy: rollingUpdate: partition: 0 type: RollingUpdate volumeClaimTemplates: - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nacos-mysql spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: cbs volumeMode: Filesystem "},"docs/Kubernetes/k8s-deploy-rabbitmq.html":{"url":"docs/Kubernetes/k8s-deploy-rabbitmq.html","title":"Kubernetes部署rabbitmq服务","summary":"kubernetes部署rabbitmq服务","keywords":"","body":"待补充，先上yaml apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 name: rabbitmq namespace: default secrets: - name: rabbitmq --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 name: rabbitmq-endpoint-reader namespace: default rules: - apiGroups: - \"\" resources: - endpoints verbs: - get - apiGroups: - \"\" resources: - events verbs: - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 name: rabbitmq-endpoint-reader namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: rabbitmq-endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq apiVersion: v1 data: rabbitmq.conf: |- ## Username and password default_user = user default_pass = CHANGEME ## Clustering cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal # queue master locator queue_master_locator = min-masters # enable guest user loopback_users.guest = false #disk_free_limit.absolute = 50MB #management.load_definitions = /app/load_definition.json kind: ConfigMap metadata: labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 name: rabbitmq-config namespace: default --- apiVersion: v1 data: rabbitmq-erlang-cookie: b3hWVXpRQmlWWUt4VUR3bWRSbTlDRldseU01Ulg3SzM= rabbitmq-password: ZGtBU3o5ODZ2ZQ== kind: Secret metadata: labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 name: rabbitmq namespace: default type: Opaque apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 name: rabbitmq-headless namespace: default spec: clusterIP: None ports: - name: epmd port: 4369 targetPort: epmd - name: amqp port: 5672 targetPort: amqp - name: dist port: 25672 targetPort: dist - name: stats port: 15672 targetPort: stats selector: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/name: rabbitmq --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 name: rabbitmq namespace: default spec: ports: - name: amqp nodePort: null port: 5672 targetPort: amqp - name: epmd nodePort: null port: 4369 targetPort: epmd - name: dist nodePort: null port: 25672 targetPort: dist - name: http-stats nodePort: null port: 15672 targetPort: stats selector: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/name: rabbitmq type: ClusterIP apiVersion: apps/v1 kind: StatefulSet metadata: annotations: meta.helm.sh/release-name: rabbitmq meta.helm.sh/release-namespace: cjweichen labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 name: rabbitmq namespace: default spec: podManagementPolicy: OrderedReady replicas: 3 selector: matchLabels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/name: rabbitmq serviceName: rabbitmq-headless template: metadata: annotations: checksum/secret: c3d76de93266cdb4f6a35506a2a5e005e61c7074cd500d28f210e06a6043a0b7 labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: rabbitmq helm.sh/chart: rabbitmq-7.5.7 spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - eklet-subnet-fixt567x-ipcmleaw containers: - env: - name: BITNAMI_DEBUG value: \"false\" - name: MY_POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP - name: MY_POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: K8S_SERVICE_NAME value: rabbitmq-headless - name: K8S_ADDRESS_TYPE value: hostname - name: RABBITMQ_FORCE_BOOT value: \"no\" - name: RABBITMQ_NODE_NAME value: rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local - name: K8S_HOSTNAME_SUFFIX value: .$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local - name: RABBITMQ_MNESIA_DIR value: /bitnami/rabbitmq/mnesia/$(RABBITMQ_NODE_NAME) - name: RABBITMQ_LDAP_ENABLE value: \"no\" - name: RABBITMQ_LOGS value: '-' - name: RABBITMQ_ULIMIT_NOFILES value: \"65536\" - name: RABBITMQ_USE_LONGNAME value: \"true\" - name: RABBITMQ_ERL_COOKIE valueFrom: secretKeyRef: key: rabbitmq-erlang-cookie name: rabbitmq - name: RABBITMQ_USERNAME value: user - name: RABBITMQ_PASSWORD valueFrom: secretKeyRef: key: rabbitmq-password name: rabbitmq - name: RABBITMQ_PLUGINS value: rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap image: ccr.ccs.tencentyun.com/tke-market/rabbitmq:3.8.5-debian-10-r38 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - bash - -ec - rabbitmqctl stop_app livenessProbe: exec: command: - /bin/bash - -ec - rabbitmq-diagnostics -q check_running failureThreshold: 6 initialDelaySeconds: 120 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 20 name: rabbitmq ports: - containerPort: 5672 name: amqp protocol: TCP - containerPort: 25672 name: dist protocol: TCP - containerPort: 15672 name: stats protocol: TCP - containerPort: 4369 name: epmd protocol: TCP readinessProbe: exec: command: - /bin/bash - -ec - rabbitmq-diagnostics -q check_running failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 20 resources: {} volumeMounts: - mountPath: /bitnami/rabbitmq/conf name: configuration - mountPath: /bitnami/rabbitmq/mnesia name: data dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: fsGroup: 1001 runAsUser: 1001 serviceAccount: rabbitmq serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 volumes: - configMap: defaultMode: 420 items: - key: rabbitmq.conf path: rabbitmq.conf name: rabbitmq-config name: configuration updateStrategy: type: RollingUpdate volumeClaimTemplates: - apiVersion: v1 kind: PersistentVolumeClaim metadata: labels: app.kubernetes.io/instance: rabbitmq app.kubernetes.io/name: rabbitmq name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: cbs volumeMode: Filesystem "},"docs/Kubernetes/k8s-deployment-mysql.html":{"url":"docs/Kubernetes/k8s-deployment-mysql.html","title":"Kubernetes部署主从架构Mysql集群","summary":"使用Kubernetes部署主从架构的Mysql集群","keywords":"","body":"概述 随着kubernetes发展，越来越多的人开始使用kubernetes部署自己应用，它是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能，然而很多应用也使用到数据库，下面通过kubernetes上部署个主从架构的Mysql集群供应用使用 准备环境 kubernetes集群（本次实验使用的是腾讯云TKE集群，版本1.18） 操作步骤 主要通过一下几个步骤完整的搭建一个MySQL集群 搭建一个主从复制（Master-Slave）的MySQL集群 从节点可以进行水平扩展，扩容多个节点 所有的写操作只能在MySQL主节点上执行 读操作可以在MySQL主从节点上执行 从节点能自动同步主节点的数据 服务部署 1，创建mysql使用的Namespace（如果不创建可以使用默认命名空间，一般建议单独给数据创建个命名空间使用） apiVersion: v1 kind: Namespace metadata: name: mysql labels: app: mysql 2，创建数据库的配置文件configmap 使用ConfigMap为Master/Slave节点分配不同的配置文件 apiVersion: v1 kind: ConfigMap metadata: name: mysql namespace: mysql labels: app: mysql data: master.cnf: | # Master主节点配置 [mysqld] log-bin=mysqllog skip-name-resolve slave.cnf: | # Slave从节点配置 [mysqld] super-read-only skip-name-resolve log-bin=mysql-bin replicate-ignore-db=mysql 3，创建MySQL密码Secret apiVersion: v1 kind: Secret metadata: name: mysql-secret namespace: mysql labels: app: mysql type: Opaque data: password: MTIzNDU2 # echo -n \"123456\" | base64 4，使用Service为MySQL提供读写分离 用户所有写请求，必须以DNS记录的方式直接访问到Master节点，也就是mysql-0.mysql这条DNS记录。 用户所有读请求，必须访问自动分配的DNS记录可以被转发到任意一个Master或Slave节点上，也就是mysql-read这条DNS记录。 apiVersion: v1 kind: Service metadata: name: mysql namespace: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- apiVersion: v1 kind: Service metadata: name: mysql-read namespace: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql 5，创建MySQL集群实例 使用StatefulSet搭建MySQL主从集群 整体的StatefulSet有两个Replicas，一个Master，一个Slave，然后使用init-mysql这个initContainers进行配置文件的初始化。接着使用clone-mysql这个initContainers进行数据的传输；同时使用xtrabackup这个sidecar容器进行SQL初始化和数据传输功能。 apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: mysql labels: app: mysql spec: selector: matchLabels: app: mysql serviceName: mysql #注意这个千万别少些 replicas: 2 template: metadata: labels: app: mysql spec: initContainers: - name: init-mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password command: - bash - \"-c\" - | set -ex # 从Pod的序号，生成server-id [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} echo [mysqld] > /mnt/conf.d/server-id.cnf # 由于server-id不能为0，因此给ID加100来避开它 echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf # 如果Pod的序号为0，说明它是Master节点，从ConfigMap里把Master的配置文件拷贝到/mnt/conf.d目录下 # 否则，拷贝ConfigMap里的Slave的配置文件 if [[ ${ordinal} -eq 0 ]]; then cp /mnt/config-map/master.cnf /mnt/conf.d else cp /mnt/config-map/slave.cnf /mnt/conf.d fi volumeMounts: - name: conf mountPath: /mnt/conf.d - name: config-map mountPath: /mnt/config-map - name: clone-mysql image: gcr.tencentcloudcr.com/google-samples/xtrabackup:1.0 #使用腾讯镜像加速 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password command: - bash - \"-c\" - | set -ex # 拷贝操作只需要在第一次启动时进行，所以数据已经存在则跳过 [[ -d /var/lib/mysql/mysql ]] && exit 0 # Master 节点（序号为 0）不需要这个操作 [[ $(hostname) =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} [[ $ordinal == 0 ]] && exit 0 # 使用ncat指令，远程地从前一个节点拷贝数据到本地 ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql # 执行 --prepare，这样拷贝来的数据就可以用作恢复了 xtrabackup --prepare --target-dir=/var/lib/mysql volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d containers: - name: mysql image: mysql:5.7 env: # - name: MYSQL_ALLOW_EMPTY_PASSWORD # value: \"1\" - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 500m memory: 1Gi livenessProbe: exec: command: [\"mysqladmin\", \"ping\", \"-uroot\", \"-p${MYSQL_ROOT_PASSWORD}\"] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: command: [\"mysqladmin\", \"ping\", \"-uroot\", \"-p${MYSQL_ROOT_PASSWORD}\"] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 - name: xtrabackup image: gcr.tencentcloudcr.com/google-samples/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 3307 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password command: - bash - \"-c\" - | set -ex cd /var/lib/mysql # 从备份信息文件里读取MASTER_LOG_FILE和MASTER_LOG_POS这2个字段的值，用来拼装集群初始化SQL if [[ -f xtrabackup_slave_info ]]; then # 如果xtrabackup_slave_info文件存在，说明这个备份数据来自于另一个Slave节点 # 这种情况下，XtraBackup工具在备份的时候，就已经在这个文件里自动生成了“CHANGE MASTER TO”SQL语句 # 所以，只需要把这个文件重命名为change_master_to.sql.in，后面直接使用即可 mv xtrabackup_slave_info change_master_to.sql.in # 所以，也就用不着xtrabackup_binlog_info了 rm -f xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # 如果只是存在xtrabackup_binlog_info文件，说明备份来自于Master节点，就需要解析这个备份信息文件，读取所需的两个字段的值 [[ $(cat xtrabackup_binlog_info) =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm xtrabackup_binlog_info # 把两个字段的值拼装成SQL，写入change_master_to.sql.in文件 echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\ MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in fi # 如果存在change_master_to.sql.in，就意味着需要做集群初始化工作 if [[ -f change_master_to.sql.in ]]; then # 但一定要先等MySQL容器启动之后才能进行下一步连接MySQL的操作 echo \"Waiting for mysqld to be ready（accepting connections）\" until mysql -h 127.0.0.1 -uroot -p${MYSQL_ROOT_PASSWORD} -e \"SELECT 1\"; do sleep 1; done echo \"Initializing replication from clone position\" # 将文件change_master_to.sql.in改个名字 # 防止这个Container重启的时候，因为又找到了change_master_to.sql.in，从而重复执行一遍初始化流程 mv change_master_to.sql.in change_master_to.sql.orig # 使用change_master_to.sql.orig的内容，也就是前面拼装的SQL，组成一个完整的初始化和启动Slave的SQL语句 mysql -h 127.0.0.1 -uroot -p${MYSQL_ROOT_PASSWORD} 可以看到，StatefulSet启动成功后，会有两个Pod运行。接下来，我们可以尝试向这个MySQL集群发起请求，执行一些SQL操作来验证它是否正常。整个过程因为拉取mysql和一个gcr.io/google-samples/xtrabackup:1.0（使用腾讯云加速镜像地址gcr.tencentcloudcr.com）国外的镜像会很慢,但是在创建mysql-0拉取一次之后，后续创建mysql-1就相对很快了。 最后，容器检查pod的运行状态 [root@VM-0-17-tlinux ~]# kubectl get all -n mysql -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/mysql-0 2/2 Running 0 108s 172.18.1.4 192.168.2.40 pod/mysql-1 2/2 Running 0 76s 172.18.1.5 192.168.2.40 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/mysql ClusterIP None 3306/TCP 2m24s app=mysql service/mysql-read ClusterIP 172.18.253.108 3306/TCP 2m24s app=mysql NAME READY AGE CONTAINERS IMAGES statefulset.apps/mysql 2/2 108s mysql,xtrabackup mysql:5.7,gcr.tencentcloudcr.com/google-samples/xtrabackup:1.0 服务验证 1，验证主从关系 [root@VM-0-17-tlinux ~]# kubectl -n mysql exec mysql-1 -c mysql -- bash -c \"mysql -uroot -p123456 -e 'show slave status \\G'\" mysql: [Warning] Using a password on the command line interface can be insecure. *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: mysql-0.mysql.mysql Master_User: root Master_Port: 3306 Connect_Retry: 10 Master_Log_File: mysqllog.000003 Read_Master_Log_Pos: 154 Relay_Log_File: mysql-1-relay-bin.000002 Relay_Log_Pos: 319 Relay_Master_Log_File: mysqllog.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: mysql Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 528 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 100 Master_UUID: f8d3bd9a-4df4-11ec-9930-52d15f478b07 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 2，接下来，我们通过Master容器创建数据库和表、插入数据库。 kubectl -n mysql exec mysql-0 -c mysql -- bash -c \"mysql -uroot -p123456 -e 'create database test'\" kubectl -n mysql exec mysql-0 -c mysql -- bash -c \"mysql -uroot -p123456 -e 'use test;create table counter(c int);'\" kubectl -n mysql exec mysql-0 -c mysql -- bash -c \"mysql -uroot -p123456 -e 'use test;insert into counter values(123)'\" 3，然后，我们观察Slave节点是否都同步到数据了 kubectl -n mysql exec mysql-1 -c mysql -- bash -c \"mysql -uroot -p123456 -e 'use test;select * from counter'\" 执行返回结果是，当看到输出结果，主从同步正常了。 [root@VM-0-17-tlinux ~]# kubectl -n mysql exec mysql-1 -c mysql -- bash -c \"mysql -uroot -p123456 -e 'use test;select * from counter'\" c 123 扩展从节点 在有了StatefulSet以后，你就可以像Deployment那样，非常方便地扩展这个MySQL集群，比如： kubectl -n mysql scale statefulset mysql --replicas=3 statefulset.apps/mysql scaled [root@VM-0-17-tlinux ~]# kubectl get pods -n mysql NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 10m mysql-1 2/2 Running 0 10m mysql-2 0/2 Init:1/2 0 24s 这时候，一个新的mysql-2就创建出来了，我们继续验证新扩容的节点是否都同步到主节点的数据。 kubectl -n mysql exec mysql-2 -c mysql -- bash -c \"mysql -uroot -p123456 -e 'use test;select * from counter'\" 当看到输出结果，主从同步正常了。也就是说从StatefulSet为我们新创建的mysql-2上，同样可以读取到之前插入的记录。也就是说，我们的数据备份和恢复，都是有效的 "},"docs/Kubernetes/k8s-eviction.html":{"url":"docs/Kubernetes/k8s-eviction.html","title":"Kubernetes常见驱逐Eviction问题","summary":"Kubernetes常见驱逐Eviction问题","keywords":"","body":" 在高可用的k8s集群中，当Node节点挂掉，kubelet无法提供工作的时候，pod将会驱逐并自动调度到其他的节点上去，或者在node节点的资源紧缺的条件下，kubelet为了保证node节点的稳定性，也回触发主动驱逐pod的机制 POD驱逐 Eviction状态介绍： Eviction，即驱逐的意思，意思是当节点出现异常时，为了保证工作负载的可用性，kubernetes将有相应的机制驱逐该节点上的Pod。 目前kubernetes中存在两种eviction机制，分别由kube-controller-manager和kubelet实现。 kube-controller-manager的eviction机制是粗粒度的，即驱赶一个节点上的所有pod，而kubelet则是细粒度的，它驱赶的是节点上的某些Pod，驱赶哪些Pod与Pod的Qos机制有关。该Eviction会周期性检查本节点内存、磁盘等资源，当资源不足时，按照优先级驱逐部分pod kube-controller-manager实现的eviction kube-controller-manager主要由多个控制器构成，而eviction的功能主要由node controller这个控制器实现。该Eviction会周期性检查所有节点状态，当节点处于NotReady状态超过一段时间后，驱逐该节点上所有pod Kubelet 状态更新的基本流程： kubelet 自身会定期更新状态到 apiserver，通过参数--node-status-update-frequency指定上报频率，默认是 10s 上报一次。 kube-controller-manager 会每隔--node-monitor-period时间去检查 kubelet 的状态，默认是 5s。 当 node 失联一段时间后，kubernetes 判定 node 为 notready 状态，这段时长通过--node-monitor-grace-period参数配置，默认 40s。 当 node 失联一段时间后，kubernetes 判定 node 为 unhealthy 状态，这段时长通过--node-startup-grace-period参数配置，默认 1m0s。 当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过--pod-eviction-timeout参数配置，默认 5m0s。 kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果--node-status-update-frequency设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。 启动参数控制eviction： pod-eviction-timeout：即当节点宕机该时间间隔后，开始eviction机制，驱赶宕机节点上的Pod，默认为5min。 node-eviction-rate：驱赶速率，即驱赶Node的速率，由令牌桶流控算法实现，默认为0.1，即每秒驱赶0.1个节点，注意这里不是驱赶Pod的速率，而是驱赶节点的速率。相当于每隔10s，清空一个节点。 secondary-node-eviction-rate：二级驱赶速率，当集群中宕机节点过多时，相应的驱赶速率也降低，默认为0.01。 unhealthy-zone-threshold：不健康zone阈值，会影响什么时候开启二级驱赶速率，默认为0.55，即当该zone中节点宕机数目超过55%，而认为该zone不健康。 large-cluster-size-threshold：大集群阈值，当该zone的节点多余该阈值时，则认为该zone是一个大集群。大集群节点宕机数目超过55%时，则将驱赶速率降为0.01，假如是小集群，则将速率直接降为0。 社区默认的配置： 参数 值 –node-status-update-frequency 10s –node-monitor-period 5s –node-monitor-grace-period 40s –pod-eviction-timeout 5m kubelet基于节点压力eviction机制 如果节点处于资源压力，那么kubelet就会执行驱逐策略。驱逐会考虑Pod的优先级，资源使用和资源申请。当优先级相同时，资源使用/资源申请最大的Pod会被首先驱逐，更多详情可以参考官方文档 kubelet提供了以下参数控制eviction： eviction-soft：软驱逐阈值设置，具有一系列阈值，比如memory.available eviction-soft-grace-period：默认为90秒，当eviction-soft时，终止Pod的grace的时间，即软驱逐宽限期，软驱逐信号与驱逐处理之间的时间差。 eviction-max-pod-grace-period：最大驱逐pod宽限期，停止信号与kill之间的时间差。 eviction-pressure-transition-period：默认为5分钟，脱离pressure condition的时间，超过阈值时，节点会被设置为memory pressure或者disk pressure，然后开启pod eviction。 eviction-minimum-reclaim：表示每一次eviction必须至少回收多少资源。 eviction-hard**：**强制驱逐设置，也具有一系列的阈值，比如memory.available 驱逐问题处理 1，使用如下命令发现很多pod的状态为Evicted： kubectl get pods -o wide --all-namespaces | grep -i Evicted 2，在节点的kubelet日志中会记录Evicted相关内容，搜索方法可参考如下命令： journalctl -u kubelet | grep -i Evicted -C3 3，查看节点监控是否有内存打满或者 节点磁盘 4，清理异常状态Evicted的POD 为命名空间名称，请根据需要指定。 kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod 或者使用这个命令清理所有命名空间驱逐状态的POD kubectl get pod -o wide --all-namespaces | awk '{if($4==\"Evicted\"){cmd=\"kubectl -n \"$1\" delete pod \"$2; system(cmd)}}' 5，pod驱逐后，如果新调度到的节点也有驱逐情况，就会再次被驱逐；甚至出现pod不断被驱逐的情况,，需要确保集群资源充足 6，如果是由kube-controller-manager触发的驱逐，会留下一个状态为Terminating的pod；直到容器所在节点状态恢复后，pod才会自动删除。如果节点已经删除或者其他原因导致的无法恢复，可以使用“强制删除”删除pod， 7，如果是由kubelet触发的驱逐，会留下一个状态为Evicted的pod，此pod只是方便后期定位的记录，可以直接删除。 "},"docs/Kubernetes/k8s-service-anno.html":{"url":"docs/Kubernetes/k8s-service-anno.html","title":"kubernetes的service模式分析","keywords":"","body":"kubernetes service模式分析 概述 Kubernetes service是为POD提供统一访问入口的，实现主要依靠kube-proxy实现，kube-proxy有三种模式userspace、iptables，ipvs，同时我们也知道service有三种类型clusterIP、nodeport，loadblance和三种端口类型port，targetport，nodeport。 kube-proxy模式分析 userspace userspace为kube-proxy为早期的模式，Kubernetes1.2版本之前主要使用这个模式，转发原理参考 https://kubernetes.io/zh/docs/concepts/services-networking/service/ 这个模式最大缺点就是，所以端口请求都需要先经过kube-proxy然后在通过iptables转发，这样带来一个问题就是需要在用户态和内核态不断进行切换，效率低。 Iptables ​ Kubernetes在1.2版本开始将iptables做为kube-proxy的默认模式，iptables根之前userspace相比，完全工作在内核态而且不用在经过kube-proxy中转一次性能更强，下面介绍Kubernetes中iptables转发流程， ​ iptables有链和表的概念，链就相当于一道道关卡，表就是这个关卡上对应的规则总共有四个表和五条链，kube-proxy在这里就使用了两个表分别是filter和nat表， 也自定义了五个链 KUBE-SERVICES KUBE-NODE-PORTS KUBE-POSTROUTING KUBE-MARK-MASQ KUBE-MARK-DROP 目前kubernetes提供了两种负载分发策略：RoundRobin和SessionAffinity RoundRobin： 轮询模式，即轮询将请求转发到后端的各个Pod上（默认是RoundRobin模式）。 SessionAffinity：基于客户端IP地址进行会话保持的模式，第一次客户端访问后端某个Pod，之后的请求都转发到这个Pod上 iptables数据包转发流程 1、首先一个数据包经过网卡进来，先经过PREROUTING链 2、判定目的地址是否为主机，为本机就通过INPUT链转发 3、若不为本机通过FORWARDl链转发到POSTROUTING出去 以下展示一个实例展示kube-proxy是如何根据service不同类型生成对应规则 我们创建名为nginx的deployment，镜像为nginx:latest，replicas为3个 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: nginx qcloud-app: nginx name: nginx namespace: default spec: replicas: 3 selector: matchLabels: k8s-app: nginx qcloud-app: nginx template: metadata: labels: k8s-app: nginx qcloud-app: nginx spec: containers: - image: nginx:stable imagePullPolicy: IfNotPresent name: nginx resources: limits: cpu: 500m memory: 256Mi requests: cpu: 100m memory: 64Mi dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey 在给这个deployment创建一个ClusterIP类型的service [root@VM-0-17-tlinux ~]# kubectl expose deployment/nginx --type=ClusterIP --port=80 [root@VM-0-17-tlinux ~]# kubectl get deployment,pods,svc | grep nginx deployment.apps/nginx 3/3 3 3 3m23s pod/nginx-754f548f9b-5rpzf 1/1 Running 0 3m23s pod/nginx-754f548f9b-vfdjv 1/1 Running 0 3m23s pod/nginx-754f548f9b-x6kv5 1/1 Running 0 3m23s service/nginx ClusterIP 172.18.255.127 80/TCP 44s 导出iptables规则： ClusterIP类型 查看规则，首先Kubernetes会指对每个service在创建一些名为KUBE-SEP-xxx，KUBE-SVC-xxx的链 以刚刚创建的类型为Cluster-ip名为test这个service为例，创建了以下规则 [root@VM-0-17-tlinux ~]# iptables-save | grep nginx | grep -v nginx-ingress -A KUBE-SEP-G4A7DIXDTUZ6AHE3 -s 172.18.0.5/32 -m comment --comment \"default/nginx:\" -j KUBE-MARK-MASQ -A KUBE-SEP-G4A7DIXDTUZ6AHE3 -p tcp -m comment --comment \"default/nginx:\" -m tcp -j DNAT --to-destination 172.18.0.5:80 -A KUBE-SEP-J5YCYOGHESVN34H2 -s 172.18.0.6/32 -m comment --comment \"default/nginx:\" -j KUBE-MARK-MASQ -A KUBE-SEP-J5YCYOGHESVN34H2 -p tcp -m comment --comment \"default/nginx:\" -m tcp -j DNAT --to-destination 172.18.0.6:80 -A KUBE-SEP-TPLSNUHWNBT6MT2B -s 172.18.0.7/32 -m comment --comment \"default/nginx:\" -j KUBE-MARK-MASQ -A KUBE-SEP-TPLSNUHWNBT6MT2B -p tcp -m comment --comment \"default/nginx:\" -m tcp -j DNAT --to-destination 172.18.0.7:80 -A KUBE-SERVICES -d 172.18.255.127/32 -p tcp -m comment --comment \"default/nginx: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx:\" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-G4A7DIXDTUZ6AHE3 -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-J5YCYOGHESVN34H2 -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment \"default/nginx:\" -j KUBE-SEP-TPLSNUHWNBT6MT2B https://www.bladewan.com/2018/12/10/kubernetes_service_mode/ "},"docs/Kubernetes/k8s-lxcfs-demo.html":{"url":"docs/Kubernetes/k8s-lxcfs-demo.html","title":"Kubernetes容器资源限制和lxcfs问题","summary":"Kubernetes容器资源限制和lxcfs问题","keywords":"","body":"Docker容器资源限制问题 以下是基于腾讯云TKE容器服务测试验证 背景 Linux利用CGroup实现了对容器资源的限制，但是在容器内部还是默认挂载宿主机 /proc 目录下的资源信息文件，如：meminfo,cpuinfo,stat,uptiem，等。当进入Containers执行free，df，top等命令的时候，这时候默认读取的是 /proc 目录内的资源信息文件内容，而这些资源信息文件使用的是宿主机的，所以我们看到的是宿主机的使用信息。 关于LXCFS LXCFS是一个开源的FUSE（用户态文件系统），用来支持LXC容器，也支持Docker容器，社区中常用此工具来实现容器中的资源可见性。 LXCFS原理： 以内存资源为列：通过将宿主机的 /var/lib/lxcfs/meminfo 文件挂载到容器内的/proc/meminfo，然后LXCFS会从容器的CGroup中读取正确的内存限制，然后应用到 /var/lib/lxcfs/meminfo ，这时候容器内部从而就得到了正确的内存信息。 说明：/var/lib/lxcfs/meminfo 是服务启动的时候默认指定的目录。 操作步骤 目前腾讯云TKE里面想实现对容器资源的限制，在容器里面执行free，df，top等命令的时候看到容器真正的资源，有两种方案 方案一：目前 TencentOS Server 特性已支持容器资源展示隔离 增加主机级开关：内核已实现了类似 LXCFS 特性。用户无需在节点部署 LXCFS 文件系统及修改 POD spec，仅需在节点开启全局开关（sysctl -w kernel.stats_isolated=1），/proc/cpuinfo 及 /proc/meminfo 等文件获取即可按容器隔离 增加容器级开关：针对类似节点监控组件等特殊容器，增加了容器级开关 kernel.container_stats_isolated。在主机级开关开启时，仅需在容器启动脚本中关闭容器级开关（sysctl -w kernel.container_stats_isolated=0），即可在容器中读取 /proc/cpuinfo 及 /proc/meminfo 文件时获取到主机信息。 1，环境准备 已创建集群并添加节点，使用操作系统tlinux 2.4，节点规格4C8G [root@VM-0-17-tlinux ~/lxcfs]# uname -r 4.14.105-19-0020.1 [root@VM-0-17-tlinux ~/lxcfs]# sysctl -a| grep kernel.stats_isolated kernel.stats_isolated = 0 #默认是0 2，查看当前主机节点上资源情况 [root@VM-0-17-tlinux ~/lxcfs]# cat /proc/meminfo | grep MemTotal MemTotal: 8035132 kB [root@VM-0-17-tlinux ~/lxcfs]# cat /proc/cpuinfo | grep processor | wc -l 4 [root@VM-0-17-tlinux ~/lxcfs]# free -m total used free shared buff/cache available Mem: 7846 2394 2571 2 2880 5299 Swap: 0 0 0 3，未修改前登录容器查看资源，看到的是宿主机的使用信息。 [root@VM-0-17-tlinux ~/lxcfs]# kubectl exec -it centos-5ccb64bbdb-h7nb9 -- /bin/bash [root@centos-5ccb64bbdb-h7nb9 /]# cat /proc/meminfo | grep MemTotal MemTotal: 8035132 kB [root@centos-5ccb64bbdb-h7nb9 /]# cat /proc/cpuinfo | grep processor | wc -l 4 [root@centos-5ccb64bbdb-h7nb9 /]# free -m total used free shared buff/cache available Mem: 7846 2409 2555 2 2881 5285 Swap: 0 0 0 4， 开启全局开关 （sysctl -w kernel.stats_isolated=1） [root@VM-0-17-tlinux ~]# sysctl -w kernel.stats_isolated=1 kernel.stats_isolated = 1 [root@VM-0-17-tlinux ~]# sysctl -p 5，进一步查看容器资源情况 [root@centos-5ccb64bbdb-h7nb9 /]# cat /proc/meminfo | grep MemTotal MemTotal: 1048576 kB [root@centos-5ccb64bbdb-h7nb9 /]# cat /proc/cpuinfo | grep processor | wc -l 1 [root@centos-5ccb64bbdb-h7nb9 /]# free -m total used free shared buff/cache available Mem: 1024 4 1014 0 4 934 Swap: 1024 0 1024 5，低内核版本不支持该参数 另外一个节点 操作系统是centos 内核版本是 3.10 [root@VM-2-46-centos ~]# sysctl -w kernel.stats_isolated=1 sysctl: cannot stat /proc/sys/kernel/stats_isolated: No such file or directory [root@VM-2-46-centos ~]# uname -a Linux VM-2-46-centos 3.10.0-1160.11.1.el7.x86_64 #1 SMP Fri Dec 18 16:34:56 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux 方案二:使用LXCFS lxcfs官方介绍 在节点上安装LXCFS ， 通过将宿主机的 /var/lib/lxcfs/meminfo 文件挂载到容器内的/proc/meminfo，然后LXCFS会从容器的CGroup中读取正确的内存限制，然后应用到 /var/lib/lxcfs/meminfo ，这时候容器内部从而就得到了正确的内存信息 1，环境准备 （ node节点OS：centos 7.6 ） #安装依赖 yum install -y fuse-libs git clone https://github.com/denverdino/lxcfs-admission-webhook.git cd lxcfs-admission-webhook 2，部署安装 #部署lxcfs daemonset kubectl apply -f deployment/lxcfs-daemonset.yaml #部署lxcfs admission webhook #sh deployment/install.sh #删除的话使用sh deployment/uninstall.sh #执行kubectl get po ，确认所有pod都处于Running状态 [root@VM-2-46-centos lxcfs-admission-webhook]# kubectl get pods | grep lxcfs lxcfs-4bjxh 1/1 Running 0 8m44s lxcfs-56225 1/1 Running 0 8m44s lxcfs-admission-webhook-deployment-58d6fdcf49-jmxd9 1/1 Running 0 8m3s lxcfs-f2gt5 1/1 Running 0 8m44s lxcfs-h6smp 1/1 Running 0 8m44s 3，验证效果，启动lxcfs 对于要使用 lxcfs 的namespace，使用如下命令启用lxcfs admission webhook的自动注入（以lxcf为例）： kubectl label namespace lxcfs lxcfs-admission-webhook=enabled kubectl get ns --show-labels kubectl get pods -n lxcfs #部署POD到centos 7.6 节点上 确认内存信息 确认CPU信息 如果pod设置了cpu limit，看到cpu数量为cpu limit值向上取整 4，卸载清理lxcfs 清理 lxcfs-admission-webhook deployment/uninstall.sh 清理 lxcfs kubectl delete -f deployment/lxcfs-daemonset.yaml lxcfs 支持容器镜像 Centos系统、Ubuntu系统、Debian系统，但是不支持容器镜像 Alpine系统。因为 Alpine 不是使用 Gnu libc，而是使用 musl libc 附注：1.12 TKE集群版本集群验证 在一次处理客户问题时候，客户咨询1.12版本是否支持lxcfs ，所做以下验证，仅作为记录 1，环境准备 centos7.6.0_x64 集群版本1.12 [root@VM-2-2-centos ~]# cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core) [root@VM-2-2-centos ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.1.2.2 Ready 18m v1.12.4-tke.29 10.1.2.2 114.117.211.163 CentOS Linux 7 (Core) 3.10.0-1160.11.1.el7.x86_64 docker://19.3.9 2，查看当前主机资源情况** 3，未安装lxcfs之前部署POD resources: limits: cpu: 800m memory: 1Gi requests: cpu: 200m memory: 256Mi 4，安装lxcfs组件 并查看是否成功 45，测试，销毁刚才创建的工作负载 5，登录POD里面查看资源 6 ，CPU 1:2 内存1:1 参考链接： https://github.com/denverdino/lxcfs-admission-webhook "},"docs/Kubernetes/k8s-pod-data.html":{"url":"docs/Kubernetes/k8s-pod-data.html","title":"Kubernetes中pod数据存储","summary":"Kubernetes中pod数据存储","keywords":"","body":"POD如何使用节点磁盘 K8S中，容器container在运行过程中，会产生一些日志，临时文件，如果没有任何限制的话，会写满POD所在节点磁盘空间，从而会影响对应节点 已经节点上其他POD应用， 容器的临时存储，例如 emptyDir，位于目录/var/lib/kubelet/pods 下 通过如下命令可以查询到集群POD所对应的POD_ID kubectl get pods -o custom-columns=podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeIP:.status.hostIP,Pod_ID:.metadata.uid [root@VM-249-47-tlinux /var/lib/kubelet/pods]# kubectl get pods -o custom-columns=podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeIP:.status.hostIP,Pod_ID:.metadata.uid podName podIP podStatus nodeIP Pod_ID centos-74cd685986-rcfqk 10.200.0.5 Running 172.30.249.47 9ad30306-f6cd-49eb-a279-cd58201be8c3 [root@VM-249-47-tlinux /var/lib/kubelet/pods]# [root@VM-249-47-tlinux /var/lib/kubelet/pods]# tree 9ad30306-f6cd-49eb-a279-cd58201be8c3 9ad30306-f6cd-49eb-a279-cd58201be8c3 #pod的 uid ├── containers # pod 里面的container 容器 │ ├── busybox #容器1 │ │ └── 21efdec2 │ └── centos #容器2 │ └── 64bbf490 ├── etc-hosts # 命名空间的Host文件 ├── plugins │ └── kubernetes.io~empty-dir │ ├── wrapped_cm │ │ └── ready │ ├── wrapped_default-token-7llnd │ │ └── ready │ └── wrapped_secret │ └── ready └── volumes # Pod的卷 ├── kubernetes.io~configmap # ConfigMap类型的卷 │ └── cm │ └── app -> ..data/app ├── kubernetes.io~qcloud-cbs #CBS类型的数据卷 │ └── pvc-137607ae-974d-4c25-90dc-efc3c2a6c5a8 #PV名称，对应POD里面挂载点 │ └── lost+found └── kubernetes.io~secret #Secret类型的卷 ├── default-token-7llnd │ ├── ca.crt -> ..data/ca.crt │ ├── namespace -> ..data/namespace │ └── token -> ..data/token └── secret └── password -> ..data/password 17 directories, 11 files 持久卷的挂载点也位于/var/lib/kubelet/pods 下，但是不会导致存储空间的消耗。 容器的日志，存放在/var/log/pods 目录下 [root@VM-249-47-tlinux /var/log/pods]# ls -lrt | grep centos drwxr-xr-x 4 root root 4096 Aug 7 18:23 default_centos-74cd685986-rcfqk_9ad30306-f6cd-49eb-a279-cd58201be8c3 目录命名方式是：命名空间_POD名称_POD-UID 9ad30306-f6cd-49eb-a279-cd58201be8c3 这个指的就是POD的uid 日志是软链接到/var/lib/docker/containers/容器ID/容器ID-json.log 使用 Docker 时，容器的 rootfs位于/var/lib/docker 下，具体位置取决于存储驱动。 "},"docs/Kubernetes/k8s-deploy-lnmp.html":{"url":"docs/Kubernetes/k8s-deploy-lnmp.html","title":"kubernetes中搭建LNMP环境","keywords":"","body":"在kubernetes中搭建LNMP环境，并安装Discuzx 本实验，需要已经搭建好kubernetes集群和harbor服务（使用腾讯云镜像仓库）。 首先克隆本项目：git clone https://github.com/donxan/k8s_lnmp_discuzx.git 下载镜像 docker pull mysql:5.7 docker pull richarvey/nginx-php-fpm 用dockerfile重建nginx-php-fpm镜像 cd k8s_discuz/dz_web_dockerfile/ docker build -t ccr.ccs.tencentyun.com/v_cjweichen/nginx-php . 将镜像push到镜像仓库 docker login ccr.ccs.tencentyun.com docker push ccr.ccs.tencentyun.com/v_cjweichen/nginx-php:latest docker tag mysql:5.7 ccr.ccs.tencentyun.com/v_cjweichen/mysql:5.7 docker push ccr.ccs.tencentyun.com/v_cjweichen/mysql:5. 搭建MySQL服务 创建secret (设定mysql的root密码) kubectl create secret generic mysql-pass --from-literal=password=DzPasswd1 创建pv cd ../../k8s_discuz/mysql kubectl apply -f mysql-pv.yaml --- apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv spec: accessModes: - ReadWriteMany capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 192.168.1.6 path: /k8s/discuz/db volumeHandle: mysql-pv persistentVolumeReclaimPolicy: Retain storageClassName: cfs volumeMode: Filesystem 创建pvc kubectl apply -f mysql-pvc.yaml --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-claim namespace: lnmp spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: cfs volumeMode: Filesystem volumeName: mysql-pv status: accessModes: - ReadWriteMany capacity: storage: 10Gi 创建deployment kubectl apply -f mysql-dp.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: dz-mysql labels: app: discuz spec: selector: matchLabels: app: discuz tier: mysql strategy: type: Recreate template: metadata: labels: app: discuz tier: mysql spec: imagePullSecrets: - name: harbor-secret containers: - image: ccr.ccs.tencentyun.com/v_cjweichen/mysql:5.7 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: - containerPort: 3306 name: dz-mysql volumeMounts: - name: mysql-persistent-storage mountPath: /data volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-claim 创建service kubectl apply -f mysql-svc.yaml --- apiVersion: v1 kind: Service metadata: name: dz-mysql labels: app: discuz spec: ports: - port: 3306 selector: app: discuz tier: mysql 搭建Nginx+php-fpm服务 注意搭建步骤，在部署mysql时，不能deploy，svc一起执行，需要一步一步来操作。 搭建pv cd ../../k8s_discuz/nginx_php kubectl apply -f web-pv.yaml ---- apiVersion: v1 kind: PersistentVolume metadata: name: web-pv spec: accessModes: - ReadWriteMany capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 192.168.1.6 path: /k8s/discuz/web volumeHandle: mysql-pv persistentVolumeReclaimPolicy: Retain storageClassName: cfs volumeMode: Filesystem 创建pvc kubectl apply -f web-pvc.yaml --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: web-claim namespace: lnmp spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: cfs volumeMode: Filesystem volumeName: web-pv status: accessModes: - ReadWriteMany capacity: storage: 10Gi 创建deployment kubectl apply -f web-dp.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: dz-web labels: app: discuz spec: replicas: 1 selector: matchLabels: app: discuz tier: nginx-php template: metadata: labels: app: discuz tier: nginx-php spec: imagePullSecrets: - name: harbor-secret containers: - image: ccr.ccs.tencentyun.com/v_cjweichen/nginx-php:latest name: dz-web ports: - containerPort: 9000 - containerPort: 80 name: dz-web volumeMounts: - name: mysql-persistent-storage mountPath: /var/www/html/ volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: web-claim 创建service kubectl apply -f web-svc.yaml --- apiVersion: v1 kind: Service metadata: name: dz-web labels: app: discuz spec: type: NodePort ports: - port: 80 nodePort: 30001 selector: app: discuz tier: nginx-php 安装Discuz 下载dz代码 (到NFS服务器上) cd /tmp/ git clone https://gitee.com/Discuz/DiscuzX.git cd /nfsroot/k8s/discuz/web/ mv /tmp/DiscuzX/upload/* . chown -R 100 data uc_server/data/ uc_client/data/ config/ 设置MySQL普通用户 kubectl get svc dz-mysql //查看service的cluster-ip，我的是10.68.122.120 mysql -uroot -h10.68.122.120 -pDzPasswd1 //这里的密码是在上面步骤中设置的那个密码 > create database dz; > grant all on dz.* to 'dz'@'%' identified by 'dz-passwd-123'; 部署traefik ingress cd ../../k8s_discuz/nginx_php kubectl apply -f web-ingress.yaml 如果没有部署ingress，可以使用安装nginx,配置nginx反向代理。 参考如下。 设置Nginx代理 注意：目前nginx服务是运行在kubernetes集群里，node节点以及master节点上是可以通过cluster-ip访问到，但是外部的客户端就不能访问了。 所以，可以在任意一台node或者master上建一个nginx反向代理即可访问到集群内的nginx。 kubectl get svc dz-web //查看cluster-ip，我的ip是10.68.190.99 nginx代理配置文件内容如下： server { listen 80; server_name dz.abcgogo.com; location / { proxy_pass http://10.68.190.99:80; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 做完Nginx代理，就可以通过node的IP来访问discuz了。 安装Discuz 按照提示配置即可。 "},"docs/Kubernetes/k8s-cgroup-killed.html":{"url":"docs/Kubernetes/k8s-cgroup-killed.html","title":"Kubernetes中的cgroup-Killed问题","summary":"Kubernetes中的cgroup Killed问题","keywords":"","body":" 环境： 1，腾讯云TKE集群 2，CVM节点 内核：tlinux 4.14.105-19-0024 3，容器目录和kubelet目录默认设置 Cgroup OOM原因 一般是由于容器的内存实际使用量超过了容器内存限制值limit而导致的事件。比如容器的内存限制值配置了1Gi，而容器的内存随着容器内进程内存使用量的增加超过了1Gi，就会导致容器被操作系统Cgroup Kill。发生容器被Kill之后，容器已经被停止，所以后续会出现应用实例被重启的情况 解决方案 检查容器内进程是否有内存泄漏问题，同时适当调整容器内存的限制值limit大小。可以结合应用监控来看变化趋势。需要注意的是，容器内存限制值大小不应该过大，否则可能导致极端资源争抢情况下，容器被迫驱逐的问题。 oom score 在遇到较高内存使用压力时，Linux 内核会杀掉一些不太重要的进程，腾出空间保障系统正常运行。它会给每个进程（/proc/$pid/oom_score）分配一个得分（oom_score），分数越高，被 OOM 的概率就越大。 这个参数本身只反映该进程的可用资源在系统中所占的百分比，并没有“该进程有多重要”的概念 在kubernetes中各个POD之间资源是通过cgroup进行资源隔离，查看POD cgroup相关信息，登录POD所在节点，执行 cd /sys/fs/cgroup/memory/kubepods/burstable/pod[pod的uuid] #uuid可以通过下面命令查询 示例： cd /sys/fs/cgroup/memory/kubepods/burstable/pod7b5d76c8-a37b-4f1c-8db9-383017063244 查下集群POD的uuid（根据具体情况可以根据命名空间查询） kubectl get pods -o custom-columns=Namespace:..metadata.namespace,podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeIP:.status.hostIP,Pod_ID:.metadata.uid,ContainerName:.spec.containers[*].name 当POD发生OOM时候，可以查看dmesg日志： dmesg | grep -A 20 7b5d76c8-a37b-4f1c-8db9-38301706324 如何通过oom kill日志反查对应的容器 通过dmesg日志查到 有如下oom 信息： dmesg | grep -A 20 -i killed 日志如下 [198542.576728] Task in /kubepods/burstable/pod0f5eb28f-6722-4cf5-ab0f-b62748c018d4/0dc1822caf9e150de25daa1cf572418426ce6b72757b5efdff52acef348368a3 killed as a result of limit of /kubepods/burstable/pod0f5eb28f-6722-4cf5-ab0f-b62748c018d4 [198542.576733] memory: usage 122880kB, limit 122880kB, failcnt 83 [198542.576734] memory+swap: usage 122880kB, limit 9007199254740988kB, failcnt 0 [198542.576734] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0 [198542.576735] Memory cgroup stats for /kubepods/burstable/pod0f5eb28f-6722-4cf5-ab0f-b62748c018d4: cache:0KB rss:0KB rss_huge:0KB shmem:0KB mapped_file:0KB dirty:0KB writeback:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB 可以查到POD的uuid：0f5eb28f-6722-4cf5-ab0f-b62748c018d4 通过kubelet 日志或者message日志过滤该uid journalctl -u kubelet | grep \"0f5eb28f-6722-4cf5-ab0f-b62748c018d4\" #POD uuid 日志如下，可以看到具体POD信息 包括POD名称和所在namespace Dec 11 17:29:21 VM-249-96-tlinux kubelet[10264]: E1211 17:29:21.519957 10264 pod_workers.go:191] Error syncing pod 0f5eb28f-6722-4cf5-ab0f-b62748c018d4 (\"memory-request-limit-54ff657644-wm7pj_default(0f5eb28f-6722-4cf5-ab0f-b62748c018d4)\"), skipping: failed to \"StartContainer\" for \"memory-demo-ctr\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=memory-demo-ctr pod=memory-request-limit-54ff657644-wm7pj_default(0f5eb28f-6722-4cf5-ab0f-b62748c018d4)\" "},"docs/Kubernetes/k8s-docker-logs.html":{"url":"docs/Kubernetes/k8s-docker-logs.html","title":"Kubernetes中的容器日志路径目录","keywords":"","body":"容器日志文件路径 本文大概介绍kubernetes集群中POD容器日志存储位置及把运行日志记录至文件 ，主要包括标准输出日志，容器内文件日志，以及其他持久卷存储位置路径等，仅供参考 环境： 1，TKE集群 2，容器目录和kubelet目录都是默认配置 声明：以下仅个人经验总结，如果有不对的地方勿喷 标准输出 各种容器运行时都提供了对容器标准输出的处理。腾讯云TKE目前支持两种容器运行时——docker和containerd。 docker: 真实路径：/var/lib/docker/containers/$CONTAINERID 软连接：kubelet会在/var/log/pods和/var/log/containers创建软连接指向/var/lib/docker/containers/$CONTAINERID 配置文件：/etc/docker/daemon.json 参数： \"log-driver\": \"json-file\", \"log-level\": \"warn\", \"log-opts\": { \"max-file\": \"10\", \"max-size\": \"100m\" }, containerd : 日志存储路径： 真实路径：/var/log/pods/$CONTAINER_NAMES 软连接：同时kubelet也会在/var/log/containers目录下创建软链接指向 /var/log/pods/$CONTAINER_NAMES 日志配置参数： 配置文件：/etc/kubernetes/kubelet kubelet配置参数: –container-log-max-files=5 \\ –container-log-max-size=“100Mi” \\ –logging-format=“json” \\ docker运行时标准输出日志 如果Docker作为K8S容器运行时，容器日志的落盘将由docker来完成，处理的方式是logging driver，docker支持多种logging drivers，可以将日志以特定的格式输出到不同的目标，保存在/var/lib/docker/containers/ 目录下。Kubelet 会在 /var/log/pods 和 /var/log/containers 下面建立软链接，指向 /var/lib/docker/containers/ 该目录下的容器日志文件 TKE docker节点使用的logging driver是json-file，会将容器的标准输出以 json 的格式写到宿主机的文件里，文件路径为： /var/lib/docker/containers//-json.log #其中容器ID可以通过下面命令查询 批量查下集群里面容器conatiner ID： kubectl get pods -o custom-columns=podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[0].containerID | awk -F 'docker://|| containerd://' '{print $1,$2}' containerd运行时标准输出日志 如果Containerd作为K8 容器运行时， 容器标准输出日志的落盘由 Kubelet 来完成，保存至 /var/log/pods 目录下，同时在 /var/log/containers 目录下创建软链接，指向日志文件 文件路径为： /var/log/pods/__//0.log 批量查询集群POD的uid kubectl get pods -o custom-columns=Namespace:..metadata.namespace,podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeIP:.status.hostIP,Pod_ID:.metadata.uid,ContainerName:.spec.containers[*].name kubelet处理运行时日志 无论使用那种容器运行时，kubelet都会在目录“/var/log/containers”和“/var/log/pods”下创建指向容器标准输出日志文件（目录）的软链接，文件路径如下： /var/log/containers/__-.log /var/log/pods/__//0.log 容器文件路径 容器中的文件路径，是相对于容器的文件系统而言。容器中的文件日志也是落在宿主机node节点上的，可以通过宿主机目录查看到对应文件信息 没有挂载数据卷的目录 在这种情况下，可以根据storage driver（例如：aufs、overlay2）以及容器运行时（docker、containerd），查看容器根目录在宿主机上的挂载点，再根据日志文件在容器文件系统中的路径，找到日志文件在宿主机上的位置。 目前TKE节点主要安装的操作系统是ubuntu、CentOS和tlinux，使用的storage driver为 aufs或者overlay2。 获取容器根目录挂载点的方法如下： docker运行时 容器根目录在宿主机的挂载点 如何查看id aufs /var/lib/docker/aufs/mnt/ cat /var/lib/docker/image/aufs/layerdb/mounts//mount-id overlay2 /var/lib/docker/overlay2//merged cat /var/lib/docker/image/overlay2/layerdb/mounts//mount-id 查询示例： [root@VM-249-47-tlinux ~]# kubectl get pods -o custom-columns=podName:.metadata.name,podIP:.status.podIP,podStatus:.status.phase,nodeName:.spec.nodeName,nodeIP:.status.hostIP,containerID:.status.containerStatuses[0].containerID | awk -F 'docker://|| containerd://' '{print $1,$2}' | grep nginx nginx-0 10.200.0.10 Running 172.30.249.47 172.30.249.47 a0d96814caa6248996dcbcea552fa5f405143500f417a764160010018411f368 #容器ID [root@VM-249-47-tlinux ~]# cat /var/lib/docker/image/overlay2/layerdb/mounts/a0d96814caa6248996dcbcea552fa5f405143500f417a764160010018411f368/mount-id dfdb4b151beca3e68047e748f2832a5acd04161beb4088ca454e7bdf0bdd7aa4 [root@VM-249-47-tlinux ~]# cd /var/lib/docker/overlay2/dfdb4b151beca3e68047e748f2832a5acd04161beb4088ca454e7bdf0bdd7aa4/merged [root@VM-249-47-tlinux /var/lib/docker/overlay2/dfdb4b151beca3e68047e748f2832a5acd04161beb4088ca454e7bdf0bdd7aa4/merged]# ls bin boot dev docker-entrypoint.d docker-entrypoint.sh etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 容器的根目录在宿主机上的挂载点 容器的存储ID找到对应的容器ID和镜像ID 1，进入overlay2目录 cd /var/lib/docker/overlay2 如果遇到日志处理时候可以查看1G以上的文件大小：du -sh * |grep ‘G’ | sort 2，查看占用空间的pid，以及对应的容器名称 docker ps -q | xargs docker inspect --format ‘{{.State.Pid}},{{.Name}},{{.GraphDriver.Data.WorkDir}}’ | grep 4d79558c06c01e7b078cf1829676a06f615371953b15b104cb83d94d4f9c2c43 如果运行的容器没有，可以在镜像里面查下 docker inspect -f $'{{.RepoTags}}\\t{{.GraphDriver.Data.LowerDir}}' $(docker images -q) | grep 4d79558c06c01e7b078cf1829676a06f615371953b15b104cb83d94d4f9c2c43 containerd运行时 /run/containerd/io.containerd.runtime.v1.linux/k8s.io//rootfs /run/containerd/io.containerd.runtime.v2.task/k8s.io//rootfs #最新containerd运行时目录 可以看到，容器根目录在宿主机的挂在点取决于容器的标识（id或者container-id）。 在默认配置/etc/containerd/config.toml 中还有两个关于存储的配置路径 : root = \"/var/lib/containerd\" state = \"/run/containerd\" 其中 root 是用来保存持久化数据，包括 Snapshots, Content, Metadata 以及各种插件的数据，每一个插件都有自己单独的目录，Containerd 本身不存储任何数据，它的所有功能都来自于已加载的插件。 而另外的 state 是用来保存运行时的临时数据的，包括 sockets、pid、挂载点、运行时状态以及不需要持久化的插件数据。 则容器里面的文件日志路径，可以通过宿主机的挂在点获取到， 容器中路径 宿主机路径(docker + overlay2) /data/logs/nginx.log /run/containerd/io.containerd.runtime.v2.task/k8s.io/0b895cd3a0fb017337df1796530904b0185db7469f442b9413333f4c8c878fee/rootfs/data/logs/test.log 挂载数据卷的目录日志 TKE支持以下几种挂载卷类型 hostpath hostpath的处理比较简单，直接查看POD所在节点的挂载路径日志 emptyDir emptyDir卷在宿主机上的路径为: kubelet root-dir的默认值：/var/lib/kubelet /pods//volumes/kubernetes.io~empty-dir/ 腾讯云CBS/CFS cbs持久化卷在宿主机上的路径为： #/pods//volumes/kubernetes.io~qcloud-cbs//mount #qcloud-cbs类型的CBS #container节点和docker运行节点一样 # /plugins/kubernetes.io/csi/pv//globalmount #cbs-csi类型的CBS /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-a8c018b1-6e93-41e2-b620-9585d46d3619/globalmount cfs持久化卷在宿主机上的路径为： #/pods//volumes/kubernetes.io~csi//mount /var/lib/kubelet/pods/4f65675c-ead4-4d75-b1b9-89784a332cf4/volumes/kubernetes.io~csi/pvc-24b1a5c5-f59a-4c5e-8ea8-bad84bacf9df/mount 验证，如果是一个CFS类型的PVC挂载不同的POD上，节点上只有一个挂载点， "},"docs/Kubernetes/kk8s-node-prusere.html":{"url":"docs/Kubernetes/kk8s-node-prusere.html","title":"Kubernetes中节点压力驱逐机制","summary":"Kubernetes中节点压力驱逐机制","keywords":"","body":"K8S节点压力驱逐原理 节点压力驱逐是 kubelet 主动终止 Pod 以回收节点上资源的过程。 kubelet 监控集群节点的 CPU、内存、磁盘空间和文件系统的 inode 等资源。 当这些资源中的一个或者多个达到特定的消耗水平， kubelet 可以主动地使节点上一个或者多个 Pod 失效，以回收资源防止节点崩溃。 在节点压力驱逐期间，kubelet 将所选 Pod 的 PodPhase 设置为 Failed状态。这将终止 Pod，节点压力驱逐不同于 API 发起的驱逐。 kubelet 并不理会你配置的 PodDisruptionBudget 或者是 Pod 的 terminationGracePeriodSeconds。 如果你使用了软驱逐条件，kubelet 会考虑你所配置的 eviction-max-pod-grace-period。 如果你使用了硬驱逐条件，它使用 0s 宽限期来终止 Pod。 如果 Pod 是高级资源对象管理的POD （例如 StatefulSet 或者 Deployment）管理， 则控制平面或 kube-controller-manager 会在其他节点上创建新的 Pod 来代替被驱逐的 Pod。 说明：kubelet 在驱逐Pod 之前会尝试回收节点级资源。 例如，它会在磁盘资源不足时删除未使用的容器镜像。 kubelet 使用各种参数来做出驱逐决定，如下所示： 驱逐信号 驱逐条件 监控间隔 驱逐信号 驱逐信号是特定资源在特定时间点的当前状态。 kubelet 使用驱逐信号，通过将信号与驱逐条件进行比较来做出驱逐决定， 驱逐条件是节点上应该可用资源的最小量。 kubelet 使用以下驱逐信号： 驱逐信号 描述 memory.available memory.available := node.status.capacity[memory] - node.stats.memory.workingSet nodefs.available nodefs.available := node.stats.fs.available nodefs.inodesFree nodefs.inodesFree := node.stats.fs.inodesFree imagefs.available imagefs.available := node.stats.runtime.imagefs.available imagefs.inodesFree imagefs.inodesFree := node.stats.runtime.imagefs.inodesFree pid.available pid.available := node.stats.rlimit.maxpid - node.stats.rlimit.curproc 在上表中，描述列显示了 kubelet 如何获取信号的值。每个信号支持百分比值或者是字面值。 kubelet 计算相对于与信号有关的总量的百分比值。 memory.available 的值来自 cgroupfs，而不是取自 free -m 这样的工具，这很重要，因为 free -m 在容器中不起作用，如果用户使用 节点可分配资源 这一功能特性，资源不足的判定是基于 cgroup 层次结构中的用户 Pod 所处的局部及 cgroup 根节点作出的。 这个脚本 重现了 kubelet 为计算 memory.available 而执行的相同步骤。 kubelet 在其计算中排除了 inactive_file（即非活动 LRU 列表上基于文件来虚拟的内存的字节数）， 因为它假定在压力下内存是可回收的。 kubelet 支持以下文件系统分区： nodefs：节点的主要文件系统，用于本地磁盘卷、emptyDir、日志存储等。 例如，nodefs 包含 /var/lib/kubelet/。 imagefs：可选文件系统，供容器运行时存储容器镜像和容器可写层。 当 nodefs 用量到达阈值，Kubelet 会选择性的驱逐 Pod（及其容器）来释放空间。 当 imagefs 用量到达驱逐阈值，Kubelet 会删除所有未使用的镜像，释放空间。 驱逐条件 你可以为 kubelet 指定自定义驱逐条件，以便在作出驱逐决定时使用。 驱逐条件的形式为 [eviction-signal][operator][quantity]，其中： eviction-signal 是要使用的驱逐信号。 operator 是你想要的关系运算符， 比如 quantity 是驱逐条件数量，例如 1Gi。 quantity 的值必须与 Kubernetes 使用的数量表示相匹配。 你可以使用文字值或百分比（%）。 例如，如果一个节点的总内存为 10Gi 并且你希望在可用内存低于 1Gi 时触发驱逐， 则可以将驱逐条件定义为 memory.available 你可以配置软和硬驱逐条件。 软驱逐条件 软驱逐条件将驱逐条件与用户所必须指定的宽限期配对。 在超过宽限期之前，kubelet 不会驱逐 Pod。 如果没有指定的宽限期，kubelet 会在启动时返回错误。你可以既指定软驱逐条件宽限期，又指定 Pod 终止宽限期的上限，给 kubelet 在驱逐期间使用。 如果你指定了宽限期的上限并且 Pod 满足软驱逐阈条件，则 kubelet 将使用两个宽限期中的较小者。 如果你没有指定宽限期上限，kubelet 会立即杀死被驱逐的 Pod，不允许其体面终止。 你可以使用以下标志来配置软驱逐条件： eviction-soft：一组驱逐条件，如 memory.available eviction-soft-grace-period：一组驱逐宽限期， 如 memory.available=1m30s，定义软驱逐条件在触发 Pod 驱逐之前必须保持多长时间。 eviction-max-pod-grace-period：在满足软驱逐条件而终止 Pod 时使用的最大允许宽限期（以秒为单位）。 硬驱逐条件 硬驱逐条件没有宽限期。当达到硬驱逐条件时， kubelet 会立即杀死 pod，而不会正常终止以回收紧缺的资源。 你可以使用 eviction-hard 标志来配置一组硬驱逐条件， 例如 memory.available kubelet 具有以下默认硬驱逐条件： memory.available nodefs.available imagefs.available nodefs.inodesFree 驱逐监测间隔： kubelet 根据其配置的 housekeeping-interval（默认为 10s）评估驱逐条件。 节点条件 kubelet 报告节点状况以反映节点处于压力之下，因为满足硬或软驱逐条件，与配置的宽限期无关。 kubelet 根据下表将驱逐信号映射为节点状况： 节点条件 驱逐信号 描述 MemoryPressure memory.available 节点上的可用内存已满足驱逐条件 DiskPressure nodefs.available、nodefs.inodesFree、imagefs.available 或 imagefs.inodesFree 节点的根文件系统或镜像文件系统上的可用磁盘空间和 inode 已满足驱逐条件 PIDPressure pid.available (Linux) 节点上的可用进程标识符已低于驱逐条件 kubelet 根据配置的 --node-status-update-frequency 更新节点条件，默认为 10s。 节点条件振荡 在某些情况下，节点在软驱逐条件上下振荡，而没有保持定义的宽限期。 这会导致报告的节点条件在 true 和 false 之间不断切换，从而导致错误的驱逐决策。 为了防止振荡，你可以使用 eviction-pressure-transition-period 标志， 该标志控制 kubelet 在将节点条件转换为不同状态之前必须等待的时间。 过渡期的默认值为 5m。 回收节点级资源 kubelet 在驱逐最终用户 Pod 之前会先尝试回收节点级资源。 当报告 DiskPressure 节点状况时，kubelet 会根据节点上的文件系统回收节点级资源。 有 imagefs 如果节点有一个专用的 imagefs 文件系统供容器运行时使用，kubelet 会执行以下操作： 如果 nodefs 文件系统满足驱逐条件，kubelet 垃圾收集死亡 Pod 和容器。 如果 imagefs 文件系统满足驱逐条件，kubelet 将删除所有未使用的镜像。 没有 imagefs 如果节点只有一个满足驱逐条件的 nodefs 文件系统， kubelet 按以下顺序释放磁盘空间： 对死亡的 Pod 和容器进行垃圾收集 删除未使用的镜像 kubelet 驱逐时 Pod 的选择 如果 kubelet 回收节点级资源的尝试没有使驱逐信号低于条件， 则 kubelet 开始驱逐最终用户 Pod。 kubelet 使用以下参数来确定 Pod 驱逐顺序： Pod 的资源使用是否超过其请求 Pod 优先级 Pod 相对于请求的资源使用情况 因此，kubelet 按以下顺序排列和驱逐 Pod： 首先考虑资源使用量超过其请求的 BestEffort 或 Burstable Pod。 这些 Pod 会根据它们的优先级以及它们的资源使用级别超过其请求的程度被逐出。 资源使用量少于请求量的 Guaranteed Pod 和 Burstable Pod 根据其优先级被最后驱逐。 说明：kubelet 不使用 Pod 的 QoS 类来确定驱逐顺序。 在回收内存等资源时，你可以使用 QoS 类来估计最可能的 Pod 驱逐顺序。 QoS 不适用于临时存储（EphemeralStorage）请求， 因此如果节点在 DiskPressure 下，则上述场景将不适用。 仅当 Guaranteed Pod 中所有容器都被指定了请求和限制并且二者相等时，才保证 Pod 不被驱逐。 这些 Pod 永远不会因为另一个 Pod 的资源消耗而被驱逐。 如果系统守护进程（例如 kubelet 和 journald） 消耗的资源比通过 system-reserved 或 kube-reserved 分配保留的资源多， 并且该节点只有 Guaranteed 或 Burstable Pod 使用的资源少于其上剩余的请求， 那么 kubelet 必须选择驱逐这些 Pod 中的一个以保持节点稳定性并减少资源匮乏对其他 Pod 的影响。 在这种情况下，它会选择首先驱逐最低优先级的 Pod。 当 kubelet 因 inode 或 PID 不足而驱逐 Pod 时， 它使用优先级来确定驱逐顺序，因为 inode 和 PID 没有请求。 kubelet 根据节点是否具有专用的 imagefs 文件系统对 Pod 进行不同的排序： 有 imagefs 如果 nodefs 触发驱逐， kubelet 会根据 nodefs 使用情况（本地卷 + 所有容器的日志）对 Pod 进行排序。 如果 imagefs 触发驱逐，kubelet 会根据所有容器的可写层使用情况对 Pod 进行排序。 没有 imagefs 如果 nodefs 触发驱逐， kubelet 会根据磁盘总用量（本地卷 + 日志和所有容器的可写层）对 Pod 进行排序。 最小驱逐回收 在某些情况下，驱逐 Pod 只会回收少量的紧俏资源。 这可能导致 kubelet 反复达到配置的驱逐条件并触发多次驱逐。 你可以使用 --eviction-minimum-reclaim 标志或 kubelet 配置文件 为每个资源配置最小回收量。 当 kubelet 注意到某个资源耗尽时，它会继续回收该资源，直到回收到你所指定的数量为止。 例如，以下配置设置最小回收量： apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration evictionHard: memory.available: \"500Mi\" nodefs.available: \"1Gi\" imagefs.available: \"100Gi\" evictionMinimumReclaim: memory.available: \"0Mi\" nodefs.available: \"500Mi\" imagefs.available: \"2Gi\" 在这个例子中，如果 nodefs.available 信号满足驱逐条件， kubelet 会回收资源，直到信号达到 1Gi 的条件， 然后继续回收至少 500Mi 直到信号达到 1.5Gi。 类似地，kubelet 会回收 imagefs 资源，直到 imagefs.available 信号达到 102Gi。 对于所有资源，默认的 eviction-minimum-reclaim 为 0。 节点内存不足触发的驱逐 如果节点在 kubelet 能够回收内存之前遇到内存不足（OOM）事件， 则节点依赖 oom_killer 来响应。 kubelet 根据 Pod 的服务质量（QoS）为每个容器设置一个 oom_score_adj 值。 服务质量 oom_score_adj Guaranteed -997 BestEffort 1000 Burstable min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999) 说明：kubelet 还将具有 system-node-critical 优先级 的 Pod 中的容器 oom_score_adj 值设为 -997。 如果 kubelet 在节点遇到 OOM 之前无法回收内存， 则 oom_killer 根据它在节点上使用的内存百分比计算 oom_score， 然后加上 oom_score_adj 得到每个容器有效的 oom_score。 然后它会杀死得分最高的容器。 这意味着低 QoS Pod 中相对于其调度请求消耗内存较多的容器，将首先被杀死。 与 Pod 驱逐不同，如果容器被 OOM 杀死， kubelet 可以根据其 RestartPolicy 重新启动它。 磁盘容量不足触发的驱逐 ​ 当不可压缩资源（内存、磁盘）不足时，节点上的 Kubelet 会尝试驱逐掉某些 Pod，以释放资源，防止整个系统受到影响。 其中，磁盘资源不足的信号来源有两个： imagefs：容器运行时用作存储镜像、可写层的文件系统 nodefs：Kubelet 用作卷、守护进程日志的文件系统 当 imagefs 用量到达驱逐阈值，Kubelet 会删除所有未使用的镜像，释放空间。 当 nodefs 用量到达阈值，Kubelet 会选择性的驱逐 Pod（及其容器）来释放空间。 本地临时存储触发的驱逐 K8S 支持设置每个 Pod 可以使用的临时存储的 request/limit，驱逐行为可以更具有针对性。如果 Pod 使用了超过限制的本地临时存储，Kubelet 将设置驱逐信号，触发 Pod 驱逐流程： 对于容器级别的隔离，如果一个容器的可写层、日志占用磁盘超过限制，则 Kubelet 标记 Pod 为待驱逐 对于 Pod 级别的隔离，Pod 总用量限制，是每个容器限制之和。如果各容器用量之和+Pod 的 emptyDir 卷超过 Pod 总用量限制，标记 Pod 为待驱逐 驱逐问题最佳实践 以下部分描述了驱逐配置的最佳实践。 可调度的资源和驱逐策略 当你为 kubelet 配置驱逐策略时， 你应该确保调度程序不会在 Pod 触发驱逐时对其进行调度，因为这类 Pod 会立即引起内存压力。 考虑以下场景： 节点内存容量：10Gi 操作员希望为系统守护进程（内核、kubelet 等）保留 10% 的内存容量 操作员希望在节点内存利用率达到 95% 以上时驱逐 Pod，以减少系统 OOM 的概率。 为此，kubelet 启动设置如下： --eviction-hard=memory.available在此配置中，--system-reserved 标志为系统预留了 1.5Gi 的内存， 即 总内存的 10% + 驱逐条件量。 如果 Pod 使用的内存超过其请求值或者系统使用的内存超过 1Gi， 则节点可以达到驱逐条件，这使得 memory.available 信号低于 500Mi 并触发条件。 DaemonSet驱逐 Pod 优先级是做出驱逐决定的主要因素。 如果你不希望 kubelet 驱逐属于 DaemonSet 的 Pod， 请在 Pod 规约中为这些 Pod 提供足够高的 priorityClass。 你还可以使用优先级较低的 priorityClass 或默认配置， 仅在有足够资源时才运行 DaemonSet Pod。 "},"docs/Kubernetes/k8s-networkpolicy.html":{"url":"docs/Kubernetes/k8s-networkpolicy.html","title":"NetworkPolicy进行网络访问控制","keywords":"","body":"Network Policy Network Policy 是 Kubernetes 提供的一种资源，用于定义基于 Pod 的网络隔离策略。描述了一组 Pod 是否可以与其他组 Pod，以及其他 network endpoints 进行通信。 在 TKE 上启用 NetworkPolicy 扩展组件 具体操作步骤可参见 NetworkPolicy 说明。 本次测试环境：集群版本1.18 容器网络 ： 172.16.0.0/16 集群网络： 172.30.0.0/16 NetworkPolicy 配置示例 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: nginx qcloud-app: nginx name: nginx namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: nginx qcloud-app: nginx template: metadata: labels: k8s-app: nginx qcloud-app: nginx spec: containers: - image: nginx:stable imagePullPolicy: IfNotPresent name: nginx resources: {} --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos qcloud-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos qcloud-app: centos template: metadata: labels: k8s-app: centos qcloud-app: centos spec: containers: - image: ccr.ccs.tencentyun.com/v_cjweichen/centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} [root@VM-249-8-tlinux ~]# kubectl get pods -A -o wide | grep -v kube-system NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES default centos-59cd95cbbf-jjm6j 1/1 Running 0 3m15s 172.16.0.5 172.30.249.8 default nginx-686bf668dc-vgndc 1/1 Running 0 2m38s 172.16.0.6 172.30.249.8 nsa centos-f47c886c9-l2tk2 1/1 Running 0 3m51s 172.16.0.4 172.30.249.8 nsa nginx-686bf668dc-kckln 1/1 Running 0 2m4s 172.16.0.7 172.30.249.8 案例一：nsa namespace 下的 Pod 可互相访问，而不能被其他任何 Pod 访问。 为 部署network policy 之前是可以正常访问 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: npa namespace: nsa spec: ingress: - from: - podSelector: {} podSelector: {} policyTypes: - Ingress 在nsa 命名空间下的centos POD访问同一个命名空间（nsa）下的nginx服务 在default 命名空间下的centos 访问nsa命名空间下nginx服务，无法访问 或使用命令行 [root@VM-249-8-tlinux ~]# kubectl exec -it centos-66dbbdc5b6-8cbk4 -- curl -I 172.16.0.7 案例二： nsa namespace 下的 Pod 不能被任何 Pod 访问 [root@VM-249-8-tlinux ~]# kubectl -n nsa delete NetworkPolicy npa networkpolicy.networking.k8s.io \"npa\" deleted apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: npa namespace: nsa spec: podSelector: {} policyTypes: - Ingress [root@VM-249-8-tlinux ~]# kubectl -n nsa get NetworkPolicy NAME POD-SELECTOR AGE npa 46s 在nsa 命名空间下的centos POD访问同一个命名空间（nsa）下的nginx服务，无法访问 在default 命名空间下的centos 访问nsa命名空间下nginx服务，无法访问 案例三： nsa namespace 下的 Pod 只在 80/TCP 端口可以被带有标签 app: nsb 的 namespace 下的 Pod 访问，而不能被其他任何 Pod 访问。 root@VM-249-8-tlinux ~]# kubectl -n nsa delete NetworkPolicy npa networkpolicy.networking.k8s.io \"npa\" deleted apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: npa namespace: nsa spec: ingress: - from: - namespaceSelector: matchLabels: app: nsb ports: - port: 80 protocol: TCP podSelector: {} policyTypes: - Ingress 目前集群内所有的namespace 都没有app: nsb 标签，访问失败 给default命名空间打labels [root@VM-249-8-tlinux ~]# kubectl label ns default app=nsb namespace/default labeled [root@VM-249-8-tlinux ~]# kubectl get ns --show-labels NAME STATUS AGE LABELS default Active 52m app=nsb "},"docs/Kubernetes/k8s-pod-create.html":{"url":"docs/Kubernetes/k8s-pod-create.html","title":"POD创建和更新流程","keywords":"","body":" pod 状态变更：将 Pod 设置为 Terminating 状态，并从所有 Service 的 Endpoints 列表中删除。此时，Pod 停止获得新的流量，但在 Pod 中运行的容器不会受到影响； 执行 preStop Hook：Pod 删除时会触发 preStop Hook，preStop Hook 支持 bash 脚本、TCP 或 HTTP 请求； 发送 SIGTERM 信号：向 Pod 中的容器发送 SIGTERM 信号； 等待指定的时间：terminationGracePeriodSeconds 字段用于控制等待时间，默认值为 30 秒。该步骤与 preStop Hook 同时执行，因此 - - terminationGracePeriodSeconds 需要大于 preStop 的时间，否则会出现 preStop 未执行完毕，pod 就被 kill 的情况； 发送 SIGKILL 信号：等待指定时间后，向 pod 中的容器发送 SIGKILL 信号，删除 pod。 参考文档：https://www.sohu.com/a/400910043_612370 pod的状态分析 Pod状态 状态 描述 Running 该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 Pending Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。创建pod的请求已经被k8s接受，但是容器并没有启动成功，可能处在：写数据到etcd，调度，pull镜像，启动容器这四个阶段中的任何一个阶段，pending伴随的事件通常会有：ADDED, Modified这两个事件的产生 Succeeded Pod中的所有的容器已经正常的自行退出，并且k8s永远不会自动重启这些容器，一般会是在部署job的时候会出现。 Failed Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。 Unknown 出于某种原因，无法获得Pod的状态，通常是由于与Pod主机通信时出错。 Pod 的详细的状态说明 状态 描述 CrashLoopBackOff 容器退出，kubelet正在将它重启 InvalidImageName 无法解析镜像名称 ImageInspectError 无法校验镜像 ErrImageNeverPull 策略禁止拉取镜像 ImagePullBackOff 正在重试拉取 RegistryUnavailable 连接不到镜像中心 ErrImagePull 通用的拉取镜像出错 CreateContainerConfigError 不能创建kubelet使用的容器配置 CreateContainerError 创建容器失败 m.internalLifecycle.PreStartContainer 执行hook报错 RunContainerError 启动容器失败 PostStartHookError 执行hook报错 ContainersNotInitialized 容器没有初始化完毕 ContainersNotRead 容器没有准备完毕 ContainerCreating 容器创建中 PodInitializing pod 初始化中 DockerDaemonNotReady docker还没有完全启动 NetworkPluginNotReady 网络插件还没有完全启动 "},"docs/Kubernetes/k8s-pod-scheduler.html":{"url":"docs/Kubernetes/k8s-pod-scheduler.html","title":"Pod调度策略","keywords":"","body":"Pod 调度策略 概述 节点亲和性：设置节点亲和性，通过Worker节点的Label标签进行设置。 节点调度支持硬约束和软约束（Required/Preferred），以及丰富的匹配表达式（In, NotIn, Exists, DoesNotExist. Gt, and Lt）： 必须满足，即硬约束，一定要满足，对应requiredDuringSchedulingIgnoredDuringExecution，效果与NodeSelector相同。本例中Pod只能调度到具有对应标签的Worker节点。您可以定义多条硬约束规则，但只需满足其中一条。 尽量满足，即软约束，不一定满足，对应preferredDuringSchedulingIgnoredDuringExecution。本例中，调度会尽量不调度Pod到具有对应标签的Node节点。您还可为软约束规则设定权重，具体调度时，若存在多个符合条件的节点，权重最大的节点会被优先调度。您可定义多条软约束规则，但必须满足全部约束，才会进行调度。 POD亲和性：决定应用的Pod可以和哪些Pod部署在同一拓扑域。例如，对于相互通信的服务，可通过应用亲和性调度，将其部署到同一拓扑域（如同一个主机）中，减少它们之间的网络延迟。 根据节点上运行的Pod的标签（Label）来进行调度，支持硬约束和软约束，匹配的表达式有：In, NotIn, Exists, DoesNotExist。 必须满足 ，即硬约束，一定要满足，对应 requiredDuringSchedulingIgnoredDuringExecution ，Pod的亲和性调度必须要满足后续定义的约束条件。 命名空间：该策略是依据Pod的Label进行调度，所以会受到命名空间的约束。 拓扑域：即TopologyKey，指定调度时作用域，这是通过Node节点的标签来实现的，例如指定为kubernetes.io/hostname，那就是以Node节点为区分范围；如果指定为beta.kubernetes.io/os，则以Node节点的操作系统类型来区分。 选择器：单击选择器右侧的加号按钮，您可添加多条硬约束规则。 查看应用列表：单击应用列表，弹出对话框，您可在此查看各命名空间下的应用，并可将应用的标签导入到亲和性配置页面。 硬约束条件：设置已有应用的标签、操作符和标签值。本例中，表示将待创建的应用调度到该主机上，该主机运行的已有应用具有app:nginx标签。 尽量满足 ，即软约束，不一定满足，对应 preferredDuringSchedulingIgnoredDuringExecution 。Pod的亲和性调度会尽量满足后续定义的约束条件。对于软约束规则，您可配置每条规则的权重，其他配置规则与硬约束规则相同。 说明 权重：设置一条软约束规则的权重，介于1~100，通过算法计算满足软约束规则的节点的权重，将Pod调度到权重最大的节点上。 POD反亲和性：决定应用的Pod不与哪些Pod部署在同一拓扑域。应用非亲和性调度的场景包括： 将一个服务的Pod分散部署到不同的拓扑域（如不同主机）中，提高服务本身的稳定性。 给予Pod一个节点的独占访问权限来保证资源隔离，保证不会有其它Pod来分享节点资源。 把可能会相互影响的服务的Pod分散在不同的主机上。 说明 应用非亲和性调度的设置方式与亲和性调度相同，但是相同的调度规则代表的意思不同，请根据使用场景进行选择。 调度容忍：容忍被应用于Pod，允许这个Pod被调度到相对应的污点上。 将 Pod 打散调度到不同地方，可避免因软硬件故障、光纤故障、断电或自然灾害等因素导致服务不可用，以实现服务的高可用部署。 Kubernetes 支持两种方式将 Pod 打散调度: Pod 反亲和 (Pod Anti-Affinity) Pod 拓扑分布约束 (Pod Topology Spread Constraints) 使用 podAntiAffinity 将 Pod 强制打散调度到不同节点上(强反亲和)，以避免单点故障: apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: nginx containers: - name: nginx image: nginx labelSelector.matchLabels 替换成选中 Pod 实际使用的 label。 topologyKey: 节点的某个 label 的 key，能代表节点所处拓扑域，可以用 Well-Known Labels，常用的是 kubernetes.io/hostname (节点维度)、topology.kubernetes.io/zone (可用区/机房 维度)。也可以自行手动为节点打上自定义的 label 来定义拓扑域，比如 rack (机架维度)、machine (物理机维度)、switch (交换机维度)。 若不希望用强制，可以使用弱反亲和，让 Pod 尽量调度到不同节点: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: topologyKey: kubernetes.io/hostname weight: 100 将 Pod 强制打散调度到不同可用区(机房)，以实现跨机房容灾: 将 kubernetes.io/hostname 换成 topology.kubernetes.io/zone，其余同上。 使用 topologySpreadConstraints 将 Pod 最大程度上均匀的打散调度到各个节点上: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-topologyspreadconstraints namespace: podaffinity spec: replicas: 1 selector: matchLabels: app: nginx-topologyspreadconstraints template: metadata: labels: app: nginx-topologyspreadconstraints spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: nginx-topologyspreadconstraints containers: - name: nginx image: nginx topologyKey: 与 podAntiAffinity 中配置类似。 labelSelector: 与 podAntiAffinity 中配置类似，只是这里可以支持选中多组 pod 的 label。 maxSkew: 必须是大于零的整数，表示能容忍不同拓扑域中 Pod 数量差异的最大值。这里的 1 意味着只允许相差 1 个 Pod。 whenUnsatisfiable: 指示不满足条件时如何处理。DoNotSchedule 不调度 (保持 Pending)，类似强反亲和；ScheduleAnyway 表示要调度，类似弱反亲和； 以上配置连起来解释: 将所有 nginx 的 Pod 严格均匀打散调度到不同节点上，不同节点上 nginx 的副本数量最多只能相差 1 个，如果有节点因其它因素无法调度更多的 Pod (比如资源不足)，那么就让剩余的 nginx 副本 Pending。 所以，如果要在所有节点中严格打散，通常不太可取，可以加下 nodeAffinity，只在部分资源充足的节点严格打散: spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: io operator: In values: - high topologySpreadConstraints: - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: DoNotSchedule labelSelector: - matchLabels: app: nginx 或者类似弱反亲和， 将 Pod 尽量均匀的打散调度到各个节点上，不强制 (DoNotSchedule 改为 ScheduleAnyway): spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: ScheduleAnyway labelSelector: - matchLabels: app: nginx 如果集群节点支持跨可用区，也可以 将 Pod 尽量均匀的打散调度到各个可用区 以实现更高级别的高可用 (topologyKey 改为 topology.kubernetes.io/zone): spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: ScheduleAnyway labelSelector: - matchLabels: app: nginx 更进一步地，可以 将 Pod 尽量均匀的打散调度到各个可用区的同时，在可用区内部各节点也尽量打散: spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: ScheduleAnyway labelSelector: - matchLabels: app: nginx - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: ScheduleAnyway labelSelector: - matchLabels: app: nginx 小结 从示例能明显看出，topologySpreadConstraints 比 podAntiAffinity 功能更强，提供了提供更精细的调度控制，我们可以理解成 topologySpreadConstraints 是 podAntiAffinity 的升级版。topologySpreadConstraints 特性在 K8S v1.18 默认启用，所以建议 v1.18 及其以上的集群使用 topologySpreadConstraints 来打散 Pod 的分布以提高服务可用性。 参考资料 Pod Topology Spread Constraints 文档链接：https://imroc.cc/k8s/ha/pod-split-up-scheduling/ 尽量调度到不同节点 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: - kube-dns topologyKey: kubernetes.io/hostname 必须调度到不同节点 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - weight: 100 labelSelector: matchExpressions: - key: k8s-app operator: In values: - kube-dns topologyKey: kubernetes.io/hostname 只调度到有指定 label 的节点 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: matchExpressions: - key: ingress operator: In values: - true "},"docs/Kubernetes/port-forward.html":{"url":"docs/Kubernetes/port-forward.html","title":"Port-forward访问集群内应用","keywords":"","body":"port-forward访问TKE集群中的应用程序 本文描述了如何使用 kubectl port-forward 访问 Kubernetes 集群中的 redis server。这种连接方式在实际进行Debug时非常有效。 1，部署Redis服务 YAML文件如下： apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: redis qcloud-app: redis name: redis namespace: default spec: replicas: selector: matchLabels: k8s-app: redis qcloud-app: redis template: metadata: labels: k8s-app: redis qcloud-app: redis spec: containers: - image: redis:latest imagePullPolicy: Always name: redis resources: limits: cpu: 200m memory: 256Mi requests: cpu: 200m memory: 256Mi ports: - containerPort: 6379 创建Redis服务，YAML文件如下所示： apiVersion: v1 kind: Service metadata: labels: k8s-app: redis qcloud-app: redis name: redis namespace: default spec: ports: - name: 6379-6379-tcp port: 6379 protocol: TCP targetPort: 6379 selector: k8s-app: redis qcloud-app: redis type: ClusterIP 执行命令，检查POD 和 Service创建结果 [root@VM-1-14-tlinux ~]# kubectl get pods NAME READY STATUS RESTARTS AGE redis-7cdccd6f4-6mszh 1/1 Running 0 2m13s [root@VM-1-14-tlinux ~]# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE redis 1/1 1 1 2m24s [root@VM-1-14-tlinux ~]# kubectl get service | grep redis redis ClusterIP 172.21.252.118 6379/TCP 7m10s 验证 Redis Service已经运行，并监听了 6379 端口 ： [root@VM-1-14-tlinux ~]# kubectl get pods redis-7cdccd6f4-6mszh --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{\"\\n\"}}' 6379 2，转发本地端口到Pod的端口 使用 kubectl port-forward 命令，用户可以使用资源的名称来进行端口转发。下面的命令中的任意一行，都可以实现端口转发的效果： # 这几个命令执行任意一个即可 kubectl port-forward redis-7cdccd6f4-6mszh 7000:6379 kubectl port-forward pods/redis-7cdccd6f4-6mszh 7000:6379 kubectl port-forward deployment/redis 7000:6379 kubectl port-forward rs/redis 7000:6379 kubectl port-forward svc/redis 7000:6379 以上命令的输出结果类似： [root@VM-1-14-tlinux ~]# kubectl port-forward redis-7cdccd6f4-6mszh 7000:6379 Forwarding from 127.0.0.1:7000 -> 6379 Forwarding from [::1]:7000 -> 6379 3，redis-cli连接方法 安装redis-cli（如已安装，可跳过）。 登录待安装redis-cli的设备，例如CVM机器 执行下述命令下载Redis源码文件： wget https://github.com/redis/redis/archive/7.0.2.tar.gz 说明 具体操作，请参见Redis官网。 执行下述命令解压Redis源码文件： tar xzf redis-6.0.9.tar.gz 执行下述命令进入解压后的目录并编译安装Redis源码文件： cd redis-6.0.9&&make 说明 编译安装需要一段时间（通常2分钟~3分钟）。 启动 Redis 命令行： [root@VM-1-14-tlinux ~/redis-7.0.2]# src/redis-cli -p 7000 127.0.0.1:7000> ping PONG Redis 服务器将返回 PONG ，表示链接成功 由于一些限制，port-forward 目前只支持 TCP 协议，issue 47862 (opens new window)用来跟进对 UDP 协议的支持。 MySQL数据库等使用 TCP 协议的部署在K8S集群中的服务器，都可以使用此方式进行 DEBUG "},"docs/Kubernetes/k8s-kubectl-table.html":{"url":"docs/Kubernetes/k8s-kubectl-table.html","title":"启动kubectl自动补全功能","keywords":"","body":"kubectl 配置别名 # vim ~/.bashrc 添加 alias k='kubectl' # source ~/.bashrc 启用 shell 自动补全功能 kubectl 为 Bash 和 Zsh 提供自动补全功能，可以减轻许多输入的负担。 简介 kubectl 的 Bash 补全脚本可以用命令 kubectl completion bash 生成。 在 shell 中导入（Sourcing）补全脚本，将启用 kubectl 自动补全功能。 然而，补全脚本依赖于工具 bash-completion， 所以要先安装它（可以用命令 type _init_completion 检查 bash-completion 是否已安装） root@VM-0-17-tlinux ~]# type _init_completion -bash: type: _init_completion: not found #表示未安装 安装 bash-completion apt-get install bash-completion 或 yum install bash-completion 等命令来安装它 yum install bash-completion -y #上述命令将创建文件 /usr/share/bash-completion/bash_completion，它是 bash-completion 的主脚本 ls /usr/share/bash-completion/bash_completion 依据包管理工具的实际情况，你需要在 ~/.bashrc 文件中手工导入此文件。 [root@VM-0-17-tlinux ~]# source /usr/share/bash-completion/bash_completion #重新加载 shell，再输入命令 type _init_completion 来验证 bash-completion 的安装状态 [root@VM-0-17-tlinux ~]# type _init_completion 启动 kubectl 自动补全功能 现在需要确保一点：kubectl 补全脚本已经导入（sourced）到 shell 会话中。 这里有两种验证方法： 在文件 ~/.bashrc 中导入（source）补全脚本： echo 'source >~/.bashrc 将补全脚本添加到目录 /etc/bash_completion.d 中： kubectl completion bash >/etc/bash_completion.d/kubectl 参考链接：https://kubernetes.io/zh/docs/tasks/tools/install-kubectl-linux/ 最简单： 配置别名 # vim ~/.bashrc 添加 alias k=‘kubectl‘ # source ~/.bashrc 配置命令行补齐 # yum install -y bash-completion # chmod +x /usr/share/bash-completion/bash_completion # /usr/share/bash-completion/bash_completion # source /usr/share/bash-completion/bash_completion # source no node 问题：在使用命令行补齐时，必须使用kubectl的全称，使用k不行。 优化： # source no mas 为了每次登录，都可以自动加载： # echo \"source > /etc/bashrc yum -y install bash-completion #执行安装 chmod +x /usr/share/bash-completion/bash_completion source /usr/share/bash-completion/bash_completion #执行加载 source > ~/.bashrc #当前用户永久生效 "},"docs/Kubernetes/k8s-pod-envpare.html":{"url":"docs/Kubernetes/k8s-pod-envpare.html","title":"通过环境变量将POD信息呈现给容器","summary":"通过环境变量将POD信息呈现给容器","keywords":"","body":"本文通过环境变量将 Pod 信息呈现给容器 用 Pod 字段作为环境变量的值 这个配置文件中，你可以看到五个环境变量。env 字段是一个 EnvVars. 对象的数组。 数组中第一个元素指定 MY_NODE_NAME 这个环境变量从 Pod 的 spec.nodeName 字段获取变量值。 同样，其它环境变量也是从 Pod 的字段获取它们的变量值。 说明： 本示例中的字段是 Pod 字段，不是 Pod 中 Container 的字段。 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: dapi-envars-fieldref name: dapi-envars-fieldref namespace: default spec: selector: matchLabels: k8s-app: dapi-envars-fieldref template: metadata: labels: k8s-app: dapi-envars-fieldref spec: containers: - args: - while true; do echo -en '\\n'; printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE; printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT; sleep 10;done; command: - sh - -c env: - name: MY_NODE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIPs - name: MY_POD_SERVICE_ACCOUNT valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.serviceAccountName image: busybox imagePullPolicy: IfNotPresent name: dapi-envars-fieldref resources: {} 用 Container 字段作为环境变量的值 前面的练习中，你将 Pod 字段作为环境变量的值。 接下来这个练习中，你将用 Container 字段作为环境变量的值。这里是包含一个容器的 Pod 的配置文件 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: dapi-envars-container name: dapi-envars-container namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: dapi-envars-container template: metadata: labels: k8s-app: dapi-envars-container spec: containers: - args: - while true; do echo -en '\\n'; printenv MY_CPU_REQUEST MY_CPU_LIMIT;printenv MY_MEM_REQUEST MY_MEM_LIMIT;sleep 10;done; command: - sh - -c env: - name: MY_CPU_REQUEST valueFrom: resourceFieldRef: containerName: test-container divisor: \"1\" resource: requests.cpu - name: MY_CPU_LIMIT valueFrom: resourceFieldRef: containerName: test-container divisor: \"1\" resource: limits.cpu - name: MY_MEM_REQUEST valueFrom: resourceFieldRef: containerName: test-container divisor: \"1\" resource: requests.memory - name: MY_MEM_LIMIT valueFrom: resourceFieldRef: containerName: test-container divisor: \"1\" resource: limits.memory image: busybox:1.24 imagePullPolicy: IfNotPresent name: test-container resources: limits: cpu: 200m memory: 128Mi requests: cpu: 100m memory: 64Mi 这个配置文件中，你可以看到四个环境变量。env 字段是一个 EnvVars. 对象的数组。数组中第一个元素指定 MY_CPU_REQUEST 这个环境变量从 Container 的 requests.cpu 字段获取变量值。同样，其它环境变量也是从 Container 的字段获取它们的变量值。 说明： 本例中使用的是 Container 的字段而不是 Pod 的字段。 通过文件将 Pod 信息呈现给容器 此页面描述 Pod 如何使用 DownwardAPIVolumeFile 把自己的信息呈现给 Pod 中运行的容器。 DownwardAPIVolumeFile 可以呈现 Pod 的字段和容器字段 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: kubernetes-downwardapi-volume-example name: kubernetes-downwardapi-volume-example namespace: default spec: selector: matchLabels: k8s-app: kubernetes-downwardapi-volume-example template: metadata: labels: k8s-app: kubernetes-downwardapi-volume-example spec: containers: - args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/annotations ]]; then echo -en '\\n\\n'; cat /etc/podinfo/annotations; fi; sleep 5;done; command: - sh - -c image: busybox imagePullPolicy: IfNotPresent name: client-container resources: {} volumeMounts: - name: podinfo mountPath: /etc/podinfo volumes: - name: podinfo downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels - path: \"annotations\" fieldRef: fieldPath: metadata.annotations Downward API支持的常用字段 1，使用fieldRef可以声明使用: spec.nodeName - 宿主机名字 status.hostIP - 宿主机IP metadata.name - Pod的名字 metadata.namespace - Pod的Namespace status.podIP - Pod的IP spec.serviceAccountName - Pod的Service Account的名字 metadata.uid - Pod的UID metadata.labels[''] - 指定的Label值 metadata.annotations[''] - 指定的Annotation值 metadata.labels - Pod的所有Label metadata.annotations - Pod的所有Annotation 2，使用resourceFieldRef可以声明使用: limits.cpu-容器的CPU limit requests.cpu-容器的CPU request limits.memory-容器的memory limit requests.memory容器的memory request 完整示例： apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos-env qcloud-app: centos-env name: centos-env spec: selector: matchLabels: k8s-app: centos-env template: metadata: annotations: description: ddd labels: k8s-app: centos-env qcloud-app: centos-env spec: containers: - env: - name: pod-name valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: pod-namespacce valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: pod-labels valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.labels['k8s-app'] - name: pod-annotations valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.annotations['description'] - name: pod-uid valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.uid - name: pod-nodename valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: pod-hostIP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.hostIP - name: pod-podIP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP - name: pod-serviceAccountName valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.serviceAccountName image: ccr.ccs.tencentyun.com/chenjingwei/centos:latest imagePullPolicy: IfNotPresent name: centos-1 resources: limits: cpu: 25m memory: 64Mi requests: cpu: 25m memory: 64Mi volumeMounts: - mountPath: /etc/podinfo name: podinfo readOnly: false imagePullSecrets: - name: qcloudregistrykey volumes: - name: podinfo projected: sources: - downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels - path: \"annotations\" fieldRef: fieldPath: metadata.annotations "},"docs/Kubernetes/k8s-cpu-resource.html":{"url":"docs/Kubernetes/k8s-cpu-resource.html","title":"为容器和Pods分配CPU资源","keywords":"","body":"为容器和 Pods 分配 CPU 资源 本章展示如何为容器设置 CPU request（请求） 和 CPU limit（限制）。 容器使用的 CPU 不能超过所配置的限制。 如果系统有空闲的 CPU 时间，则可以保证给容器分配其所请求数量的 CPU 资源 创建一个名字空间 创建一个名字空间，以便将 本练习中创建的资源与集群的其余部分资源隔离（本例和内存章节使用同一个命名空间验证） kubectl create namespace cpu-example 指定 CPU 请求和 CPU 限制 要为容器指定 CPU 请求，请在容器资源清单中包含 resources: requests 字段。 要指定 CPU 限制，请包含 resources:limits。 在本练习中，你将创建一个具有一个容器的 Pod。容器将会请求 0.5 个 CPU，而且最多限制使用 1 个 CPU。 这是 Pod 的配置文件：cpu-request-limit.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: cpu-request-limit qcloud-app: cpu-request-limit name: cpu-request-limit namespace: mem-example spec: replicas: 1 selector: matchLabels: k8s-app: cpu-request-limit qcloud-app: cpu-request-limit template: metadata: labels: k8s-app: cpu-request-limit qcloud-app: cpu-request-limit spec: containers: - args: - -cpus - \"2\" image: vish/stress:latest imagePullPolicy: IfNotPresent name: cpu-demo-ctr resources: limits: cpu: \"1\" requests: cpu: 500m 配置文件的 args 部分提供了容器启动时的参数。 -cpus \"2\" 参数告诉容器尝试使用 2 个 CPU。 创建 Pod 验证所创建的 Pod 处于 Running 状态 [root@VM-0-17-tlinux ~/memonry]# kubectl get pod -n mem-example NAME READY STATUS RESTARTS AGE cpu-request-limit-7bbdd6c6c8-789ks 1/1 Running 0 2m20s 查看显示关于 Pod 的详细信息： resources: limits: cpu: \"1\" requests: cpu: 500m 使用 kubectl top 命令来获取该 Pod 的度量值数据： 此示例输出显示 Pod 使用的是 999milliCPU，即略低于 Pod 配置中指定的 1 个 CPU 的限制。 [root@VM-0-17-tlinux ~/memonry]# kubectl top pod cpu-request-limit-7bbdd6c6c8-789ks -n mem-example NAME CPU(cores) MEMORY(bytes) cpu-request-limit-7bbdd6c6c8-789ks 999m 1Mi 通过设置 -cpu \"2\"，你将容器配置为尝试使用 2 个 CPU， 但是容器只被允许使用大约 1 个 CPU。 容器的 CPU 用量受到限制，因为该容器正尝试使用超出其限制的 CPU 资源。 CPU 使用率低于 1.0 的另一种可能的解释是，节点可能没有足够的 CPU 资源可用。 回想一下，此练习的先决条件需要你的节点至少具有 1 个 CPU 可用。 如果你的容器在只有 1 个 CPU 的节点上运行，则容器无论为容器指定的 CPU 限制如何， 都不能使用超过 1 个 CPU。 CPU 单位 小数值是可以使用的。一个请求 0.5 CPU 的容器保证会获得请求 1 个 CPU 的容器的 CPU 的一半。 你可以使用后缀 m 表示毫。例如 100m CPU、100 milliCPU 和 0.1 CPU 都相同。 精度不能超过 1m。 CPU 请求只能使用绝对数量，而不是相对数量。0.1 在单核、双核或 48 核计算机上的 CPU 数量值是一样的。 设置超过节点能力的 CPU 请求 CPU 请求和限制与都与容器相关，但是我们可以考虑一下 Pod 具有对应的 CPU 请求和限制这样的场景。 Pod 对 CPU 用量的请求等于 Pod 中所有容器的请求数量之和。 同样，Pod 的 CPU 资源限制等于 Pod 中所有容器 CPU 资源限制数之和。 Pod 调度是基于资源请求值来进行的。 仅在某节点具有足够的 CPU 资源来满足 Pod CPU 请求时，Pod 将会在对应节点上运行 "},"docs/Kubernetes/k8s-mem-resource.html":{"url":"docs/Kubernetes/k8s-mem-resource.html","title":"为容器和Pod配内存资源","keywords":"","body":"为容器和 Pod 分配内存资源 此页面展示如何将内存 请求 （request）和内存 限制 （limit）分配给一个容器。 我们保障容器拥有它请求数量的内存，但不允许使用超过限制数量的内存。 1，创建命名空间 kubectl create namespace mem-example 2，指定内存请求和限制 要为容器指定内存请求，请在容器资源清单中包含 resources：requests 字段。 同理，要指定内存限制，请包含 resources：limits。 在本练习中，你将创建一个拥有一个容器的 Pod。 容器将会请求 100 MiB 内存，并且内存会被限制在 200 MiB 以内。 这是 Pod 的配置文件：后续全部以deployment部署 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: memory-request-limit qcloud-app: memory-request-limit name: memory-request-limit namespace: mem-example spec: replicas: 1 selector: matchLabels: k8s-app: memory-request-limit qcloud-app: memory-request-limit template: metadata: labels: k8s-app: memory-request-limit qcloud-app: memory-request-limit spec: containers: - args: - --vm - \"1\" - --vm-bytes - 150M - --vm-hang - \"1\" command: - stress image: polinux/stress:latest imagePullPolicy: IfNotPresent name: memory-demo-ctr resources: limits: cpu: 500m memory: 200Mi requests: cpu: 250m memory: 100Mi 配置文件的 args 部分提供了容器启动时的参数。 \"--vm-bytes\", \"150M\" 参数告知容器尝试分配 150 MiB 内存。 开始创建 Pod： kubectl apply -f memory-request-limit.yaml 验证 Pod 中的容器是否已运行： [root@VM-0-17-tlinux ~/memonry]# kubectl get deployments,pods -n mem-example NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/memory-request-limit 1/1 1 1 4m52s NAME READY STATUS RESTARTS AGE pod/memory-request-limit-585f76b89-sx7v8 1/1 Running 0 4m52s 输出结果显示：该 Pod 中容器的内存请求为 100 MiB，内存限制为 200 MiB。 运行 kubectl top 命令，获取该 Pod 的指标数据： root@VM-0-17-tlinux ~/memonry]# kubectl top pod -n mem-example NAME CPU(cores) MEMORY(bytes) memory-request-limit-599b8774d8-dktwv 81m 151Mi 3，超过容器限制的内存 当节点拥有足够的可用内存时，容器可以使用其请求的内存。 但是，容器不允许使用超过其限制的内存。 如果容器分配的内存超过其限制，该容器会成为被终止的候选容器。 如果容器继续消耗超出其限制的内存，则终止容器。 如果终止的容器可以被重启，则 kubelet 会重新启动它，就像其他任何类型的运行时失败一样。 在本练习中，你将创建一个 Pod，尝试分配超出其限制的内存。 这是一个 Pod 的配置文件，其拥有一个容器，该容器的内存请求为 50 MiB，内存限制为 100 MiB： apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: memory-request-limit-2 qcloud-app: memory-request-limit-2 name: memory-request-limit-2 namespace: mem-example spec: replicas: 1 selector: matchLabels: k8s-app: memory-request-limit-2 qcloud-app: memory-request-limit-2 template: metadata: labels: k8s-app: memory-request-limit-2 qcloud-app: memory-request-limit-2 spec: containers: - args: - --vm - \"1\" - --vm-bytes - 250M - --vm-hang - \"1\" command: - stress image: polinux/stress:latest imagePullPolicy: IfNotPresent name: memory-demo-ctr resources: limits: memory: 100Mi requests: memory: 50Mi 在配置文件的 args 部分中，你可以看到容器会尝试分配 250 MiB 内存，这远高于 100 MiB 的限制。 此时，容器可能正在运行或被杀死。重复前面的命令，直到容器被杀掉： 查看yaml文件 [root@VM-0-17-tlinux ~/memonry]# kubectl get pods memory-request-limit-2-7f6548fd7b-n8smk -n mem-example -o yaml 裸POD模式： apiVersion: v1 kind: Pod metadata: name: memory-demo-2 namespace: mem-example spec: containers: - name: memory-demo-2-ctr image: polinux/stress resources: requests: memory: \"50Mi\" limits: memory: \"100Mi\" command: [\"stress\"] args: [\"--vm\", \"1\", \"--vm-bytes\", \"250M\", \"--vm-hang\", \"1\"] kubectl get pod -n mem-example -o yaml containerStatuses: - containerID: docker://e2a137f7d65e0e4f2ec7da7e036c2e1085b00f1145cccf92cc5169c9a1ee0c48 image: polinux/stress imageID: docker-pullable://polinux/stress@sha256:b6144f84f9c15dac80deb48d3a646b55c7043ab1d83ea0a697c09097aaad21aa lastState: terminated: containerID: docker://e2a137f7d65e0e4f2ec7da7e036c2e1085b00f1145cccf92cc5169c9a1ee0c48 exitCode: 1 finishedAt: \"2021-08-08T11:11:33Z\" reason: OOMKilled startedAt: \"2021-08-08T11:11:33Z\" 本练习中的容器可以被重启，所以 kubelet 会重启它。 多次运行下面的命令，可以看到容器在反复的被杀死和重启： 输出结果显示：容器被杀掉、重启、再杀掉、再重启……： 4，超过整个节点容量的内存 内存请求和限制是与容器关联的，但将 Pod 视为具有内存请求和限制，也是很有用的。 Pod 的内存请求是 Pod 中所有容器的内存请求之和。 同理，Pod 的内存限制是 Pod 中所有容器的内存限制之和。 Pod 的调度基于请求。只有当节点拥有足够满足 Pod 内存请求的内存时，才会将 Pod 调度至节点上运行。 在本练习中，你将创建一个 Pod，其内存请求超过了你集群中的任意一个节点所拥有的内存。 这是该 Pod 的配置文件，其拥有一个请求 1000 GiB 内存的容器，这应该超过了你集群中任何节点的容量。 Pod 处于 PENDING 状态。 这意味着，该 Pod 没有被调度至任何节点上运行，并且它会无限期的保持该状态 5，内存单位 内存资源的基本单位是字节（byte）。你可以使用这些后缀之一，将内存表示为 纯整数或定点整数：E、P、T、G、M、K、Ei、Pi、Ti、Gi、Mi、Ki。 例如，下面是一些近似相同的值： 128974848, 129e6, 129M , 123Mi 6，如果你没有指定内存限制 如果你没有为一个容器指定内存限制，则自动遵循以下情况之一： 容器可无限制地使用内存。容器可以使用其所在节点所有的可用内存， 进而可能导致该节点调用 OOM Killer。 此外，如果发生 OOM Kill，没有资源限制的容器将被杀掉的可行性更大。 运行的容器所在命名空间有默认的内存限制，那么该容器会被自动分配默认限制。 集群管理员可用使用 LimitRange 来指定默认的内存限制。 7，内存请求和限制的目的 通过为集群中运行的容器配置内存请求和限制，你可以有效利用集群节点上可用的内存资源。 通过将 Pod 的内存请求保持在较低水平，你可以更好地安排 Pod 调度。 通过让内存限制大于内存请求，你可以完成两件事： Pod 可以进行一些突发活动，从而更好的利用可用内存。 Pod 在突发活动期间，可使用的内存被限制为合理的数量。 清理 删除命名空间。下面的命令会删除你根据这个任务创建的所有 Pod： kubectl delete namespace mem-example 参考链接：https://kubernetes.io/zh/docs/tasks/configure-pod-container/assign-memory-resource/ "},"docs/tke/tke-sysctl.html":{"url":"docs/tke/tke-sysctl.html","title":"在TKE容器集群中使用sysctl","summary":"在TKE容器集群中使用sysctl","keywords":"","body":" 在日常使用K8S部署业务时候，需要修改容器部分内核参数来做优化 在 Kubernetes 集群中使用 sysctl 环境准备 节点操作系统镜像名称：tlinux2.4x86_64 节点内核版本：4.14+ TKE集群：1.18版本 如果镜像是tlinux 宿主机内核跟容器内核是相互的独立的， tlinux 做了特殊的处理 容器运行时 与linux 可以相互独立一部分内核参数，按理来说原生的linux 与docker 是共享内核 kubelet 配置的目的是为了开启修改容器内核的权限 ， 获取 Sysctl 的参数列表 在 Linux系统 中，用户可以通过 sysctl 接口修改内核运行时的参数。在 /proc/sys/ 虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如： 内核子系统（通常前缀为: kernel.） 网络子系统（通常前缀为: net.） 虚拟内存子系统（通常前缀为: vm.） MDADM 子系统（通常前缀为: dev.） 启用非安全的 Sysctl 参数 sysctl 参数分为 安全 和 非安全的。 安全 sysctl 参数除了需要设置恰当的命名空间外，在同一 node 上的不同 Pod 之间也必须是 相互隔离的。这意味着在 Pod 上设置 安全 sysctl 参数 必须不能影响到节点上的其他 Pod 必须不能损害节点的健康 必须不允许使用超出 Pod 的资源限制的 CPU 或内存资源。 至今为止，大多数 有命名空间的 sysctl 参数不一定被认为是 安全 的。 以下几种 sysctl 参数是 安全的： kernel.shm_rmid_forced net.ipv4.ip_local_port_range net.ipv4.tcp_syncookies net.ipv4.ping_group_range （从 Kubernetes 1.18 开始） [root@VM-1-17-tlinux /proc/sys]# sysctl -a | grep -E 'net.ipv4.ip_local_port_range|kernel.shm_rmid_forced|net.ipv4.tcp_syncookies|net.ipv4.pin g_group_range' kernel.shm_rmid_forced = 0 net.ipv4.ip_local_port_range = 32768 60999 net.ipv4.ping_group_range = 1 0 net.ipv4.tcp_syncookies = 1 在未来的 Kubernetes 版本中，若 kubelet 支持更好的隔离机制，则上述列表中将会 列出更多 安全的 sysctl 参数。 所有 安全的 sysctl 参数都默认启用。 所有 非安全的 sysctl 参数都默认禁用，且必须由集群管理员在每个节点上手动开启。 那些设置了不安全 sysctl 参数的 Pod 仍会被调度，但无法正常启动。 参考上述警告，集群管理员只有在一些非常特殊的情况下（如：高可用或实时应用调整）， 才可以启用特定的 非安全的 sysctl 参数。 如需启用 非安全的 sysctl 参数，需要每个节点上分别设置 kubelet 参数，例如： --allowed-unsafe-sysctls='kernel.msg*,kernel.shm*,net.*' 设置 Pod 的 Sysctl 参数 目前，在 Linux 内核中，有许多的 sysctl 参数都是 有命名空间的 。 这就意味着可以为节点上的每个 Pod 分别去设置它们的 sysctl 参数。 在 Kubernetes 中，只有那些有命名空间的 sysctl 参数可以通过 Pod 的 securityContext 对其进行配置。 以下列出有命名空间的 sysctl 参数，在未来的 Linux 内核版本中，此列表可能会发生变化。 kernel.shm*, kernel.msg*, kernel.sem, fs.mqueue.*, net.*（内核中可以在容器命名空间里被更改的网络配置项相关参数）。然而也有一些特例 （例如，net.netfilter.nf_conntrack_max 和 net.netfilter.nf_conntrack_expect_max 可以在容器命名空间里被更改，但它们是非命名空间的）。 没有命名空间的 sysctl 参数称为 节点级别的 sysctl 参数。 如果需要对其进行设置，则必须在每个节点的操作系统上手动地去配置它们， 或者通过在 DaemonSet 中运行特权模式容器来配置。 可使用 Pod 的 securityContext 来配置有命名空间的 sysctl 参数， securityContext 应用于同一个 Pod 中的所有容器。 此示例中，使用 Pod SecurityContext 来对一个安全的 sysctl 参数 kernel.shm_rmid_forced 以及两个非安全的 sysctl 参数 net.core.somaxconn 和 kernel.msgmax 进行设置。 在 Pod 规约中对 安全的 和 非安全的 sysctl 参数不做区分。 不修改前节点和POD里面参数配置如下： CVM节点上的参数配置 [root@VM-1-17-tlinux /proc/sys]# sysctl -a | grep -E 'kernel.shm_rmid_forced|net.core.somaxconn|kernel.msgmax' kernel.msgmax = 65536 kernel.shm_rmid_forced = 0 net.core.somaxconn = 32768 容器镜像是centos启动POD参数 [root@centos-sysctl-7445b79c9f-fchxn /]# sysctl -a | grep -E 'kernel.shm_rmid_forced|net.core.somaxconn|kernel.msgmax' kernel.msgmax = 65536 kernel.shm_rmid_forced = 0 net.core.somaxconn = 4096 使用initContainers容器修改内核参数 在不配置kubelet参数allowed-unsafe-sysctls 之前 1，先修改安全参数 kernel.shm_rmid_forced =1 #登陆容器里面查看参数 [root@centos-sysctl-5c6686499c-lhbdv /]# sysctl -a | grep -E 'kernel.shm_rmid_forced' kernel.shm_rmid_forced = 1 以上测试表示安全参数是不需要启动kubelet 的allowed-unsafe-sysctls配置就可以修改成功的 2，同时修改非安全参数net.core.somaxconn=1024， kernel.msgmax=60000 yaml实例如下： apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos-sysctl qcloud-app: centos-sysctl name: centos-sysctl namespace: default spec: selector: matchLabels: k8s-app: centos-sysctl qcloud-app: centos-sysctl template: metadata: labels: k8s-app: centos-sysctl qcloud-app: centos-sysctl spec: containers: - args: - -c - sleep 300000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: limits: cpu: 200m memory: 128Mi requests: cpu: 100m memory: 64Mi dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey initContainers: - args: - -c - sysctl -w kernel.shm_rmid_forced=1 && sysctl -w net.core.somaxconn=1024 && sysctl -w kernel.msgmax=60000 command: - /bin/sh image: busybox:latest imagePullPolicy: Always name: initc securityContext: privileged: true 配置截图如下 容器内也是生效的 节点上系统参数依据是 以上测试表示安全参数 和部分非安全参数也可以直接修改 3，修改其他内核非安全参数 示例修改如下参数： net.ipv4.tcp_synack_retries=1 net.ipv4.tcp_syn_retries=1 net.ipv4.tcp_tw_recycle=1 net.ipv4.tcp_tw_reuse=1 net.ipv4.tcp_fin_timeout=1 net.ipv4.tcp_keepalive_time=30 net.ipv4.ip_local_port_range=\"1024 65000\" CVM初始值： [root@VM-1-17-tlinux ~]# sysctl -a| grep -E 'net.ipv4.tcp_synack_retries|net.ipv4.tcp_syn_retries|net.ipv4.tcp_tw_recycle|net.ipv4.tcp_tw_reuse|net.ipv4.tcp_fin_timeout|net.ipv4.tcp_keepalive_time|net.ipv4.ip_local_port_range' net.ipv4.ip_local_port_range = 32768 60999 net.ipv4.tcp_fin_timeout = 60 net.ipv4.tcp_keepalive_time = 7200 net.ipv4.tcp_syn_retries = 6 net.ipv4.tcp_synack_retries = 5 net.ipv4.tcp_tw_reuse = 0 POD未修改前和操作系统保持默认的 [root@centos-sysctl-5c4d8dc5b-rc8gv /]# sysctl -a| grep -E 'net.ipv4.tcp_synack_retries|net.ipv4.tcp_syn_retries|net.ipv4.tcp_tw_recycle|net.ipv4.tcp_tw_reuse|net.ipv4.tcp_fin_timeout|net.ipv4.tcp_keepalive_time|net.ipv4.ip_local_port_range' net.ipv4.ip_local_port_range = 32768 60999 net.ipv4.tcp_fin_timeout = 60 net.ipv4.tcp_keepalive_time = 7200 net.ipv4.tcp_syn_retries = 6 net.ipv4.tcp_synack_retries = 5 net.ipv4.tcp_tw_reuse = 0 然后使用initContainer去修改这些参数，发现会报错，不让修改，修改失败 4，kubelet 开启非安全参数 (需要将节点移除重新) *在TKE集群节点默认情况下是没有开启这个参数的，就需要用户提交工单申请开通自定义kubernetes参数，在创建节点时候可以设置，文章结尾会附上设置相关截图 --allowed-unsafe-sysctls=kernel.shm,kernel.msg,net.* 然后再去使用initContainer修改，查看修改成功 [root@centos-sysctl-5d6d89c66d-z4mxr /]# sysctl -a| grep -E 'net.ipv4.tcp_synack_retries|net.ipv4.tcp_syn_retries|net.ipv4.tcp_tw_recycle|net.ipv4.tcp_tw_reuse|net.ipv4.tcp_fin_timeout|net.ipv4.tcp_keepalive_time|net.ipv4.ip_local_port_range' net.ipv4.ip_local_port_range = 1024 65000 net.ipv4.tcp_fin_timeout = 1 net.ipv4.tcp_keepalive_time = 30 net.ipv4.tcp_syn_retries = 1 net.ipv4.tcp_synack_retries = 1 net.ipv4.tcp_tw_reuse = 1 完整yaml apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos-sysctl qcloud-app: centos-sysctl name: centos-sysctl namespace: default spec: replicas: 1 revisionHistoryLimit: 5 selector: matchLabels: k8s-app: centos-sysctl qcloud-app: centos-sysctl template: metadata: creationTimestamp: null labels: k8s-app: centos-sysctl qcloud-app: centos-sysctl spec: containers: - args: - -c - sleep 300000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: limits: cpu: 200m memory: 128Mi requests: cpu: 100m memory: 64Mi securityContext: {} dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey initContainers: - args: - -c - sysctl -w net.ipv4.tcp_synack_retries=1 | sysctl -w net.ipv4.tcp_syn_retries=1 | sysctl -w net.ipv4.tcp_tw_recycle=1 | sysctl -w net.ipv4.tcp_tw_reuse=1 | sysctl -w net.ipv4.tcp_fin_timeout=1 | sysctl -w net.ipv4.tcp_keepalive_time=30 | sysctl -w net.ipv4.ip_local_port_range=\"1024 65000\" command: - /bin/sh image: busybox:latest imagePullPolicy: Always name: initc securityContext: privileged: true 在TKE里面设置kubelet参数 参考文档：https://cloud.tencent.com/document/product/457/47775 1，需要提交工单申请加白名单（提供APPID，主账号UIN，需要修改的参数名称，组件，集群版本） 2，集群现存节点，需要先封锁节点，驱逐节点POD，然后将节点移出移入（移出移入会重装操作系统，记得做好备份，另外就是移出时候不要勾选销毁按量计费的节点，包年包月的节点不影响），添加已有节点加入时候设置参数，新增节点直接设置 allowed-unsafe-sysctls='kernel.msg*,kernel.shm*,net.*' 更多参考K8S官方文档：https://kubernetes.io/zh/docs/tasks/administer-cluster/sysctl-cluster/ "},"docs/tke/tke-traefik-ingress.html":{"url":"docs/tke/tke-traefik-ingress.html","title":"在TKE里安装和使用Traefik-Ingress","summary":"在TKE里安装和使用Traefik Ingress","keywords":"","body":"在TKE里安装和使用Traefik Ingress Traefik 官方文档 Traefik 是一个为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具 Traefik的与众不同之处在于，除了它的许多特性之外，它还能自动发现服务的正确配置。当Traefik检查您的基础设施时，它会发现相关信息并发现哪个服务服务于哪个请求。 Traefik从根本上兼容所有主要的集群技术，如Kubernetes、Docker、Docker Swarm、AWS、Mesos、Marathon等，并且可以同时处理很多问题。 使用Traefik，不需要维护和同步单独的配置文件，所有事情都是自动实时发生的(没有重启，没有连接中断)。使用Traefik，用户可以将时间花在开发和部署系统新功能上，而不是配置和维护系统的工作状态上。 环境准备 TKE集群，版本1.18 安装Traefik方式一 YAML 本文介绍两种安装方式，yaml和helm chart方式，根据自己情况 只需要选一种即可 本文介绍如何在 TKE 环境中使用Traefik Ingress，目标是学习如何在Kubernetes中运行Traefik反向代理后的应用程序。它介绍并解释了开始使用Traefik所需的基本块，如入口控制器、入口、部署、静态和动态配置。 Traefik使用Kubernetes API来发现正在运行的服务，为了使用Kubernetes API, Traefik需要一些权限。此权限机制基于集群管理员定义的角色。然后将角色绑定到应用程序使用的帐户 创建角色 traefik-role.yaml示例： kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: traefik-role namespace: kube-system rules: - apiGroups: - \"\" resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io resources: - ingresses - ingressclasses verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io resources: - ingresses/status verbs: - update 创建服务账号： apiVersion: v1 kind: ServiceAccount metadata: name: traefik-account namespace: kube-system 创建ClusterRoleBinding kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: traefik-role-binding namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-role subjects: - kind: ServiceAccount name: traefik-account namespace: kube-system # Using \"default\" because we did not specify a namespace when creating the ClusterAccount. 部署 ingress controller kind: Deployment apiVersion: apps/v1 metadata: name: traefik-ingress-controller labels: app: traefik namespace: kube-system spec: replicas: 1 selector: matchLabels: app: traefik template: metadata: labels: app: traefik spec: serviceAccountName: traefik-account containers: - name: traefik image: traefik:v2.8 args: - --api.insecure - --providers.kubernetesingress.ingressclass=traefik - --log.level=DEBUG ports: - name: web containerPort: 80 - name: dashboard containerPort: 8080 一个workload可以运行多个Traefik代理Pods，需要一个片段将流量转发到任何实例： 即一个服务。创建名为traefik-ingress-controller-services的文件。yaml并插入两个Service资源: apiVersion: v1 kind: Service metadata: name: traefik-dashboard-service namespace: kube-system spec: type: LoadBalancer ports: - port: 8080 targetPort: dashboard selector: app: traefik --- apiVersion: v1 kind: Service metadata: name: traefik-ingress-controller-services namespace: kube-system spec: type: LoadBalancer ports: - targetPort: web port: 80 selector: app: traefik 执行以下命令将 Traefik 安装到 TKE 集群 [root@VM-249-130-tlinux ~/traefik]# kubectl apply -f . serviceaccount/traefik-account created service/traefik-dashboard-service created service/traefik-ingress-controller-services created deployment.apps/traefik-ingress-controller created clusterrolebinding.rbac.authorization.k8s.io/traefik-role-binding created clusterrole.rbac.authorization.k8s.io/traefik-role created 获取流量入口的 IP 地址（如下为 EXTERNAL-IP 字段），以及Dashboard,示例如下 [root@VM-249-130-tlinux ~/traefik]# kubectl -n kube-system get pods,svc | grep traefik pod/traefik-ingress-controller-5b6dd45cfb-pvddr 1/1 Running 0 3m29s service/traefik-dashboard-service LoadBalancer 172.16.255.160 118.24.226.193 8080:30299/TCP 3m29s service/traefik-ingress-controller-services LoadBalancer 172.16.253.56 114.117.220.253 80:31292/TCP 3m29s 安装Traefik方式二 Helm Traefik可以安装在Kubernetes使用Helm 仓库从https://github.com/traefik/traefik-helm-chart。 确保满足以下要求: 已创建TKE集群 Kubernetes 1.14 + 已安装helm客户端 版本3.x以上 ，helm安装参考 添加Traefik的helm chart 仓库 helm repo add traefik https://helm.traefik.io/traefik 准备配置文件values.yaml： providers: kubernetesIngress: publishedService: enabled: true #让 Ingress的外部 IP 地址状态显示为 Traefik 的 LB IP 地址 additionalArguments: - \"--providers.kubernetesingress.ingressclass=traefik\" # 指定 ingress class 名称 - \"--log.level=DEBUG\" ports: web: expose: true exposedPort: 80 # 对外的 HTTP 端口号，使用标准端口号在国内需备案 websecure: expose: true exposedPort: 443 # 对外的 HTTPS 端口号，使用标准端口号在国内需备案 traefik: expose: true exposedPort: 9000 deployment: enabled: true replicas: 1 完整的默认配置可执行 helm show values traefik/traefik 命令查看 用helm命令行安装： helm install traefik -f values.yaml traefik/traefik #默认是安装在default命名空间下的 #可以指定命名命名空间进行安装 kubectl create ns traefik-v2 # Install in the namespace \"traefik-v2\" helm install --namespace=traefik-v2 -f values.yaml traefik traefik/traefik #后续如果修改配置使用如下命令进行更新 helm upgrade --install traefik -f values.yaml traefik/traefik 查看部署的资源 [root@VM-249-130-tlinux ~]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE pod/traefik-7bb76c56fb-skzvq 1/1 Running 0 5m22s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 172.16.252.1 443/TCP 95m service/traefik LoadBalancer 172.16.252.238 118.24.225.19 80:32467/TCP,443:30353/TCP 5m22s 使用 Ingress 我们使用示例应用程序traefik/whoami，但其原则适用于任何其他应用程序。 whoami应用程序是一个简单的HTTP服务器，运行在端口80上，它向传入的请求应答与主机相关的信息。和往常一样，首先创建一个名为whoami的文件。yml并粘贴以下部署资源: kind: Deployment apiVersion: apps/v1 metadata: name: whoami labels: app: whoami spec: replicas: 1 selector: matchLabels: app: whoami template: metadata: labels: app: whoami spec: containers: - name: whoami image: traefik/whoami ports: - name: web containerPort: 80 创建service apiVersion: v1 kind: Service metadata: name: whoami spec: ports: - name: web port: 80 targetPort: web selector: app: whoami Traefik 支持使用 Kubernetes 的 Ingress 资源作为动态配置，可直接在集群中创建 Ingress 资源用于对外暴露集群，需要加上指定的 Ingress class（安装 Traefik 时定义）。示例如下 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: whoami-ingress annotations: kubernetes.io/ingress.class: traefik # 这里指定 ingress class 名称 spec: rules: - host: traefik.chen1900s.cn http: paths: - path: / backend: serviceName: whoami servicePort: 80 TKE 暂未将 Traefik 产品化，无法直接在 TKE 控制台进行可视化创建 Ingress，需要使用 YAML 进行创建。 使用安装方式一 部署后访问测试如下 [root@VM-249-130-tlinux ~/traefik]# nslookup traefik.chen1900s.cn #解析到traefik-ingress-controller的service VIP上 Server: 183.60.82.98 Address: 183.60.82.98#53 Non-authoritative answer: Name: traefik.chen1900s.cn Address: 114.117.220.253 [root@VM-249-130-tlinux ~/traefik]# curl http://traefik.chen1900s.cn/ Hostname: whoami-5db58df676-vwvzv IP: 127.0.0.1 IP: 172.16.0.5 RemoteAddr: 172.16.0.7:49846 GET / HTTP/1.1 Host: traefik.chen1900s.cn User-Agent: curl/7.29.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 172.16.0.1 X-Forwarded-Host: traefik.chen1900s.cn X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-ingress-controller-745b557778-jrjzn X-Real-Ip: 172.16.0.1 使用安装方式二 部署后访问测试如下： [root@VM-0-33-tlinux ~]# curl http://traefik.chen1900s.cn/ Hostname: whoami-5db58df676-856ss IP: 127.0.0.1 IP: 172.16.0.10 RemoteAddr: 172.16.0.9:58498 GET / HTTP/1.1 Host: traefik.chen1900s.cn User-Agent: curl/7.29.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 172.16.0.1 X-Forwarded-Host: traefik.chen1900s.cn X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-7bb76c56fb-skzvq X-Real-Ip: 172.16.0.1 [root@VM-0-33-tlinux ~]# nslookup traefik.chen1900s.cn #正常解析到 方式二 对应的service VIP上 Server: 183.60.83.19 Address: 183.60.83.19#53 Non-authoritative answer: Name: traefik.chen1900s.cn Address: 118.24.225.19 使用Traefik Dashboard 使用第一种方式安装时候默认安装了Traefik Dashboard，使用方式二Helm chart 方式安装，出于安全考虑，这个Helm Chart默认情况下不公开Traefik Dashboard。因此，如果需要暴露Dashboard端口，需要在values.yaml里面添加 traefik: expose: true exposedPort: 9000 还有更多参数可以通过helm show values traefik/traefik 命令查看， 比如暴露metrics端口等等 或者使用IngressRoute CRD apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: dashboard spec: entryPoints: - web routes: - match: Host(`traefikdashboard.chen1900s.cn`) && (PathPrefix(`/dashboard`) || PathPrefix(`/api`)) kind: Rule services: - name: api@internal kind: TraefikService 方式一安装默认是8080端口 方式二helm方式安装，端口设置的是9000 http://EXTERNAL-IP:9000/dashboard/#/ 使用 IngressRoute Traefik 不仅支持标准的 Kubernetes Ingress 资源，也支持 Traefik 特有的 CRD 资源，例如 IngressRoute，可以支持更多 Ingress 不具备的高级功能。IngressRoute 使用示例如下： apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: whoami-ingressroute spec: entryPoints: - web routes: - match: Host(`traefikroute.chen1900s.cn`) && PathPrefix(`/`) kind: Rule services: - name: whoami port: 80 创建ingressroute #kubectl apply -f whoami-ingressroute.yaml ingressroute.traefik.containo.us/whoami-ingressroute created 查看是否创建成功 # kubectl get ingressroute NAME AGE whoami-ingressroute 48s 访问验证 [root@VM-249-130-tlinux ~]# curl http://traefikroute.chen1900s.cn Hostname: whoami-5db58df676-856ss IP: 127.0.0.1 IP: 172.16.0.10 RemoteAddr: 172.16.0.9:57430 GET / HTTP/1.1 Host: traefikroute.chen1900s.cn User-Agent: curl/7.29.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 172.16.0.1 X-Forwarded-Host: traefikroute.chen1900s.cn X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-7bb76c56fb-skzvq X-Real-Ip: 172.16.0.1 "},"docs/tke/k8s-source-ip.html":{"url":"docs/tke/k8s-source-ip.html","title":"在k8s中获取客户端真实IP","keywords":"","body":"应用场景 当需明确服务请求来源以满足业务需求时，则需后端服务能够准确获取请求客户端的真实源 IP。例如以下场景： 具有对服务请求的来源进行审计的需求，例如异地登录告警。 具有针对安全攻击或安全事件溯源的需求，例如 APT 攻击及 DDoS 攻击等。 业务场景具有数据分析的需求，例如业务请求区域统计。 其他需获取客户端地址的需求。 实现方法 在 TKE 中默认的外部负载均衡器为 腾讯云负载均衡 作为服务流量的访问首入口，腾讯云负载均衡器会将请求流量负载转发到 Kubernetes 工作节点的 Kubernetes Service（默认）。此负载均衡过程会保留客户端真实源 IP（透传转发），但在 Kubernetes Service 转发场景下，无论使用 iptbales 或 ipvs 的负载均衡转发模式，转发时都会对数据包做 SNAT，即不会保留客户端真实源 IP。在 TKE 使用场景下，本文提供以下4种方式获取客户端真实源 IP，请参考本文按需选择适用方式。 实践验证 一 GR网络模式的集群 1，通过 Service 资源的配置选项保留客户端源 IP apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: nginx name: nginx namespace: service7 spec: replicas: 1 selector: matchLabels: k8s-app: nginx template: metadata: labels: k8s-app: nginx spec: containers: - image: nginx:latest imagePullPolicy: Always name: nginx 2，查看POD状态及调度节点 调度在 192.168.0.11 [root@VM-0-17-tlinux ~]# kubectl get pods -nservice -owide | grep nginx nginx-56745866db-hcc55 1/1 Running 0 35d 172.18.0.80 192.168.0.11 3，直接在集群内节点访问POD IP 1） 在集群另外一个节点17上访问，获取的还是192.168.0.17 节点IP，因为在集群网络内不做Snat ,POD看到的就是真实IP 2） 在POD所在节点上去访问如下172.18.0.65 ,有些人会问，这个IP是什么IP呢，其实是POD所在节点的网桥cbr0的IP 3） 在集群内其他节点上POD直接访问，获取到的是POD真实IP 4） 在相同节点上POD去访问 4，非local模式CLB类型的service apiVersion: v1 kind: Service metadata labels: k8s-app: nginx name: nginx namespace: service spec: clusterIP: 172.18.251.84 externalTrafficPolicy: Cluster ports: - name: 80-80-tcp nodePort: 32197 port: 80 protocol: TCP targetPort: 80 selector: k8s-app: nginx sessionAffinity: None type: LoadBalancer [root@VM-0-17-tlinux ~]# kubectl get svc -n service | grep nginx nginx LoadBalancer 172.18.251.84 114.117.221.190 80:32197/TCP 5m55s #集群外节点去访问： [root@k8s-node01 ~]# for i in {1..12};do curl -I 114.117.221.190 ;done 1）其他客户端访问公网CLB 查看发现，访问客户端IP全部Snat为节点IP 2）集群内节点通过公网去访问 5，service 使用externalTrafficPolicy：Local模式 apiVersion: v1 kind: Service metadata: annotations: service.cloud.tencent.com/direct-access: \"false\" service.cloud.tencent.com/local-svc-weighted-balance: \"true\" service.kubernetes.io/local-svc-only-bind-node-with-pod: \"true\" labels: k8s-app: nginx name: nginx namespace: service spec: clusterIP: 172.18.251.84 externalTrafficPolicy: Local healthCheckNodePort: 32699 ports: - name: 80-80-tcp nodePort: 32197 port: 80 protocol: TCP targetPort: 80 selector: k8s-app: nginx sessionAffinity: None type: LoadBalancer 1）CLB只绑定POD所在节点 2） 其他客户端访问公网CLB 查看发现，可以获取到客户端真实IP 3） 在集群内节点访问公网CLB 查看，还是节点的IP 6，后端service是local模式的clb 类型的ingress apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: qcloud kubernetes.io/ingress.rule-mix: \"false\" name: nginx namespace: service spec: rules: - host: chen.nginx.cn http: paths: - backend: serviceName: nginx servicePort: 80 path: / [root@VM-0-17-tlinux ~]# kubectl get ingress -n service NAME CLASS HOSTS ADDRESS PORTS AGE nginx chen.nginx.cn 139.186.101.117 80 2m35s 1）集群外节点访问 2） 集群内节点去访问 二 VPC-CNI网络模式 CLB 直通 Pod 转发模式获取 该方式优缺点分析如下： 优点：为 TKE 原生支持的功能特性，只需在控制台参考对应文档配置即可。 缺点：集群需开启 VPC-CNI 网络模式，详情请参见 VPC-CNI 模式说明。 使用 TKE 原生支持的 CLB 直通 Pod 的转发功能（CLB 透传转发，并绕过 Kubernetes Service 流量转发），后端 Pods 收到的请求的源 IP 即为客户端真实源 IP，此方式适用于四层及七层服务的转发场景。转发原理如下图 1，部署POD资源对象 apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx-deployment-eni spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: annotations: tke.cloud.tencent.com/networks: tke-route-eni labels: app: nginx spec: containers: - image: nginx name: nginx resources: requests: tke.cloud.tencent.com/eni-ip: \"1\" limits: tke.cloud.tencent.com/eni-ip: \"1\" [root@VM-0-17-tlinux ~]# kubectl get pod -n service -o wide | grep eni nginx-deployment-eni-bc566bd66-2gfdz 1/1 Running 0 68s 192.168.253.9 192.168.0.17 1/1 2，直接在集群内访问POD IP 3，service直连POD模式 apiVersion: v1 kind: Service metadata: annotations: service.cloud.tencent.com/direct-access: \"true\" labels: app: nginx name: nginx-eni namespace: service spec: clusterIP: 172.18.253.247 externalTrafficPolicy: Cluster ports: - name: 80-80-tcp nodePort: 30240 port: 80 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None type: LoadBalancer 1）集群外节点通过公网VIP访问 能够正常获取到 2）在集群内节点访问 看到是节点内外IP 4，service非直连POD模式 apiVersion: v1 kind: Service metadata: annotations: service.cloud.tencent.com/direct-access: \"false\" labels: app: nginx name: nginx-eni namespace: service spec: clusterIP: 172.18.253.247 externalTrafficPolicy: Cluster ports: - name: 80-80-tcp nodePort: 30240 port: 80 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None type: LoadBalancer 及时后端POD使用的是eni模式，但是service如果不选择直连POD模式，还是通过NodePort模式转发的 5，ingress直连POD模式 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: ingress.cloud.tencent.com/direct-access: \"true\" kubernetes.io/ingress.class: qcloud name: nginx-eni namespace: service spec: rules: - host: chen.nginx-eni.cn http: paths: - backend: serviceName: nginx-eni servicePort: 80 path: / 1）集群外节点去访问 2）集群内节点访问 走的是公网IP 6，ingress非直连POD模式 三 VPC-CNI网络模式nginx-ingress获取客户端源IP root@VM-0-17-tlinux ~]# kubectl get pod -n kube-system -owide |grep nginx-ingress-eni nginx-ingress-eni-ingress-nginx-controller-744d9ff489-649wp 1/1 Running 0 18m 192.168.253.10 192.168.2.36 1/1 1，后端POD使用的是GR模式，nginx-ingress-controller使用的是VPC-CNI直连模式（正常获取） apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-ingress-eni kubernetes.io/ingress.rule-mix: \"false\" nginx.ingress.kubernetes.io/use-regex: \"true\" name: nginx namespace: service spec: rules: - host: chen.nginx.cn http: paths: - backend: serviceName: nginx servicePort: 80 path: / 测试命令： for i in {1..4};do curl -H \"Host: chen.nginx.cn\" -s http://114.117.221.188/ ;done 1）VPC外其他节点访问 2）集群内节点访问 2，后端POD使用的是VPC-CNI模式，nginx-ingress-controller使用的是VPC-CNI直连模式（正常获取） apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-ingress-eni kubernetes.io/ingress.rule-mix: \"false\" nginx.ingress.kubernetes.io/use-regex: \"true\" name: nginx-eni namespace: service spec: rules: - host: chen.nginx-eni.cn http: paths: - backend: serviceName: nginx-eni servicePort: 80 path: / 测试命令： for i in {1..4};do curl -H \"Host: chen.nginx-eni.cn\" -s http://114.117.221.188/ ;done 1）VPC外其他节点访问 2）集群内节点访问 3，后端POD使用的是GR模式，nginx-ingress-controller使用的是local模式（正常获取） apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: nginx-local namespace: service spec: rules: - host: chen.nginx-local.cn http: paths: - backend: serviceName: nginx servicePort: 80 path: / 测试命令： for i in {1..4};do curl -H \"Host: chen.nginx-local.cn\" -s http://118.24.224.103/ ;done 1）VPC外节点访问 4，后端POD使用的是GR模式，nginx-ingress-controller使用默认模式（无法获取） apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-ingress name: nginx-cluster namespace: service spec: rules: - host: chen.nginx-cluster.cn http: paths: - backend: serviceName: nginx servicePort: 80 path: / 测试命令 for i in {1..4};do curl -H \"Host: chen.nginx-cluster.cn\" -s http://114.117.219.97/ ;done 四 k8s nginx-ingress获取真实IP地址配置 https://developer.aliyun.com/article/699074 https://www.imooc.com/article/303503 GR模式+非local模式 集群是GR模式，创建的nginx-ingress实例默认使用GR网络模式，并且service使用的是非local模式 nginx-ingress-controller 配置： apiVersion: v1 kind: Service metadata: labels: k8s-app: realip-nginx-ingress-nginx-controller qcloud-app: realip-nginx-ingress-nginx-controller name: realip-nginx-ingress-nginx-controller namespace: kube-system spec: externalTrafficPolicy: Cluster ports: - name: http nodePort: 31827 port: 80 protocol: TCP targetPort: http - name: https nodePort: 31404 port: 443 protocol: TCP targetPort: https selector: k8s-app: realip-nginx-ingress-nginx-controller qcloud-app: realip-nginx-ingress-nginx-controller sessionAffinity: None type: LoadBalancer 部署测试案例 apiVersion: apps/v1 kind: Deployment metadata: name: whoami namespace: cjweichen labels: app: whoami spec: replicas: 1 selector: matchLabels: app: whoami template: metadata: labels: app: whoami spec: containers: - image: ccr.ccs.tencentyun.com/chenjingwei/whoami:latest imagePullPolicy: Always name: whoami ports: - containerPort: 80 name: 80tcp02 protocol: TCP dnsPolicy: ClusterFirst restartPolicy: Always --- apiVersion: v1 kind: Service metadata: labels: app: whoami name: whoami namespace: cjweichen spec: ports: - name: 80-80-tcp port: 80 protocol: TCP targetPort: 80 selector: app: whoami sessionAffinity: None type: ClusterIP --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx kubernetes.io/ingress.rule-mix: \"false\" nginx.ingress.kubernetes.io/use-regex: \"true\" name: whoami namespace: cjweichen spec: rules: - host: whoami.chen1900s.cn http: paths: - backend: serviceName: whoami servicePort: 80 path: / pathType: ImplementationSpecific 访问服务 其中X-Forwarded-For 会SNAT 成集群节点IP [root@172-16-155-8 ~]# curl http://whoami.chen1900s.cn/ Hostname: whoami-5696f977bb-5xks6 IP: 127.0.0.1 IP: 10.55.1.14 RemoteAddr: 10.55.1.17:53128 GET / HTTP/1.1 Host: whoami.chen1900s.cn User-Agent: curl/7.29.0 Accept: */* X-Forwarded-For: 172.16.55.9 X-Forwarded-Host: whoami.chen1900s.cn X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Real-Ip: 172.16.55.9 X-Request-Id: c3e95454d539564bb547352431464788 X-Scheme: http 其中10.55.1.1 为pod所在CVM节点的cbr0 IP GR模式+local模式 nginx-ingress-controller service 配置 apiVersion: v1 kind: Service metadata: annotations: service.cloud.tencent.com/direct-access: \"false\" service.cloud.tencent.com/local-svc-weighted-balance: \"false\" service.kubernetes.io/local-svc-only-bind-node-with-pod: \"true\" labels: k8s-app: realip-nginx-ingress-nginx-controller qcloud-app: realip-nginx-ingress-nginx-controller name: realip-nginx-ingress-nginx-controller namespace: kube-system spec: externalTrafficPolicy: Local healthCheckNodePort: 30186 ports: - name: http nodePort: 31827 port: 80 protocol: TCP targetPort: 80 - name: https nodePort: 31404 port: 443 protocol: TCP targetPort: 443 selector: k8s-app: realip-nginx-ingress-nginx-controller qcloud-app: realip-nginx-ingress-nginx-controller sessionAffinity: None type: LoadBalancer 通过客户端curl 可以看到，能够正常获取到客户端源IP 直连POD模式+非local模式 nginx-ingress 配置（注意点：需要后端POD也是使用eni模式 ） apiVersion: v1 kind: Service metadata: annotations: service.cloud.tencent.com/direct-access: \"true\" labels: k8s-app: realip-nginx-ingress-nginx-controller qcloud-app: realip-nginx-ingress-nginx-controller name: realip-nginx-ingress-nginx-controller namespace: kube-system spec: externalTrafficPolicy: Cluster ports: - name: http nodePort: 31827 port: 80 protocol: TCP targetPort: 80 - name: https nodePort: 31404 port: 443 protocol: TCP targetPort: 443 selector: k8s-app: realip-nginx-ingress-nginx-controller qcloud-app: realip-nginx-ingress-nginx-controller sessionAffinity: None type: LoadBalancer 修改configmap nginx-configuration配置 compute-full-forwarded-for: \"true\" forwarded-for-header: \"X-Forwarded-For\" use-forwarded-headers: \"true\" "},"docs/tke/k8s-kms.html":{"url":"docs/tke/k8s-kms.html","title":"使用KMS进行Kubernetes数据源加密","keywords":"","body":"集群版本：1.18 操作场景 腾讯云 TKE-KMS 插件 集成密钥管理系统（Key Management Service，KMS）丰富的密钥管理功能，针对 Kubernetes 集群中 Secret 提供强大的加密/解密能力。本文介绍如何通过 KMS 对 Kubernetes 集群进行数据加密。 基本概念 密钥管理系统 KMS 密钥管理系统（Key Management Service，KMS）是一款安全管理类服务，使用经过第三方认证的硬件安全模块 HSM（Hardware Security Module） 来生成和保护密钥。帮助用户轻松创建和管理密钥，满足用户多应用多业务的密钥管理需求，符合监管和合规要求。 前提条件 已创建符合以下条件的容器服务独立集群： Kubernetes 版本为1.10.0及以上。 Etcd 版本为3.0及以上。 说明： 如需检查版本，可前往 “集群管理” 页面，选择集群 ID 并进入集群“基本信息”页面进行查看。 操作步骤 创建 KMS 密钥并获取 ID 登录 密钥管理系统（合规） 控制台，进入“用户密钥”页面。 在“用户密钥”页面上方，选择需要创建密钥的区域并单击新建。 在弹出的“新建密钥”窗口，参考以下信息进行配置。如下图所示： 主要参数信息如下，其余参数请保持默认设置： 密钥名称：必填且在区域内唯一，密钥名称只能为字母、数字及字符_和-，且不能以 KMS- 开头。本文以 tke-kms 为例。 描述信息：选填，可用来说明计划保护的数据类型或计划与 CMK 配合使用的应用程序。 密钥用途：选择“对称加解密”。 密钥材料来源：提供 “KMS” 和“外部”两种选择，请根据实际需求进行选择。本文以选择 “KMS” 为例。 单击确定后返回“用户密钥”页面，即可查看已成功创建的密钥。 单击密钥 ID，进入密钥信息页，记录该密钥完整 ID。如下图所示： 创建并获取访问密钥 说明： 如已创建访问密钥，则请跳过此步骤。 登录 访问管理控制台，选择左侧导航栏中的访问密钥 > API密钥管理，进入 “API密钥管理”页面。 在 “API密钥管理”页面中，单击新建密钥并等待创建完成。 创建完成即可在 “API密钥管理”页面查看该密钥信息，包含 SecretId、SecretKey。如下图所示： 创建 DaemonSet 并部署 tke-kms-plugin 登录 腾讯云容器服务控制台，选择左侧导航栏中集群。 在“集群管理”页面中，选择符合条件的集群 ID，进入该集群详情页。 选择该集群任意界面右上角YAML创建资源，进入 YAML 创建资源页，输入tke-kms-plugin.yaml 内容。如下所示： 说明： 请根据实际情况替换以下参数： ``：KMS 密钥所在地域，有效值可参见 地域列表。 ``：输入 创建 KMS 密钥并获取 ID 步骤中所获取的 KMS 密钥 ID。 和：输入 创建并获取访问密钥 步骤中创建的 SecretID 和 SecretKey。 images: ccr.ccs.tencentyun.com/tke-plugin/tke-kms-plugin:1.0.0：tke-kms-plugin 镜像地址。当您需要使用自己制作的 tke-kms-plugin 镜像时，可自行进行更换。 下载相关yaml文件：https://github.com/Tencent/tke-kms-plugin ##设置变量 REGION=ap-shanghai KEY_ID=xxxxx SECRET_ID=xxxx SECRET_KEY=xxxx sed \"s//$REGION/g; s//$KEY_ID/g; s//$SECRET_ID/g; s//$SECRET_KEY/g\" deployment/tke-kms-plugin.yaml > ds-tke-kms-plugin.yaml 生成的yaml配置如下： apiVersion: apps/v1 kind: DaemonSet metadata: name: tke-kms-plugin namespace: kube-system spec: selector: matchLabels: name: tke-kms-plugin template: metadata: labels: name: tke-kms-plugin spec: nodeSelector: node-role.kubernetes.io/master: \"true\" hostNetwork: true restartPolicy: Always volumes: - name: tke-kms-plugin-dir hostPath: path: /var/run/tke-kms-plugin type: DirectoryOrCreate tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: tke-kms-plugin image: ccr.ccs.tencentyun.com/tke-plugin/tke-kms-plugin:1.0.0 command: - /tke-kms-plugin - --region=ap-shanghai - --key-id=xxxxxxx - --unix-socket=/var/run/tke-kms-plugin/server.sock - --v=2 livenessProbe: exec: command: - /tke-kms-plugin - health-check - --unix-socket=/var/run/tke-kms-plugin/server.sock initialDelaySeconds: 5 failureThreshold: 3 timeoutSeconds: 5 periodSeconds: 30 env: - name: SECRET_ID value: xxx - name: SECRET_KEY value: xxx volumeMounts: - name: tke-kms-plugin-dir mountPath: /var/run/tke-kms-plugin readOnly: false 单击完成并等待 DaemonSet 创建成功即可。 [root@10-1-3-7 ~/tke-kms-plugin-master]# kubectl get po -n kube-system -l name=tke-kms-plugin NAME READY STATUS RESTARTS AGE tke-kms-plugin-2whbc 1/1 Running 0 2m44s tke-kms-plugin-64c9n 1/1 Running 0 2m44s tke-kms-plugin-ln8mb 1/1 Running 0 2m44s 配置 kube-apiserver 参考使用标准方式登录 Linux 实例（推荐），分别登录该集群每一个 Master 节点。 说明：Master 节点安全组默认关闭22端口，执行登录节点操作前请首先前往其安全组界面打开22端口。详情请参见 添加安全组规则。 执行以下命令，新建并打开该 YAML 文件。 vim /etc/kubernetes/encryption-provider-config.yaml 按i切换至编辑模式，对上述 YAML 文件进行编辑。对应实际使用的 K8S 版本，输入如下内容： K8S v1.13+： [root@10-1-3-7 ~]# cat /etc/kubernetes/encryption-provider-config.yaml apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: tke-kms-plugin timeout: 3s cachesize: 1000 endpoint: unix:///var/run/tke-kms-plugin/server.sock - identity: {} 编辑完成后，按 Esc，输入 :wq，保存文件并返回。 执行以下命令，对该 YAML 文件进行编辑（操作前建议备份下api yaml） mkdir /tmp/k8s-master cp /etc/kubernetes/manifests/* /tmp/k8s-master/ ls -l /tmp/k8s-master/ vi /etc/kubernetes/manifests/kube-apiserver.yaml 按i切换至编辑模式，对应实际使用的 K8S 版本，将以下内容添加至args。 说明：K8S v1.10.5 版本的独立集群，需要先将 kube-apiserver.yaml 移/etc/kubernetes/manifests 目录，编辑完成之后再移入。 K8S v1.13+： - --encryption-provider-config=/etc/kubernetes/encryption-provider-config.yaml 为/var/run/tke-kms-plugin/server.sock添加 Volume 指令，其中添加位置及内容如下所示： 说明：/var/run/tke-kms-plugin/server.sock` 是 tke kms server 启动时监听的一个 unix socket，kube apiserver 会通过访问该 socket 来访问 tke kms server。 为volumeMounts:添加以下内容： - mountPath: /var/run/tke-kms-plugin name: tke-kms-plugin-dir 为volume:添加以下内容： - hostPath: path: /var/run/tke-kms-plugin name: tke-kms-plugin-dir 编辑完成后，按 Esc，输入 :wq，保存 /etc/kubernetes/manifests/kube-apiserver.yaml 文件，等待 kube-apiserver 重启完成。 验证数据已经加密 登录该集群 Node 节点，执行以下命令新建 Secret。 kubectl create secret generic kms-secret -n default --from-literal=mykey=mydata 执行以下命令，验证 Secret 是否已正确解密。 kubectl get secret kms-secret -o=jsonpath='{.data.mykey}' | base64 -d 输出若为 mydata，即与创建 Secret 的值相同，则表示 Secret 已正确解密。如下图所示： 参考资料 "},"docs/tke/tke-kubernetes-dashboard.html":{"url":"docs/tke/tke-kubernetes-dashboard.html","title":"部署kubernetes-dashboard服务","summary":"在TKE集群部署kubernetes-dashboard","keywords":"","body":"在TKE集群部署kubernetes-dashboard服务 具体参考 基于已经创建好的Kubernetes集群进行部署Kubernetes-dashboard 下载部署yaml文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml 下载镜像 由于镜像是国外镜像，国内集群部署有可能会拉取镜像失败，可以先手动拉取下来，然后修改对应的yaml文件 docker pull kubernetesui/dashboard:v2.0.4 docker pull kubernetesui/metrics-scraper:v1.0.4 docker tag 46d0a29c3f61 ccr.ccs.tencentyun.com/chenjingwei/dashboard:v2.0.4 #重新打tag, 对应到自己账号下个人镜像参考，我这里上传到我自己的镜像仓库，如果有需要 也可以从这个地址下载 docker tag 86262685d9ab ccr.ccs.tencentyun.com/chenjingwei/metrics-scraper:v1.0.4 docker push ccr.ccs.tencentyun.com/chenjingwei/dashboard:v2.0.4 docker push ccr.ccs.tencentyun.com/chenjingwei/metrics-scraper:v1.0.4 访问方式配置（可选） 对应的service 默认是clusterIP方式，如果需要CLB去访问 或者nodePort方式去访问，可以修改对应的yaml文件，也可以部署完后修改 kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard type: NodePort #添加这个 service类型 如果需要是CLB类型 则修改成 LoadBalancer 部署kubernetes-dashboard kubectl apply -f recommended.yaml 查看是否部署成功 [root@VM-249-41-tlinux ~]# kubectl -n kubernetes-dashboard get pod,svc NAME READY STATUS RESTARTS AGE pod/dashboard-metrics-scraper-7748f84fc-8ldkn 1/1 Running 0 92m pod/kubernetes-dashboard-6fbb5497cf-qscnk 2/2 Running 1 29m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dashboard-metrics-scraper ClusterIP 172.16.253.139 8000/TCP 92m service/kubernetes-dashboard NodePort 172.16.252.218 443:31778/TCP 92m 访问kubernetes-dashboard 由于上面service使用的是NodePort类型，可以通过nodeIP+NodePort端口去访问，kubernetes-dashboard后端服务是https协议的，则需要通过https://节点IP:NodePort 验证方式选择token TKE集群获取Token认证方式 # 创建serviceaccount kubectl create serviceaccount dashboard-serviceaccount -n kubernetes-dashboard # 创建clusterrolebinding kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-serviceaccount 获取token [root@VM-249-41-tlinux ~]# kubectl get secret -n kubernetes-dashboard | grep dashboard-serviceaccount-token dashboard-serviceaccount-token-pddv4 kubernetes.io/service-account-token 3 98m [root@VM-249-41-tlinux ~]# kubectl describe secret dashboard-serviceaccount-token-pddv4 -n kubernetes-dashboard 复制token 到控制台 基于Istio的访问kubernetes-dashboard配置 参考istio官方文档 前提条件： 1，服务网格已经关联集群 2，已经创建边缘代理网关istio-ingressgateway 首先需要开启 Sidecar 自动注入配置，命名空间选择kubernetes-dashboard，然后销毁重建kubernetes-dashboard的POD #这边是基于TKE的容器服务网格1.12.5版本的，其他版本需要修改成对应版本 kubectl label namespace kubernetes-dashboard istio.io/rev=1-12-5 #如果是使用的是自建sidecar 则需要使用下面这个命令开启自动注入 #kubectl label namespace xxx istio-injection=enalbed 为某个 namespace 开启 sidecar 自动注入 新建Gateway 这里使用自定义的域名kubernetes-dashboard.chen1900s.com 对应yaml: apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: kubernetes-dashboard-gateway namespace: kubernetes-dashboard spec: servers: - port: number: 443 name: HTTPS-1-m00g protocol: HTTPS hosts: - kubernetes-dashboard.chen1900s.com tls: mode: PASSTHROUGH selector: app: istio-ingressgateway istio: ingressgateway 创建Virtual Service yaml文件 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: kubernetes-dashboard-vs namespace: kubernetes-dashboard spec: hosts: - kubernetes-dashboard.chen1900s.com gateways: - kubernetes-dashboard/kubernetes-dashboard-gateway tls: - match: - sniHosts: - kubernetes-dashboard.chen1900s.com route: - destination: host: kubernetes-dashboard.kubernetes-dashboard.svc.cluster.local port: number: 443 weight: 100 设置完成后，配置如下本地host映射，即可通过域名 https://kubernetes-dashboard.chen1900s.com 访问，效果与之前的NodePort一致 [root@VM-249-41-tlinux ~]# kubectl -n istio-system get svc istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 172.16.255.54 114.117.219.33 443:32430/TCP 3h47m #114.117.219.33 kubernetes-dashboard.chen1900s.com 输入上面查询到的token 可以正常登陆 Istio中基于Secure Ingress的访问方式还有多种 基于nginx-ingress方式访问kubernetes-dashboard Nginx Ingress Controller默认使用HTTP协议转发请求到后端业务容器。当后端业务容器为HTTPS协议时，可以通过使用注解nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"来使得Nginx Ingress Controller使用HTTP协议转发请求到后端业务容器 kubernetes-dashboard服务正是HTTPS协议服务，则需要使用这个annotations 环境准备： 1，已经创建nginx-ingress-controller 实例 2，已经创建TLS证书 首先是如果不添加这个annotations 访问会报 Client sent an HTTP request to an HTTPS server. 添加nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" 后访问正常 yaml文件： apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx kubernetes.io/ingress.rule-mix: \"true\" nginx.ingress.kubernetes.io/backend-protocol: HTTPS #重点这个annotations nginx.ingress.kubernetes.io/use-regex: \"true\" name: kubernetes-dashboard namespace: kubernetes-dashboard spec: rules: - host: kubernetes-dashboard.chen1900s.cn #域名替换成自己的域名 http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 443 path: / pathType: ImplementationSpecific tls: - hosts: - kubernetes-dashboard.chen1900s.cn secretName: chen1900s #替换成自己的证书secret 基于CLB类型ingress方式访问kubernetes-dashboard配置 CLB类型ingress，对应后端服务协议是默认HTTP的，后端协议是指 CLB 与后端服务之间协议，后端协议选择 HTTP 时，后端服务需部署 HTTP 服务。后端协议选中 HTTPS 时，后端服务需部署 HTTPS 服务，HTTPS 服务的加解密会让后端服务消耗更多资源 如果需要后端协议为HTTPS 则需要使用TkeServiceConfig来配置ingress自动创建的CLB 如果不通过TkeServiceConfig配置后端是HTTPS服务时候，访问会异常，如下： 使用 TkeServiceConfig 配置 CLB 创建 Ingress 时，设置 ingress.cloud.tencent.com/tke-service-config-auto: ，将自动创建 -auto-ingress-config apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: ingress.cloud.tencent.com/tke-service-config-auto: \"true\" #自动创建TkeServiceConfig kubernetes.io/ingress.class: qcloud kubernetes.io/ingress.rule-mix: \"true\" name: kubernetes-dashboard namespace: kubernetes-dashboard spec: rules: - host: kubernetes-dashboard.chen1900s.cn http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 443 path: / pathType: ImplementationSpecific tls: - hosts: - kubernetes-dashboard.chen1900s.cn secretName: chen1900s-ye4ubdzo 查看TkeServiceConfig配置 [root@VM-249-41-tlinux ~]# kubectl -n kubernetes-dashboard get TkeServiceConfig kubernetes-dashboard-auto-ingress-config NAME AGE kubernetes-dashboard-auto-ingress-config 66s 编辑这个配置文件，修改后端协议 spec.loadBalancer.l7listeners.protocol.domain.rules.url.forwardType: 指定后端协议 kubectl -n kubernetes-dashboard edit TkeServiceConfig kubernetes-dashboard-auto-ingress-config 然后使用这个CLB对应的域名进行访问 附： apiVersion: cloud.tencent.com/v1alpha1 kind: TkeServiceConfig metadata: name: kubernetes-dashboard-auto-ingress-config namespace: kubernetes-dashboard spec: loadBalancer: l7Listeners: - defaultServer: kubernetes-dashboard.chen1900s.cn domains: - domain: kubernetes-dashboard.chen1900s.cn rules: - forwardType: HTTPS #主要修改这个 指定后端协议 healthCheck: enable: true healthNum: 3 httpCheckDomain: kubernetes-dashboard.chen1900s.cn httpCheckMethod: HEAD httpCheckPath: / httpCode: 31 intervalTime: 5 sourceIpType: 0 timeout: 5 unHealthNum: 3 scheduler: WRR session: enable: false url: / keepaliveEnable: 0 port: 443 protocol: HTTPS - keepaliveEnable: 0 port: 80 protocol: HTTP "},"docs/tke/tke-nginx-ingress.html":{"url":"docs/tke/tke-nginx-ingress.html","title":"TKE集群中Nginx-ingress常用案例","summary":"NGINX Ingress Controller常用的案例介绍和使用","keywords":"","body":"一、Ingress简介 nginx-ingress官方文档介绍 在Kubernetes中，服务和Pod的IP地址仅可以在集群网络内部使用，对于集群外的应用是不可见的。为了使外部的应用能够访问集群内的服务，在Kubernetes 目前 提供了以下几种方案： NodePort类型 LoadBalancer类型 Ingress 1，Ingress组成 ingress controller：实际 Nginx 负载，会 watch kubernetes ingress 对象的变化更新在集群中，将新加入的Ingress转化成Nginx的配置文件并使之生效 ingress服务：将Nginx的配置抽象成一个Ingress对象，每添加一个新的服务只需写一个新的Ingress的yaml文件即可 2，Ingress工作原理 ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化， 然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置， 再写到nginx-ingress-control的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中， 然后reload一下使配置生效，以此达到域名分配置和动态更新的问题。 3，Ingress 可以解决什么问题 动态配置服务 如果按照传统方式， 当新增加一个服务时，我们可能需要在流量入口加一个反向代理指向我们新的k8s服务. 而如果用了Ingress, 只需要配置好这个服务, 当服务启动时，自动注册到Ingress的中, 不需要额外的操作. 减少不必要的端口暴露 配置过k8s的都清楚，第一步是要关闭防火墙的, 主要原因是k8s的很多服务会以NodePort方式映射出去，这样就相当于给宿主机打了很端口， 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去，其他服务都不要用NodePort方式 二、部署安装 见组件管理 三、相关案例 环境准备： 创建TKE集群 安装nginx-ingress组件 域名和证书 部署nginx服务用于测试和验证 [root@VM-249-130-tlinux ~]# kubectl -n nginx-ingress get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx-v1 ClusterIP 172.16.252.224 80/TCP 13m app=nginx,version=v1 nginx-v2 ClusterIP 172.16.252.208 80/TCP 13m app=nginx,version=v2 nginx-v3 ClusterIP 172.16.254.220 80/TCP 7m40s app=nginx,version=v3 nginx-v4 ClusterIP 172.16.255.165 80/TCP 6m13s app=nginx,version=v4 [root@VM-249-130-tlinux ~]# curl http://172.16.252.224:80 nginx-v1 [root@VM-249-130-tlinux ~]# curl http://172.16.252.208:80 nginx-v2 [root@VM-249-130-tlinux ~]# curl http://172.16.254.220:80 nginx-v3 [root@VM-249-130-tlinux ~]# curl http://172.16.255.165:80 nginx-v4 表示访问对应的service 成功的 案例1 最简单基础配置 yaml示例如下： apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/use-regex: \"true\" name: nginx namespace: nginx-ingress spec: rules: - host: nginx.chen1900s.cn #相当于定义了nginx的一个server_name http: paths: - backend: serviceName: nginx-v1 #定义后端的service servicePort: 80 path: / #一个path就相当于一个location，path的值必须为“/”。这里为匹配的规则，根表示默认请求转发规则 pathType: ImplementationSpecific - host: nginx.chen1900s.cn http: paths: - backend: serviceName: nginx-v2 servicePort: 80 path: /.*.(txt|css|doc) #可以进入到ingress controller查看nginx的配置,这里相当于把结尾为txt,css,doc的url请求转发到nginx-v2 service pathType: ImplementationSpecific - host: nginx.chen1900s.cn http: paths: - backend: serviceName: nginx-v3 servicePort: 80 path: /(api|app)/ #这里相当于将api和app开头的目录语法转发至nginx-v3 service pathType: ImplementationSpecific - host: nginx.chen1900s.cn http: paths: - backend: serviceName: nginx-v4 servicePort: 80 path: /api #这里相当于将api开头的url（可以是一个文件，也可以是一个目录）的请求，转发到nginx-4 pathType: ImplementationSpecific #如果上面的都没匹配到，则默认转到“/” 也就是nginx-v1 执行以下命令，访问Nginx服务。 # curl nginx.chen1900s.cn #默认转发规则 nginx-v1 # curl nginx.chen1900s.cn/nginx.txt #结尾为txt,css,doc的url请求转发到nginx-v2 service nginx-v2 # curl nginx.chen1900s.cn/nginx.css #结尾为txt,css,doc的url请求转发到nginx-v2 service nginx-v2 # curl nginx.chen1900s.cn/nginx.doc #结尾为txt,css,doc的url请求转发到nginx-v2 service nginx-v2 # curl nginx.chen1900s.cn/api/ #将api和app开头的目录语法转发至nginx-v3 service nginx-v3 # curl nginx.chen1900s.cn/api/hello #将api和app开头的目录语法转发至nginx-v3 service nginx-v3 # curl nginx.chen1900s.cn/app/ #将api和app开头的目录语法转发至nginx-v3 service nginx-v3 # curl nginx.chen1900s.cn/api 将api开头的url（可以是一个文件，也可以是一个目录）的请求，转发到nginx-4 nginx-v4 # curl nginx.chen1900s.cn/api111 将api开头的url（可以是一个文件，也可以是一个目录）的请求，转发到nginx-4 nginx-v4 # curl nginx.chen1900s.cn/app #默认转发到/ nginx-v1上面 nginx-v1 annotations配置作用于server 指定了我们使用后端ingress controller的类别，如果后端有多个ingress controller的时候很重要 kubernetes.io/ingress.class: \"nginx\" 指定我们的rules的path可以使用正则表达式，如果我们没有使用正则表达式，此项则可不使用 nginx.ingress.kubernetes.io/use-regex: \"true\" 说明： 上面定义的所有path到ingress controller都将会转换成nginx location规则，那么关于location的优先级与nginx一样，path转换到nginx后，会将path规则最长的排在最前面，最短的排在最后面。 案例2 个性化配置 在案例1的基础上面我们可以增加了annotations的一些配置 kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/use-regex: \"true\" #连接超时时间，默认为5s nginx.ingress.kubernetes.io/proxy-connect-timeout: \"600\" #后端服务器回转数据超时时间，默认为60s nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\" #后端服务器响应超时时间，默认为60s nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\" #客户端上传文件，最大大小，默认为20m nginx.ingress.kubernetes.io/proxy-body-size: \"10m\" 案例3 配置URL重定向rewrite-target 使用Nginx Ingress Controller的时候，Nginx会将路径完整转发到后端（如，从Ingress访问的/service1/api路径会直接转发到后端Pod的/service1/api/路径）。如果您后端的服务路径为/api，则会出现路径错误，导致404的情况。该情况下，您可以通过配置rewrite-target的方式，来将路径重写至需要的目录 nginx.ingress.kubernetes.io/rewrite-target: /$2 完整配置如下 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/use-regex: \"true\" nginx.ingress.kubernetes.io/rewrite-target: /$2 name: nginx namespace: nginx-ingress spec: rules: - host: nginx.chen1900s.cn http: paths: - backend: serviceName: nginx-v1 servicePort: 80 path: /svc(/|$)(.*) pathType: ImplementationSpecific 案例4 rewrite配置二 匹配请求头，主要用于根据请求头信息将用户请求转发到不同的应用，比如根据不同的客户端转发请求 nginx.ingress.kubernetes.io/server-snippet：扩展配置到Server章节。 nginx.ingress.kubernetes.io/configuration-snippet：扩展配置到Location章节。 annotations: nginx.ingress.kubernetes.io/server-snippet: | rewrite ^/v4/(.*)/card/query http://www.chen1900s.cn/v5/#!/card/query permanent; nginx.ingress.kubernetes.io/configuration-snippet: | rewrite ^/v6/(.*)/card/query http://www.chen1900s.cn/v7/#!/card/query permanent; 示例配置生成的nginx.conf如下所示 ## start server www.chen1900s.cn server { server_name www.chen1900s.cn ; listen 80; listen [::]:80; set $proxy_upstream_name \"-\"; ### server-snippet配置。 rewrite ^/v4/(.*)/card/query http://www.chen1900s.cn/v5/#!/card/query permanent; ... ### configuration-snippet配置。 rewrite ^/v6/(.*)/card/query http://www.chen1900s.cn/v7/#!/card/query permanent; ... } ## end server www.chen1900s.cn 这里直接使用了“nginx.ingress.kubernetes.io/server-snippet”来指定配置，这里可以直接写nginx的配置，通过这里可以不止是实现rewrite重写，还可以实现更多的功能需求，只要是作用于server的都可以 案例5 域名登录认证 有时候我们的服务没有提供登录认证，但是有不希望将服务提供给所有的人都能访问，那么可以通过ingress上的认证控制访问，常用的2种认证方式。 1，基本身份认证 创建secret 用于访问凭证 [root@VM-0-17-tlinux ~/nginx-ingress]# htpasswd -c auth admin New password: Re-type new password: Adding password for user admin #创建secret [root@VM-0-17-tlinux ~/nginx-ingress]# kubectl create secret generic basic-auth --from-file=auth -n nginx-ingress secret/basic-auth created apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx kubernetes.io/ingress.rule-mix: \"true\" nginx.ingress.kubernetes.io/auth-realm: Authentication Required - admin #请求用户名 nginx.ingress.kubernetes.io/auth-secret: basic-auth #对应的secret nginx.ingress.kubernetes.io/auth-type: basic #认证方式 nginx.ingress.kubernetes.io/use-regex: \"true\" name: nginx-basic-auth namespace: nginx-ingress spec: rules: - host: nginx.chen1900s.cn http: paths: - backend: serviceName: nginx-v1 servicePort: 80 path: / 验证 #再不输入认真情况下，访问会出现 401 Unauthorized [root@VM-249-130-tlinux ~/nginx-ingress]# curl -kI https://nginx.chen1900s.cn/ HTTP/1.1 401 Unauthorized Date: Sun, 18 Sep 2022 07:10:24 GMT Content-Type: text/html Content-Length: 172 Connection: keep-alive WWW-Authenticate: Basic realm=\"Authentication Required - admin\" Strict-Transport-Security: max-age=15724800; includeSubDomains #携带访问凭证访问 [root@VM-249-130-tlinux ~/nginx-ingress]# curl -kI https://nginx.chen1900s.cn/ -u 'admin:admin123' HTTP/1.1 200 OK Date: Sun, 18 Sep 2022 07:11:32 GMT Content-Type: text/plain Connection: keep-alive Strict-Transport-Security: max-age=15724800; includeSubDomains 2.2 外部身份验证 有时候我们有自己的鉴权中心，也是可以使用外部身份进行认证的，这里我们采用https://httpbin.org/basic-auth/user/passwd这个作为外部身份，这个默认账号和密码user/passwd apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx kubernetes.io/ingress.rule-mix: \"true\" nginx.ingress.kubernetes.io/auth-url: https://httpbin.org/basic-auth/user/passwd nginx.ingress.kubernetes.io/use-regex: \"true\" name: nginx-basic-auth-out namespace: nginx-ingress spec: rules: - host: nginx.chen1900s.cn http: paths: - backend: serviceName: nginx-v1 servicePort: 80 path: / # curl -k https://nginx.chen1900s.cn/ -v -H 'Host: nginx.chen1900s.cn' -u 'user:passwd' 案例6 访问白名单 有时候我们需要给域名配置下访问白名单，只希望部分ip可以访问我的服务，这时候需要用到ingress的whitelist-source-range，我们可以通过这个注解来配置我们希望放通访问的ip。下面我们只放通81.69.221.19 也可以指定某一网段 可以访问 该案例需要nginx-ingress能够正常获取到客户端源IP，nginx-ingress-controller 对应的service需要是local模式 或者直连POD模式 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx kubernetes.io/ingress.rule-mix: \"true\" nginx.ingress.kubernetes.io/whitelist-source-range: 81.69.221.19 nginx.ingress.kubernetes.io/use-regex: \"true\" name: nginx-whitelist-ip namespace: nginx-ingress spec: rules: - host: nginx.chen1900s.cn http: paths: - backend: serviceName: nginx-v2 servicePort: 80 path: / 源IP未81.69.221.19正常访问 [root@VM-0-33-tlinux ~]# curl myip.ipip.net 当前 IP：81.69.221.19 来自于：中国 上海 上海 电信 [root@VM-0-33-tlinux ~]# curl http://nginx.chen1900s.cn nginx-v2 其他客户端禁止访问 [root@172-16-155-8 ~]# curl myip.ipip.net 当前 IP：121.5.26.195 来自于：中国 上海 上海 电信 [root@172-16-155-8 ~]# curl http://nginx.chen1900s.cn 403 Forbidden 403 Forbidden nginx 案例7 永久重定向配置重定向错误码 redirect主要用于域名重定向，比如访问a.com被重定向到b.com。 nginx.ingress.kubernetes.io/permanent-redirect: https://www.baidu.com nginx.ingress.kubernetes.io/permanent-redirect-code: \"308\" 案例8 客户端请求body的大小 如果遇到请求报错是 413 Request Entity Too Large 可以配置客户端请求body的大小，创建 ingress 时添加 annotations（注释） metadata: annotations: nginx.ingress.kubernetes.io/proxy-body-size: 1024m 案例9 414 Request URI too large或400 bad request错 如遇到调用后端接口时候，需要在header中传一段很长的token，会报\"414 Request URI too large\"，可以登陆nginx-ingress-controller pod里查看配置 解决方法是修改两个参数 参数一： #client_header_buffer_size：客户端请求头缓冲区大小， client_header_buffer_size 128k;#如果请求头总长度大于小于128k，则使用此缓冲区 参数二： #large_client_header_buffers：请求头总长度大于128k时使用large_client_header_buffers设置的缓存区 large_client_header_buffers 4 128k; #large_client_header_buffers 指令参数4为个数，128k为大小，默认是8k。申请4个128k。 apiVersion: v1 data: client-header-buffer-size: 128k #注意参数是中横线 large-client-header-buffers: 4 128k #注意参数是中横线 allow-backend-server-header: \"true\" enable-underscores-in-headers: \"true\" 案例10 上传超时 504：Gateway Timeout metadata: annotations: 　　 nginx.ingress.kubernetes.io/proxy-connect-timeout：\"300\" nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\" nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\" #连接超时时间，默认为5s nginx.ingress.kubernetes.io/proxy-connect-timeout: \"300\" #后端服务器回转数据超时时间，默认为60s nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\" #后端服务器响应超时时间，默认为60s nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\" 案例11 白名单及请求速率限制 可以限制速率来降低后端压力，比如如下配置： apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-nginx namespace: nginx-ingress annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/limit-rate: \"100K\" nginx.ingress.kubernetes.io/limit-whitelist: 81.69.221.19 nginx.ingress.kubernetes.io/limit-rps: \"1\" #为每秒1个连接数 nginx.ingress.kubernetes.io/limit-rpm: \"5\" #单个IP每分钟的连接数 spec: rules: - host: nginx.chen1900s.cn http: paths: - path: backend: serviceName: nginx-v4 servicePort: 80 用以设置基于流量、请求连接数、请求频率的访问控制。访问控制配置说明如下表所示。 注解 类型/选项 功能描述 nginx.ingress.kubernetes.io/limit-rate number 访问流量速度限制，同 Nginx 配置指令 limit_rate nginx.ingress.kubernetes.io/limit-rate-after number 启用访问流量速度限制的最大值，同 Nginx 配置指令 limit_rate_after nginx.ingress.kubernetes.io/limit-connections number 节并发连接数限制，同 Nginx 配置指令 limit_conn nginx.ingress.kubernetes.io/limit-rps number 每秒请求频率限制，burst 参数为给定值的 5 倍，响应状态码由 ConfigMap 的 limit-req-status-code 设定 nginx.ingress.kubernetes.io/limit-rpm number 每分钟请求频率限制，burst 参数为给定值的 5 倍，响应状态码由 ConfigMap 的 limit-req-status-code 设定 nginx.ingress.kubernetes.io/limit-whitelist CIDR 对以上限制设置基于 IP 的白名单 案例12 配置URL重定向的路由服 通过以下命令创建一个简单的Ingress，所有对/svc路径的访问都会重新定向到后端服务能够识别的/路径上面。 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /$1 name: rewrite-nginx-ingress namespace: nginx-ingress spec: rules: - host: rewrite-nginx-ingress.com http: paths: - backend: serviceName: nginx-v1 servicePort: 80 path: /svc/(.*) 执行以下命令，访问Nginx服务，替换IP_ADDRESS为Ingress对应的I #curl -k -H \"Host: rewrite-test-ingress.com\" http:///svc/foo # curl -k -H \"Host: rewrite-nginx-ingress.com\" http://118.24.224.221/svc/foo nginx-v1 案例13 配置安全的路由服务 支持多证书管理，为您的服务提供安全防护。 1，准备您的服务证书。如果没有证书，可以通过下面的方法生成测试证书。 说明** 域名需要与您的Ingress配置保持一致。 ** 执行以下命令，生成一个证书文件tls.crt和一个私钥文件tls.key openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=tls-nginx-ingress.com/O=tls-nginx-ingress.com\" 执行以下命令，创建密钥。 通过该证书和私钥创建一个名为tls-nginx-ingress的Kubernetes Secret。创建Ingress时需要引用这个Secret。 kubectl create secret tls tls-nginx-ingress --key tls.key --cert tls.crt -n nginx-ingress 2，执行以下命令，创建一个安全的Ingress服务。 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: tls-nginx-ingress namespace: nginx-ingress spec: rules: - host: tls-nginx-ingress.com http: paths: - backend: serviceName: nginx-v2 servicePort: 80 path: / tls: - hosts: - tls-nginx-ingress.com secretName: tls-nginx-ingress 3，执行以下命令，查询Ingress信息。 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# kubectl get ingress -n nginx-ingress | grep tls tls-nginx-ingress tls-nginx-ingress.com 114.117.219.97 80, 443 4m3s 4，配置hosts文件或者设置域名来访问该TLS服务 案例14 配置HTTPS双向认证 某些业务场景需要启用HTTPS双向验证，Ingress-Nginx支持该特性，配置步骤参考以下示例。 1，执行以下命令，创建自签的CA证书。 openssl req -x509 -sha256 -newkey rsa:4096 -keyout ca.key -out ca.crt -days 356 -nodes -subj '/CN=Fern Cert Authority' 2，执行以下命令，创建Server端证书。 执行以下命令，生成Server端证书的请求文件。 openssl req -new -newkey rsa:4096 -keyout server.key -out server.csr -nodes -subj '/CN=test.nginx.ingress.com' 执行以下命令，使用根证书签发Server端请求文件，生成Server端证书 openssl x509 -req -sha256 -days 365 -in server.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out server.crt 3，执行以下命令，创建Client端证书。 生成Client端证书的请求文件 openssl req -new -newkey rsa:4096 -keyout client.key -out client.csr -nodes -subj '/CN=Fern' 执行以下命令，使用根证书签发Client端请求文件，生成Client端证书。 openssl x509 -req -sha256 -days 365 -in client.csr -CA ca.crt -CAkey ca.key -set_serial 02 -out client.crt 4，执行以下命令，检查创建的证书 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# ls ca.crt ca.key client.crt client.csr client.key server.crt server.csr server.key 5，执行以下命令，创建CA证书的Secret。 kubectl create secret generic ca-secret --from-file=ca.crt=ca.crt -n nginx-ingress 6，执行以下命令，创建Server证书的Secret。 kubectl create secret tls tls-secret --cert server.crt --key server.key -n nginx-ingress 7，执行以下命令，创建测试用的Ingress用例。 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: description: 配置HTTPS双向认证 kubernetes.io/ingress.class: nginx kubernetes.io/ingress.rule-mix: \"false\" nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: \"true\" nginx.ingress.kubernetes.io/auth-tls-secret: nginx-ingress/ca-secret nginx.ingress.kubernetes.io/auth-tls-verify-client: \"on\" nginx.ingress.kubernetes.io/auth-tls-verify-depth: \"1\" name: test-nginx-ingress namespace: nginx-ingress spec: rules: - host: test.nginx.ingress.com http: paths: - backend: serviceName: nginx-a servicePort: 80 path: / 8，执行以下命令，查看Ingress的IP地址 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# kubectl get ingress -n nginx-ingress | grep test test-nginx-ingress test.nginx.ingress.com 114.117.219.97 80 3m34s 9，执行以下命令，更新Hosts文件，替换下面的IP地址为真实获取的Ingress的IP地址。 echo \"114.117.219.97 test.nginx.ingress.com\" >> /etc/hosts 结果验证 客户端不传证书访问 curl --cacert ./ca.crt https://test.nginx.ingress.com #预期输出 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl --cacert ./ca.crt https://test.nginx.ingress.com 400 No required SSL certificate was sent 400 Bad Request No required SSL certificate was sent nginx 客户端传证书访问 curl --cacert ./ca.crt --cert ./client.crt --key ./client.key https://test.nginx.ingress.com #预期输出 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl --cacert ./ca.crt --cert ./client.crt --key ./client.key https://test.nginx.ingress.com nginx-a hello world 案例15 配置域名支持正则化 在Kubernetes集群中，Ingress资源不支持对域名配置正则表达式，但是可以通过nginx.ingress.kubernetes.io/server-alias注解来实现 1，创建Ingress，以正则表达式~^www\\.\\d+\\.example\\.com为例。 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: description: 配置域名支持正则化 kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/server-alias: ~^www\\.\\d+\\.example\\.com$, abc.example.com name: regex-nginx-ingress namespace: nginx-ingress spec: rules: - host: regex-nginx-ingress.com http: paths: - backend: serviceName: nginx-b servicePort: 80 path: / 2，执行以下命令，查看对应Nginx Ingress Controller的配置。 ​ 执行以下命令，查看部署Nginx Ingress Controller服务的Pod。 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# kubectl get pods -n kube-system | grep nginx-ingress-nginxnginx-ingress-nginx-controller-5ddf7ccc4f-vss4f 1/1 Running 0 5h13m ​ 执行以下命令，查看对应Nginx Ingress Controller的配置，可以发现生效的配置（Server_Name字段） [root@VM-0-17-tlinux ~/tls/nginx-ingress]# kubectl -n kube-system exec nginx-ingress-ingress-nginx-controller-7dc5fd97f-t9l9l cat /etc/nginx/nginx.conf | grep -C3 \"regex-nginx-ingress.com\" kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead. } ## end server _ ## start server regex-nginx-ingress.com server { server_name regex-nginx-ingress.com abc.example.com ~^www\\.\\d+\\.example\\.com$ ; listen 80 ; listen 443 ssl ; -- } } ## end server regex-nginx-ingress.com ## start server rewrite-nginx-ingress.com server { 3，执行以下命令，获取Ingress对应的IP。 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# kubectl get ingress -n nginx-ingress | grep regex regex-nginx-ingress regex-nginx-ingress.com 114.117.219.97 80 12m 4，执行以下命令，进行不同规则下的服务访问测试，配置以下IP_ADDRESS为上一步获取的IP地址。 执行以下命令，通过Host: regex-nginx-ingress.com访问服务 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl -H \"Host: regex-nginx-ingress.com\" 114.117.219.97/ nginx-b 执行以下命令，通过Host: www.123.example.com访问服务 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl -H \"Host: www.123.example.com\" 114.117.219.97/ nginx-b 执行以下命令，通过Host: www.321.example.com访问服务 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl -H \"Host: www.321.example.com\" 114.117.219.97/ nginx-b 案例16 配置域名支持泛化 在Kubernetes集群中，Ingress资源支持对域名配置泛域名，例如，可配置*.ingress-regex.com泛域名。 1，部署以下模板，创建Ingress apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: regex-ingress namespace: nginx-ingress spec: rules: - host: '*.regex-ingress.com' http: paths: - backend: serviceName: nginx-c servicePort: 80 path: / 2，执行以下命令，查看对应Nginx Ingress Controller的配置，可以发现生效的配置（Server_Name字段） kubectl exec -n kube-system cat /etc/nginx/nginx.conf | grep -C3 \"regex-ingress.com\" [root@VM-0-17-tlinux ~/tls/nginx-ingress]# kubectl -n kube-system exec nginx-ingress-ingress-nginx-controller-7dc5fd97f-t9l9l cat /etc/nginx/nginx.conf | grep -C3 \"regex-ingress.com\" kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead. # Global filters ## start server *.regex-ingress.com server { server_name ~^(?[\\w-]+)\\.regex-ingress\\.com$ ; -- } } ## end server *.regex-ingress.com ## start server _ server { 执行以下命令，获取Ingress对应的IP [root@VM-0-17-tlinux ~/tls/nginx-ingress]# kubectl get ingress -nnginx-ingress | grep regex-ingress.com regex-ingress *.regex-ingress.com 114.117.219.97 80 10m 4，执行以下命令，进行不同规则下的服务访问测试，配置以下IP_ADDRESS为上一步获取的IP地址。 执行以下命令，通过Host: abc.regex-ingress.com 访问服务。 # curl -H \"Host: abc.regex-ingress.com\" / [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl -H \"Host: abc.regex-ingress.com\" 114.117.219.97/ nginx-c 预期输出： nginx-c 执行以下命令，通过Host: 123.regex-ingress.com访问服务。 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl -H \"Host: 123.regex-ingress.com\" 114.117.219.97/ nginx-c 执行以下命令，通过Host: a1b1.regex-ingress.com访问服务。 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl -H \"Host: ab1.regex-ingress.com\" 114.117.219.97/ nginx-c 案例17 cookie会话保持 nginx.ingress.kubernetes.io/session-cookie-name 默认为round-robin，在具体ingress资源中通过ingress metadata.annotations字段可具体设置 通过会话cookie进行一致性hash均衡算法 ingress.kubernetes.io/affinity: \"cookie\" ingress.kubernetes.io/session-cookie-name: \"route\" ingress.kubernetes.io/session-cookie-hash: \"sha1\" 通过客户端ip进行一致性hash的均衡算法 nginx.ingress.kubernetes.io/upstream-hash-by: \"${remote_addr}\" 通过请求uri进行一致性hash的均衡算法 nginx.ingress.kubernetes.io/upstream-hash-by: \"${request_uri}\" 案例18 nginx-ingress 开启跨域(CORS) 参考官方文档 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-ingress nginx.ingress.kubernetes.io/cors-allow-headers: >- DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization nginx.ingress.kubernetes.io/cors-allow-methods: 'PUT, GET, POST, OPTIONS' nginx.ingress.kubernetes.io/cors-allow-origin: '*' nginx.ingress.kubernetes.io/enable-cors: 'true' nginx.ingress.kubernetes.io/service-weight: '' name: cors-nginx-ingress namespace: nginx-ingress spec: rules: - host: cors-nginx-ingress.com http: paths: - backend: serviceName: nginx-v2 servicePort: 80 path: / tls: - hosts: - tls-nginx-ingress.com secretName: tls-nginx-ingress 跨域访问功能配置说明如下表所示。 注解 类型 功能描述 nginx.ingress.kubernetes.io/enable-cors true 或 false 是否启用跨域访问支持，默认为 false nginx.ingress.kubernetes.io/cors-allow-origin string 允许跨域访问的域名，默认为 *，表示接受任意域名的访问 nginx.ingress.kubernetes.io/cors-allow-methods string 允许跨域访问方法，默认为 GET、PUT、POST、DELETE、PATCH、OPTIONS nginx.ingress.kubernetes.io/cors-allow-headers string 允许跨域访问的请求头，默认为 DNT，X-CustomHeader、Keep-Alive、User-Agent、X-Requested-With、If-Modified-Since、Cache-Control、Content-Type、Authorization nginx.ingress.kubernetes.io/cors-allow-credentials true 或 false 设置在响应头中 Access-Control-Allow-Credentials 的值，设置是否允许客户端携带验证信息，如 cookie 等，默认为 true nginx.ingress.kubernetes.io/cors-max-age number 设置响应头中 Access-Control-Max-Age 的值，设置返回结果可以用于缓存的最长时间，默认为 1728000 秒 案例19 nginx-ingress关闭80强制跳转443 默认情况下，如果ingress对象入口启用了TLS，则ingress-controller将使用308永久重定向响应将HTTP客户端重定向到HTTPS端口443 1，执行以下命令，创建一个安全的Ingress服务(带TLS证书 ) apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-ingress name: tls-nginx-ingress namespace: nginx-ingress spec: rules: - host: tls-nginx-ingress.com http: paths: - backend: serviceName: nginx-v2 servicePort: 80 path: / tls: - hosts: - tls-nginx-ingress.com secretName: tls-nginx-ingress 2，执行以下命令，返回的是308信息。 [root@VM-0-17-tlinux ~/tls/nginx-ingress]# curl -I http://tls-nginx-ingress.com HTTP/1.1 308 Permanent Redirect Date: Thu, 25 Nov 2021 10:07:30 GMT Content-Type: text/html Content-Length: 164 Connection: keep-alive Location: https://tls-nginx-ingress.com 3，可以在特定ingress资源的metadata.annotations中通过配置nginx.ingress.kubernetes.io/ssl-redirect: \"false\" 使用注释禁用此功能，关闭80强制跳转443 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-ingress nginx.ingress.kubernetes.io/ssl-redirect: 'false' #就可以禁止http强制跳转至https name: tls-nginx-ingress namespace: nginx-ingress spec: rules: - host: tls-nginx-ingress.com http: paths: - backend: serviceName: nginx-v2 servicePort: 80 path: / tls: - hosts: - tls-nginx-ingress.com secretName: tls-nginx-ingress 4，添加annotations后再去访问，就可以正常访问80端口 [root@VM-0-17-tlinux ~]# curl -I http://tls-nginx-ingress.com HTTP/1.1 200 OK Date: Thu, 25 Nov 2021 10:12:47 GMT Content-Type: text/plain Connection: keep-alive [root@VM-0-17-tlinux ~]# curl http://tls-nginx-ingress.com nginx-v2 案例20 nginx-ingress中ingress匹配优先级 首先在nginx中location的匹配优先级大致为：精准匹配 > 前缀匹配 > 正则匹配> / 其中，前缀匹配：^~，精准匹配 =，正则匹配细分为： ~ 区分大小写（大小写敏感）匹配成功；~ 不区分大小写匹配成功；!~ 区分大小写匹配失败；!~ 不区分大小写匹配失败 而ingress资源对象中，spec.rules.http.paths.path字段默认只支持不区分大小写的正则匹配，但前提需要设置nginx.ingress.kubernetes.io/use-regex注释设置 为true(默认值为false)来启用此功能 1，根据如下yaml文件创建ingress资源 apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: \"nginx-ingress\" nginx.ingress.kubernetes.io/use-regex: \"true\" namespace: nginx-ingress name: use-regex-nginx-ingress spec: rules: - host: use-regex-nginx-ingress.com http: paths: - path: / backend: serviceName: nginx-v1 servicePort: 80 - path: /wifi backend: serviceName: nginx-v2 servicePort: 80 2，进入到ingress-controller的pod中，观察nginx配置文件，发现location的正则匹配已生效 root@VM-0-17-tlinux ~]# kubectl exec -it nginx-ingress-ingress-nginx-controller-7dc5fd97f-t9l9l -n kube-system /bin/bash 3，如果创建时候nginx.ingress.kubernetes.io/use-regex: \"false\" 或者不设置验证下效果 [root@VM-0-17-tlinux ~]# kubectl exec -it nginx-ingress-ingress-nginx-controller-7dc5fd97f-t9l9l -n kube-system /bin/bash 案例21 支持websocket配置 1，准备服务证书。如果没有证书，可以通过下面的方法生成测试证书。 说明** 域名需要与您的Ingress配置保持一致。 ** 执行以下命令，生成一个证书文件tls.crt和一个私钥文件tls.key openssl req -x509 -nodes -days 1000 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=websocket-nginx-ingress.com/O=websocket-nginx-ingress.com\" 执行以下命令，创建密钥。 通过该证书和私钥创建一个名为tls-nginx-ingress的Kubernetes Secret。创建Ingress时需要引用这个Secret。 kubectl create secret tls websocket-nginx-ingress --key tls.key --cert tls.crt -n nginx-ingress 2，执行以下命令，创建一个安全的Ingress服务 (由于没有websocket，暂时没有做验证) apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx-ingress nginx.ingress.kubernetes.io/configuration-snippet: >- nginx.ingress.kubernetes.io/proxy-read-timeout 3600; nginx.ingress.kubernetes.io/proxy-send-timeout 3600; name: websocket-nginx-ingress namespace: nginx-ingress spec: rules: - host: websocket-nginx-ingress.com http: paths: - backend: serviceName: nginx-v2 servicePort: 80 path: / tls: - hosts: - websocket-nginx-ingress.com secretName: websocket-nginx-ingress 案例22 通过configmap 定义全局常规参数 data: multi_accept: on; use: epoll; user: www; worker_connections: 65535; worker_cpu_affinity: auto; worker_processes: auto; worker_rlimit_nofile: 300000; # 把真实IP地址传给后端 compute-full-forwarded-for: \"true\" forwarded-for-header: \"X-Forwarded-For\" use-forwarded-headers: \"true\" # 关闭版本显示 server-tokens: \"false\" # 客户端请求头的缓冲区大小 client-header-buffer-size: \"512k\" # 设置用于读取大型客户端请求标头的最大值number和size缓冲区 large-client-header-buffers: \"16 512k\" # 读取客户端请求body的缓冲区大小 client-body-buffer-size: \"968k\" # 代理缓冲区大小 proxy-buffer-size: \"1024k\" # 代理body大小 proxy-body-size: \"50m\" # 服务器名称哈希大小 server-name-hash-bucket-size: \"128\" # map哈希大小 map-hash-bucket-size: \"128\" # SSL加密套件 ssl-ciphers: \"ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA\" # ssl 协议 ssl-protocols: \"TLSv1 TLSv1.1 TLSv1.2\" #定义json 访问日志格式 log-format-upstream: '{\"time\": \"$time_iso8601\", \"remote_addr\": \"$proxy_protocol_addr\", \"x-forward-for\": \"$proxy_add_x_forwarded_for\", \"request_id\": \"$req_id\", \"remote_user\": \"$remote_user\", \"bytes_sent\": $bytes_sent, \"request_time\": $request_time, \"status\":$status, \"vhost\": \"$host\", \"request_proto\": \"$server_protocol\", \"path\": \"$uri\", \"request_query\": \"$args\", \"request_length\": $request_length, \"duration\": $request_time,\"method\": \"$request_method\", \"http_referrer\": \"$http_referer\", \"http_user_agent\": \"$http_user_agent\"}' 更多配置可以参考文档 案例23 获取真实IP地址配置 nginx-ingress官方是通过修改容器的配置文件来配置，配置文件：ingress-nginx/ingress-nginx-controller kubectl edit cm -n kube-system nginx-ingress-nginx-controller -o yaml compute-full-forwarded-for: \"true\" forwarded-for-header: \"X-Forwarded-For\" use-forwarded-headers: \"true\" compute-full-forwarded-for: \"true\" //如果为真，NGINX 将传入的X-Forwarded-*标头传递给上游。当 NGINX 在设置这些标头的另一个 L7 代理/负载均衡器之后使用此选项 forwarded-for-header: X-Forwarded-For //设置用于标识客户端的原始 IP 地址的标头字段。后端程序就可以在http 包的header里的X-Forwarded-For获取真实ip use-forwarded-headers: \"true\" //将远程地址附加到 X-Forwarded-For 标头，而不是用pod或者其它cookie信息替换它。启用此选项后，应用程序负责根据自己的受信任代理列表来提取客户端 IP 保存后立即生效。随后ingress的添加真实的IP行为会与RFC一样都依次添加到X-Forwarded-For中了 在TKE集群环境中，想获取客户端源IP，需要nginx-ingress-controller 是local模式或者POD直连模式才可以，不需要修改配置文件 local模式 nginx-ingress-controller-service 配置 验证 ingress示例： apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx kubernetes.io/ingress.rule-mix: \"false\" nginx.ingress.kubernetes.io/use-regex: \"true\" name: whoami namespace: nginx-ingress spec: rules: - host: whoami.chen1900s.cn http: paths: - backend: serviceName: whoami servicePort: 80 path: / pathType: ImplementationSpecific 直连POD模式参考TKE官方文档 案例24 nginx-ingress做tcp/udp4层网络转发 k8s集群通过nginx-ingress做tcp\\udp 4层网络转发 1，检查nginx-ingress是否开启tcp\\udp转发 - args: - --tcp-services-configmap=kube-system/nginx-ingress-nginx-tcp - --udp-services-configmap=kube-system/nginx-ingress-nginx-udp 2，示例 kuard-demo.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kuard namespace: nginx-ingress spec: selector: matchLabels: app: kuard replicas: 1 template: metadata: labels: app: kuard spec: containers: - image: ccr.ccs.tencentyun.com/chenjingwei/kuard-amd64:v1 imagePullPolicy: Always name: kuard ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: kuard namespace: nginx-ingress spec: ports: - port: 9527 targetPort: 8080 protocol: TCP selector: app: kuard 3，需要修改下configmap # kubectl -n kube-system get cm | grep nginx-ingress-nginx nginx-ingress-nginx-controller 9 133d nginx-ingress-nginx-tcp 0 133d nginx-ingress-nginx-udp 0 133d # kubectl -n kube-system edit cm nginx-ingress-nginx-tcp [root@VM-0-17-tlinux ~]# kubectl -n kube-system get cm nginx-ingress-nginx-tcp -o yaml apiVersion: v1 data: #TKE默认么有data \"9527\": nginx-ingress/kuard:9527 #添加这个配置 kind: ConfigMap metadata: labels: k8s-app: nginx-ingress-nginx-tcp qcloud-app: nginx-ingress-nginx-tcp name: nginx-ingress-nginx-tcp namespace: kube-system 4，进入nginx-ingress容器查看TCP services处会出现对应的负载配置 # kubectl -n kube-system exec -it nginx-ingress-nginx-controller-5ddf7ccc4f-v4pzp -- /bin/sh vi nginx.conf 镜像过滤 # TCP services server { preread_by_lua_block { ngx.var.proxy_upstream_name=\"tcp-nginx-ingress-kuard-9527\"; } listen 9527; listen [::]:9527; proxy_timeout 600s; proxy_pass upstream_balancer; } 5，编辑nginx-ingress-nginx-controller svc 添加对应端口 apiVersion: v1 kind: Service metadata: annotations: service.cloud.tencent.com/direct-access: \"false\" labels: k8s-app: nginx-ingress-nginx-controller qcloud-app: nginx-ingress-nginx-controller name: nginx-ingress-nginx-controller namespace: kube-system spec: clusterIP: 172.18.248.35 externalTrafficPolicy: Cluster ports: - name: 80-80-tcp nodePort: 31899 port: 80 protocol: TCP targetPort: 80 - name: 443-443-tcp nodePort: 32534 port: 443 protocol: TCP targetPort: 443 - name: 9527-9527-tcp-5q8prs0zx68 nodePort: 32677 port: 9527 protocol: TCP targetPort: 9527 selector: k8s-app: nginx-ingress-nginx-controller qcloud-app: nginx-ingress-nginx-controller sessionAffinity: None type: LoadBalancer , 然后通过nginx-ingress-nginx-controller 的svc clb访问 [root@VM-0-17-tlinux ~]# kubectl -n kube-system get svc | grep nginx-ingress-nginx-controller nginx-ingress-nginx-controller LoadBalancer 172.18.248.35 118.24.224.251 80:31899/TCP,443:32534/TCP 3m3s nginx-ingress-nginx-controller-admission ClusterIP 172.18.251.207 443/TCP 133d 案例25 Nginx-Ingress 实现grpc转发 参考文档：https://cloud.tencent.com/developer/article/1730604 案例26 配置HTTPS服务转发到后端容器为HTTPS协议 Nginx Ingress Controller默认使用HTTP协议转发请求到后端业务容器。当您的业务容器为HTTPS协议时，可以通过使用注解nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"来使得Nginx Ingress Controller使用HTTP协议转发请求到后端业务容器。 Nginx Ingress配置示例如下： apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/backend-protocol: HTTPS name: backend-protocol-https-ingress namespace: default spec: rules: - host: example.chen1900s.cn http: paths: - backend: service: name: nginx port: number: 443 path: / pathType: ImplementationSpecific tls: - hosts: - example.chen1900s.cn secretName: chen1900s "},"docs/tke/nginx-ingress-grpc.html":{"url":"docs/tke/nginx-ingress-grpc.html","title":"Nginx-Ingress实现gRPC服务访问","summary":"在TKE集群中使用Nginx-Ingress实现gRPC服务访问","keywords":"","body":"这里验证下如何通过Ingress-NGINX控制器将流量路由到gRPC服务 前提条件： 1，已创建或者自建kubernetes集群 2，已经申请对应域名 3，已安装了ingress-nginx-controller 4，GRPC服务 可以使用go-grpc-greeter-server作为示例 5，和域名对应的SSL证书 部署gRPC应用 确保gRPC应用程序正在运行并监听连接，本示例使用镜像ccr.ccs.tencentyun.com/v_cjweichen/grpc-server:latest创建gRPC服务 复制以下YAML内容创建grpc.yaml文件 apiVersion: apps/v1 kind: Deployment metadata: name: grpc-service namespace: nginx-ingress spec: replicas: 1 selector: matchLabels: run: grpc-service template: metadata: labels: run: grpc-service spec: containers: - image: ccr.ccs.tencentyun.com/chenjingwei/grpc-server:latest imagePullPolicy: Always name: grpc-service ports: - containerPort: 50051 protocol: TCP imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always 为gRPC应用创建Service apiVersion: v1 kind: Service metadata: labels: run: grpc-service name: grpc-service namespace: default spec: ports: - port: 50051 protocol: TCP targetPort: 50051 selector: run: grpc-service sessionAffinity: None type: ClusterIP 执行以下命令查看Pod [root@VM-0-16-tlinux ~/grpc]# kubectl get pods,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/grpc-service-5d48b78787-hcfhf 1/1 Running 0 3m57s 172.16.0.5 10.0.0.16 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/grpc-service ClusterIP 172.16.255.180 50051/TCP 2m41s run=grpc-service 为gRPC应用创建ingress路由 复制以下YAML内容创建grpc-ingress.yaml文件。 注意 部署gRPC服务所使用的Ingress需要在annotation中加入nginx.ingress.kubernetes.io/backend-protocol，值为GRPC。 本示例使用的域名为grpc.example.com，请根据实际情况修改。 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/backend-protocol: GRPC # 注意这里：必须要配置以指明后端服务为gRPC服务 name: grpc-ingress namespace: default spec: rules: - host: grpc.chen1900s.cn http: paths: - backend: serviceName: grpc-service servicePort: 50051 path: / pathType: ImplementationSpecific tls: - hosts: - grpc.chen1900s.cn secretName: grpc-secret 测试连接 一旦我们将配置应用到Kubernetes上，就该测试我们是否可以实际与后端通信了。为此，我们将使用grpcurl实用测试程序: 需要提前安装grpcurl工具，grpcurl是Go语言开源社区开发的工具，需要手工安装： wget https://github.com/fullstorydev/grpcurl/releases/download/v1.7.0/grpcurl_1.7.0_linux_x86_64.tar.gz tar -xvf grpcurl_1.7.0_linux_x86_64.tar.gz chmod +x grpcurl ./grpcurl -help 本示例中使用域名grpc.chen1900s.cn以及自签证书，执行以下命令验证请求是否成功转发到后端服务。 ##grpcurl -insecure -authority grpc.example.com :443 list ##ip_address为Nginx Ingress Controller的Service外部IP [root@VM-0-16-tlinux ~/grpc]# ./grpcurl -insecure -authority grpc.chen1900s.cn 170.106.134.2:443 list greet.GrpcService grpc.reflection.v1alpha.ServerReflection 从预期输出可得，流量被Ingress成功转发到后端gRPC服务 注意点：由于Nginx grpc_pass的限制，目前对于gRPC服务，暂不支持service-weight的配置 "},"docs/tke/horizontalpodautoscaler.html":{"url":"docs/tke/horizontalpodautoscaler.html","title":"HorizontalPodAutoscaler演练","keywords":"","body":"操作场景 实例（Pod）自动扩缩容功能（Horizontal Pod Autoscaler，HPA）可以根据目标实例 CPU 利用率的平均值等指标自动扩展、缩减服务的 Pod 数量 工作原理 HPA 后台组件会每隔15秒向腾讯云云监控拉取容器和 Pod 的监控指标，然后根据当前指标数据、当前副本数和该指标目标值进行计算，计算所得结果作为服务的期望副本数。当期望副本数与当前副本数不一致时，HPA 会触发 Deployment 进行 Pod 副本数量调整，从而达到自动伸缩的目的。 以 CPU 利用率为例，假设当前有2个实例， 平均 CPU 利用率（当前指标数据）为90%，自动伸缩设置的目标 CPU 为60%， 则自动调整实例数量为：90% × 2 / 60% = 3个。 注意： 如果用户设置了多个弹性伸缩指标，HPA 会依据各个指标，分别计算出目标副本数，取最大值进行扩缩容操作。 Pod 水平自动扩缩原理：https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/ 注意事项 当指标类型选择为 CPU 利用率（占 Request）时，必须为容器设置 CPU Request。 策略指标目标设置合理，例如设置70%给容器和应用，预留30%的余量。 保持 Pod 和 Node 健康（避免 Pod 频繁重建）。 保证用户请求的负载均衡稳定运行。 HPA 在计算目标副本数时会有一个10%的波动因子。如果在波动范围内，HPA 并不会调整副本数目。 如果服务对应的 Deployment.spec.replicas 值为0，HPA 将不起作用。 如果对单个 Deployment 同时绑定多个 HPA ，则创建的 HPA 会同时生效，会造成工作负载的副本重复扩缩。 演练 运行 php-apache 服务器并暴露服务 为了演示 Horizontal Pod Autoscaler，我们将使用一个基于 php-apache 镜像的 定制 Docker 镜像。Dockerfile 内容如下： FROM php:5-apache COPY index.php /var/www/html/index.php RUN chmod a+rx index.php 该文件定义了一个 index.php 页面来执行一些 CPU 密集型计算： #镜像构建 docker build -t tcr-chen.tencentcloudcr.com/docker/hpa-example . 使用下面的配置启动一个 Deployment 来运行这个镜像并暴露一个服务： apiVersion: apps/v1 kind: Deployment metadata: name: php-apache spec: selector: matchLabels: run: php-apache replicas: 1 template: metadata: labels: run: php-apache spec: containers: - name: php-apache image: tcr-chen.tencentcloudcr.com/docker/hpa-example:latest ports: - containerPort: 80 resources: limits: cpu: 500m requests: cpu: 200m --- apiVersion: v1 kind: Service metadata: name: php-apache labels: run: php-apache spec: ports: - port: 80 selector: run: php-apache 创建 Horizontal Pod Autoscaler 现在，php-apache 服务器已经运行，我们将通过 kubectl autoscale 命令创建 Horizontal Pod Autoscaler。 以下命令将创建一个 Horizontal Pod Autoscaler 用于控制我们上一步骤中创建的 Deployment，使 Pod 的副本数量维持在 1 到 10 之间。 大致来说，HPA 将（通过 Deployment）增加或者减少 Pod 副本的数量以保持所有 Pod 的平均 CPU 利用率在 50% 左右（由于每个 Pod 请求 200 毫核的 CPU，这意味着平均 CPU 用量为 100 毫核） [root@VM-0-17-tlinux ~/hpa/php-apache]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 horizontalpodautoscaler.autoscaling/php-apache autoscaled 我们可以通过以下命令查看 Autoscaler 的状态： [root@VM-0-17-tlinux ~/hpa/php-apache]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 2m21s 请注意当前的 CPU 利用率是 0%，这是由于我们尚未发送任何请求到服务器 （CURRENT 列显示了相应 Deployment 所控制的所有 Pod 的平均 CPU 利用率） 增加负载 我们将看到 Autoscaler 如何对增加负载作出反应。 我们将启动一个容器，并通过一个循环向 php-apache 服务器发送无限的查询请求 （请在另一个终端中运行以下命令） kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\" 一分钟时间左右之后，通过以下命令，我们可以看到 CPU 负载升高了： [root@VM-0-17-tlinux ~/hpa/php-apache]# kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 249%/50% 1 10 5 8m4s 这时，由于请求增多，CPU 利用率已经升至请求值的 249%。 可以看到，Deployment 的副本数量已经增长到了 5： [root@VM-0-17-tlinux ~/hpa/php-apache]# kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 16%/50% 1 10 8 19m 说明： 有时最终副本的数量可能需要几分钟才能稳定下来。由于环境的差异， 不同环境中最终的副本数量可能与本示例中的数量不同 停止负载 我们将通过停止负载来结束我们的示例。 在我们创建 busybox 容器的终端中，输入 + C 来终止负载的产生。 然后我们可以再次检查负载状态（等待几分钟时间）： 根据不同业务场景调节 HPA 扩缩容灵敏度 使用场景 在 K8S 1.18之前，HPA 扩缩容无法调整灵敏度： 对于缩容，由 kube-controller-manager 的 --horizontal-pod-autoscaler-downscale-stabilization-window 参数控制缩容时间窗口，默认为5分钟，即负载减小后至少需要5分钟后才开始缩容。 对于扩容，由 hpa controller 固定的算法、硬编码的常量因子来控制扩容速度，无法自定义。 K8S 设计逻辑导致用户无法自定义 HPA 的扩缩容灵敏度，不同的业务场景对于扩缩容灵敏度要求并不一样，例如： 对于有流量突发的关键业务，需要快速扩容（防止流量瓶颈）、缓慢缩容（防止另一个流量高峰）。 对于需要处理大量数据的离线业务，在处理高峰期时应尽快扩容以减少处理时间，高峰期后应尽快缩容以节约成本。 对于处理常规数据、网络流量的业务，需要以正常速度扩大和缩小规模，以减少抖动。 K8S 1.18在 HPA Spec 下新增了 behavior 字段，该字段提供 scaleUp 和 scaleDown 两个字段分别控制扩容和缩容行为 示例1：快速扩容 apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: labels: qcloud-app: hpa-php-apache name: hpa-php-apache namespace: default spec: maxReplicas: 1000 metrics: - pods: metricName: k8s_pod_rate_cpu_core_used_limit targetAverageValue: \"80\" type: Pods minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache behavior: scaleUp: policies: - type: Percent value: 300 periodSeconds: 15 ke控制台通过v2beta1的接口去获取的，因此展示的是v2beta1版本；kubectl 命令默认用的是v1的版本去获取的这个资源对象信息，所以展示的v1 解决：执行这个命令： kubectl edit HorizontalPodAutoscaler.v2beta2.autoscaling/ -n kubectl get HorizontalPodAutoscaler.v2beta2.autoscaling/ -n //查看 kubectl get HorizontalPodAutoscaler.v2beta2.autoscaling/php-apache -o yaml ####### apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: php-apache namespace: default spec: behavior: scaleDown: policies: - periodSeconds: 15 type: Percent value: 100 selectPolicy: Max stabilizationWindowSeconds: null scaleUp: policies: - periodSeconds: 15 type: Percent value: 300 selectPolicy: Max stabilizationWindowSeconds: 0 maxReplicas: 10 metrics: - resource: name: cpu target: averageUtilization: 50 type: Utilization type: Resource minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache "},"docs/tke/tke-nfs-subdir.html":{"url":"docs/tke/tke-nfs-subdir.html","title":"TKE集群NFS动态创建子目录挂载","summary":"Kubernetes NFS Subdir External Provisioner组件介绍","keywords":"","body":"背景 NFS subdir external provisioner组件是使用现有的和已配置的NFS服务器来支持通过持久卷声明动态分发Kubernetes持久卷。持久化卷按以下方式命名${namespace}-${pvcName}-${pvName}，在Kubernetes 环境中我们经常遇到是一个PV对应一个NFS实例场景，针对这种场景如果需要多个PVC&PV使用一个NFS实例，就需要提前根据NFS不同子目录创建大量的PV等待PVC去绑定，不太好维护，可以使用nfs-subdir-external-provisioner插件来实现动态创建子目录的需求，下面主要来介绍下 部署安装 环境准备： kubernetes集群一个，我这里是使用的是腾讯云TKE集群 NFS服务器实例，或者使用云厂商提供的NFS产品 helm客户端，并且能够正常链接到集群 Helm方式安装 可以根据需求修改指定参数后部署 确保您的NFS服务器可以从您的Kubernetes集群访问，并获取连接到它所需的信息。至少需要它的主机名和共享路径。 $ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ # helm repo list #查看添加helm repo仓库情况 NAME URL tcr-chen-helm https://tcr-chen.tencentcloudcr.com/chartrepo/helm nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ # 【可选步骤，可以将helm chart 包下载下来 上传到自己的镜像仓库，方便后续其他集群安装】 # 下载 helm chart 文件至本地目录，查看可以指定的 values 选项（可选） $ helm pull nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --untar $ tar zcvf nfs-subdir-external-provisioner-1.0.0.tgz nfs-subdir-external-provisioner/ $ helm cm-push nfs-subdir-external-provisioner-1.0.0.tgz tcr-chen-helm # 默认镜像是国外镜像，可以下载下来上传到自己的镜像仓库里面 $ docker pull k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 $ docker tag 932b0bface75 ccr.ccs.tencentyun.com/chenjingwei/nfs-subdir-external-provisioner:v4.0.2 $ docker push ccr.ccs.tencentyun.com/chenjingwei/nfs-subdir-external-provisioner:v4.0.2 # 使用类似下面命令安装 nfs-subdir-external-provisioner 资源 # helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=172.16.0.33 \\ --set nfs.path=/nfs \\ --set image.repository=ccr.ccs.tencentyun.com/chenjingwei/nfs-subdir-external-provisioner \\ --set image.tag=v4.0.2 #出现如下提示 表示安装成功 NAME: nfs-subdir-external-provisioner LAST DEPLOYED: Tue Feb 15 13:22:03 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: Non 手动YAML安装 1，创建账号ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 2，部署应用 apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: ccr.ccs.tencentyun.com/chenjingwei/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: cluster.local/nfs-subdir-external-provisioner - name: NFS_SERVER value: 172.16.0.33 #替换成自己的NFS服务器地址 - name: NFS_PATH value: /nfs #NFS上面的目录 volumes: - name: nfs-client-root nfs: server: 172.16.0.33 path: /nfs 3，创建storageClass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client provisioner: cluster.local/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" 基本使用 前提是nfs-subdir-external-provisioner组件已经正常运行 1，配置使用 CFS 文件系统子目录的 PVC 使用上一步部署的nfs-subdir-external-provisioner动态创建存储卷。 部署后会默认生成一个StorageClass，默认存储类名是\"nfs-client\"(也可以在部署时自定义指定)，如下： # kubectl get StorageClass | grep nfs-client nfs-client cluster.local/nfs-subdir-external-provisioner 4m8s 2，使用上面的StorageClass创建PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: nfs-subdir-pvc namespace: cjweichen spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 10Gi # kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs-subdir-pvc Bound pvc-5fdb9f45-6e98-4a9b-b6e0-920a6c7a6edc 10Gi RWX nfs-client 76s 会自动创建命令，命令命名方式是${namespace}-${pvcName}-${pvName} 3，查看自动创建的PV配置 4，创建workload 挂载对应PVC apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: cjweichen spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos volumeMounts: - mountPath: /mnt name: nfs volumes: - name: nfs persistentVolumeClaim: claimName: nfs-subdir-pvc # pwd /nfs/cjweichen-nfs-subdir-pvc-pvc-5fdb9f45-6e98-4a9b-b6e0-920a6c7a6edc "},"docs/tke/tke-nfs.html":{"url":"docs/tke/tke-nfs.html","title":"TKE使用CFS存储","keywords":"","body":"一，背景 CFS 文件存储支持 NFS v3.0 及 NFS v4.0 协议， 其中 NFS v3.0 是 NFS 协议较早期版本，兼容 Windows 客户端；NFS v4.0 协议为稍后期版本，支持文件锁等功能 在大量小文件或者大小文件混合场景下，用户在 容器 TKE 或者云服务器 CVM 等客户端使用 NFS v4.0 协议挂载 CFS 文件系统，可能在应用运行一段时候后出现：客户端负载居高不下，无限累加，业务读取数据慢或无响应，但是业务进程的 CPU 使用率并不是很高的情况。 二，优化 如果业务应用存在大量小文件的场景，或者并发操作文件数量巨大，推荐客户端使用 NFS v3.0 协议挂载。以下为规避大量小文件及大并发请求下客户端高负载的问题的方法 三，部署安装 方式一： NFS 4.0 挂载根目录 apiVersion: storage.k8s.io/v1 kind: StorageClass name: cfs parameters: pgroupid: pgroup-g6hwk27v storagetype: SD subnetid: subnet-ge8hhr3e vpcid: vpc-bkoyzvar zone: ap-chongqing-1 provisioner: com.tencent.cloud.csi.cfs reclaimPolicy: Delete volumeBindingMode: Immediate --- apiVersion: v1 kind: PersistentVolume metadata: name: cfs-pv-root spec: accessModes: - ReadWriteMany capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 192.168.1.6 path: / volumeHandle: cfs-pv-root persistentVolumeReclaimPolicy: Retain storageClassName: cfs volumeMode: Filesystem --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cfs-pvc-root namespace: storage spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: cfs volumeMode: Filesystem volumeName: cfs-pv-root --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos-cfs-root qcloud-app: centos-cfs-root name: centos-cfs-root namespace: storage spec: replicas: 1 selector: matchLabels: k8s-app: centos-cfs-root qcloud-app: centos-cfs-root template: metadata: labels: k8s-app: centos-cfs-root qcloud-app: centos-cfs-root spec: containers: - env: - name: PATH value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin image: ccr.ccs.tencentyun.com/v_cjweichen/centos:v3 imagePullPolicy: Always name: centos resources: limits: cpu: 100m memory: 64Mi requests: cpu: 100m memory: 64Mi securityContext: privileged: false volumeMounts: - mountPath: /mnt name: cfs-vol dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - name: cfs-vol persistentVolumeClaim: claimName: cfs-pvc-root 创建： [root@VM-0-17-tlinux ~]# kubectl apply -f centos-pvc-cfs.yaml persistentvolume/cfs-pv-root created persistentvolumeclaim/cfs-pvc-root created deployment.apps/centos-cfs-root created [root@VM-0-17-tlinux ~]# [root@VM-0-17-tlinux ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE cfs-pv-root 10Gi RWX Retain Bound storage/cfs-pvc-root cfs 14s [root@VM-0-17-tlinux ~]# kubectl get pvc -n storage NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cfs-pvc-root Bound cfs-pv-root 10Gi RWX cfs 29s [root@VM-0-17-tlinux ~]# kubectl get pod -n storage NAME READY STATUS RESTARTS AGE centos-cfs-root-6955f4656f-t4bhp 1/1 Running 0 35s 查看挂载情况 pod内： [root@centos-cfs-root-7775cc5fbf-jgff8 /]# nfsstat -m /mnt from 192.168.1.6:/ Flags: rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.0.17,local_lock=none,addr=192.168.1.6 [root@centos-cfs-root-7775cc5fbf-jgff8 /]# df -h Filesystem Size Used Avail Use% Mounted on overlay 50G 7.3G 40G 16% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup 192.168.1.6:/ 10G 32M 10G 1% /mnt node节点上对应的命令如下： [root@VM-0-17-tlinux ~]# df -h | grep cfs-pv-root 192.168.1.6:/ 10G 32M 10G 1% /var/lib/kubelet/pods/e7b28b08-1322-4f8c-aefc-f3892f29b652/volumes/kubernetes.io~csi/cfs-pv-root/mount 方式二 TKE POD 使用 NFS 3.0 挂载子目录 唯一不同点是 其中挂载参数要写在volumeAttributes： 示例： apiVersion: v1 kind: PersistentVolume metadata: name: cfs-pv-root spec: accessModes: - ReadWriteMany capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 192.168.1.6 path: /bbs2yahv ##这个目录 必须是CFS那边给的目录 自建目录不行 vers: \"3\" ##在这里声明NFSV3协议挂载，注意双引号 volumeHandle: cfs-pv-root persistentVolumeReclaimPolicy: Retain storageClassName: cfs volumeMode: Filesystem 总结来说 用v3挂载一定要加上fsid，在本文中对应为bbs2yahv，具体的还得参照你cfs中对应的fsid [root@centos-cfs-root-6955f4656f-2blh4 /]# nfsstat -m /mnt from 192.168.1.6://bbs2yahv Flags: rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,nolock,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=192.168.1.6,mountvers=3,mountport=892,mountproto=tcp,local_lock=all,addr=192.168.1.6 https://github.com/TencentCloud/kubernetes-csi-tencentcloud/blob/master/docs/README_CFS.md 四，权限 1，目录权限 cfs挂载目录权限 是由CFS那边权限组设置 使用说明： all_squash：所有访问用户都会被映射为匿名用户或用户组； no_all_squash：访问用户会先与本机用户匹配，匹配失败后再映射为匿名用户或用户组； root_squash：将来访的root用户映射为匿名用户或用户组； no_root_squash：来访的root用户保持root帐号权限 （默认是这个） 使用：no_root_squash这个权限 普通用户没有权限写 只有读的权限 [chen@centos-cfs-root-6955f4656f-pvnxs mnt]$ cat 1111.txt hello.txt [chen@centos-cfs-root-6955f4656f-pvnxs mnt]$ ls -lrt total 4 drwxr-xr-x 2 root root 6 Jul 17 16:06 1111 -rw-r--r-- 1 root root 10 Jul 17 16:08 1111.txt [chen@centos-cfs-root-6955f4656f-pvnxs mnt]$ echo 1111 >> 1111.txt -bash: 1111.txt: Permission denied [chen@centos-cfs-root-6955f4656f-pvnxs mnt]$ mkdir data mkdir: cannot create directory 'data': Permission denied [chen@centos-cfs-root-6955f4656f-pvnxs mnt]$ cd 1111 [chen@centos-cfs-root-6955f4656f-pvnxs 1111]$ ls [chen@centos-cfs-root-6955f4656f-pvnxs 1111]$ mkdr 111 五，部署mysql使用CFS存储 前置条件 1，创建CFS实例 2，权限组 来访地址： 用户权限是：no_root_squash *读写权限：读写 /var/lib/mysql 目录下的文件属性 都是mysql root@mysql-cfs-data-66c5dc7875-fnm22:/var/lib/mysql# pwd /var/lib/mysql root@mysql-cfs-data-66c5dc7875-fnm22:/var/lib/mysql# ls -lrt total 198064 -rw-r----- 1 mysql mysql 50331648 Jul 17 08:22 ib_logfile1 -rw-r----- 1 mysql mysql 8585216 Jul 17 08:22 '#ib_16384_1.dblwr' drwxr-x--- 2 mysql mysql 4096 Jul 17 08:23 performance_schema root@mysql-cfs-data-66c5dc7875-fnm22:/var/lib/mysql# id mysql uid=999(mysql) gid=999(mysql) groups=999(mysql） 需要将文件系统的权限组修改成 no_all_squash 访问用户会先与本机用户匹配，匹配失败后再映射为匿名用户或用户组； yaml文件： apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: mysql-cfs-data qcloud-app: mysql-cfs-data name: mysql-cfs-data namespace: storage spec: replicas: 1 selector: matchLabels: k8s-app: mysql-cfs-data qcloud-app: mysql-cfs-data strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: mysql-cfs-data qcloud-app: mysql-cfs-data spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"123456\" image: mysql:5.7.16 imagePullPolicy: IfNotPresent name: mysql resources: limits: cpu: 500m memory: 1Gi requests: cpu: 100m memory: 64Mi securityContext: privileged: false volumeMounts: - mountPath: /var/lib/mysql name: mysql-vol dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - name: mysql-vol persistentVolumeClaim: claimName: cfs-pvc-root --- ##内网型CLB apiVersion: v1 kind: Service metadata: annotations: service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-ge8hhr3e name: mysql-cfs-data namespace: storage spec: externalTrafficPolicy: Cluster ports: - name: 3306-3306-tcp nodePort: 31761 port: 3306 protocol: TCP targetPort: 3306 selector: k8s-app: mysql-cfs-data qcloud-app: mysql-cfs-data sessionAffinity: None type: LoadBalancer 查看部署结果 root@mysql-cfs-data-b58fc4767-7lzqp:/var/lib/mysql# df -h | grep /var 192.168.1.6:/111 10G 241M 9.8G 3% /var/lib/mysql root@mysql-cfs-data-b58fc4767-7lzqp:/var/lib/mysql# ls -lt total 188452 drwxr-x--- 2 mysql mysql 19 Jul 17 09:28 chen -rw-r----- 1 mysql mysql 12582912 Jul 17 09:26 ibtmp1 -rw-r----- 1 mysql mysql 50331648 Jul 17 09:25 ib_logfile0 -rw-r----- 1 mysql mysql 79691776 Jul 17 09:25 ibdata1 -rw-r----- 1 mysql mysql 1325 Jul 17 09:25 ib_buffer_pool drwxr-x--- 2 mysql mysql 8192 Jul 17 09:25 sys drwxr-x--- 2 mysql mysql 4096 Jul 17 09:25 mysql drwxr-x--- 2 mysql mysql 8192 Jul 17 09:25 performance_schema -rw-r----- 1 mysql mysql 56 Jul 17 09:25 auto.cnf -rw-r----- 1 mysql mysql 50331648 Jul 17 09:25 ib_logfile1 StatefulSet方式部署： apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: mysql qcloud-app: mysql name: mysql namespace: cjweichen spec: replicas: 1 selector: matchLabels: k8s-app: mysql qcloud-app: mysql serviceName: \"\" template: metadata: labels: k8s-app: mysql qcloud-app: mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"123456\" image: mysql:5.7 imagePullPolicy: IfNotPresent name: mysql resources: {} volumeMounts: - mountPath: /var/lib/mysql name: nfs subPath: mysql_docker/data dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey volumes: - name: nfs nfs: path: / server: 172.16.3.7 六，弹性集群EKS使用 NFS 3.0 挂载子目录 目前控制台没有CFS相关存储组件，可以创建静态CFS pv ,然后静态PVC绑定，以下是nfs 3.0协议挂载 apiVersion: v1 kind: PersistentVolume metadata: name: cfs-pv spec: mountOptions: - vers=3 - nolock - proto=tcp - noresvport accessModes: - ReadWriteMany capacity: storage: 10Gi nfs: server: 10.2.2.66 ########cfs地址，需确保网络能通 path: /vshm4707 ########cfs侧拿到的路径 persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cfs-pvc namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: \"\" volumeMode: Filesystem volumeName: cfs-pv --- apiVersion: apps/v1beta2 kind: Deployment metadata: labels: k8s-app: centos-cfs qcloud-app: centos-cfs name: centos-cfs namespace: default spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: centos-cfs qcloud-app: centos-cfs strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: annotations: eks.tke.cloud.tencent.com/security-group-id: sg-7ad888n4 labels: k8s-app: centos-cfs qcloud-app: centos-cfs spec: containers: - env: - name: PATH value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin image: ccr.ccs.tencentyun.com/v_cjweichen/centos:latest imagePullPolicy: Always name: centos resources: limits: cpu: 500m memory: 1Gi requests: cpu: 100m memory: 64Mi volumeMounts: - mountPath: /mnt name: cfs-vol dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey volumes: - name: cfs-vol persistentVolumeClaim: claimName: cfs-pvc 参考官方文档 https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/ 七，跨云联网挂载CFS 目前TKE使用PVC和PV创建CFS，只能选择当前VPC下的CFS实例。有时用户有诉求，想挂载其他VPC下的CFS实例可以参考如下: apiVersion: v1 kind: PersistentVolume metadata: name: pv-ccn spec: accessModes: - ReadWriteMany capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 10.2.2.66 path: / volumeHandle: cfs-pv2 persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-operationdata namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: \"\" volumeMode: Filesystem volumeName: pv-operationdata [root@tke-node /data]# kubectl exec -it nginx-d98d55878-445cv bash root@nginx-d98d55878-445cv:/# df -h Filesystem Size Used Avail Use% Mounted on overlay 50G 7.8G 39G 17% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup 10.2.2.66:/ 10G 32M 10G 1% /mnt /dev/vda1 50G 9.6G 38G 21% /etc/hosts /dev/vdb 50G 7.8G 39G 17% /etc/hostname shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 12K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware root@nginx-d98d55878-445cv:/# cd /mnt/ root@nginx-d98d55878-445cv:/mnt# ls 123 aa bb data etc vshm4707 八，CFS V3挂载示例 小文件及高并发场景下客户端使用卡顿是已知问题 https://cloud.tencent.com/document/product/582/46359 apiVersion: v1 kind: PersistentVolume metadata: name: cfs-pv spec: mountOptions: - vers=3 - nolock - proto=tcp - noresvport accessModes: - ReadWriteMany capacity: storage: 10Gi nfs: server: 10.0.0.11 path: /mu3qj5m4 persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cfs-pvc namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: \"\" volumeMode: Filesystem volumeName: cfs-pv 九，CFS 动态创建不同子目录的 PVC 参考：https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner https://cloud.tencent.com/developer/article/1839599 使用场景 目前TKE 使用 StorageClass 自动创建 CFS 类型 PVC 和 PV，每个 PV 都需要对应一个文件系统（CFS 实例），如果想要多个 PV（不同子路径） 使用同一个文件系统，就需要手动创建 PV 时指定 CFS 文件系统的具体的路径然后绑定 PVC 使用，这是一种办法，但是当需要的 PV 数量多了就会非常繁琐， 对于此使用场景我们可以使用社区的 nfs-client-provisioner 项目来实现动态创建 CFS 文件系统中的子路径，接下来我们来介绍下如何在 TKE 中使用nfs-client-provisioner 操作步骤 1，准备CFS实例 使用cfs-csi插件 创建或者通过cfs 控制台创建，更或者是自建NFS文件系统实例 2，安装部署 官方提供两种安装方式， helm 安装 和 手动部署 YAML 安装，这里我们提供两种的安装方式, 3，helm安装（helm客户端安装可以参考另外文档）安装 helm3 参考 helm 安装 1，可以根据需求修改指定参数后部署： $ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ # helm repo list #查看添加helm repo仓库情况 NAME URL tcr-chen-helm https://tcr-chen.tencentcloudcr.com/chartrepo/helm nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ # 【可选步骤，可以将helm chart 包下载下来 上传到自己的镜像仓库，方便后续其他集群安装】 # 下载 helm chart 文件至本地目录，查看可以指定的 values 选项（可选） $ helm pull nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --untar $ tar zcvf nfs-subdir-external-provisioner-1.0.0.tgz nfs-subdir-external-provisioner/ $ helm cm-push nfs-subdir-external-provisioner-1.0.0.tgz tcr-chen-helm # 默认镜像是国外镜像，可以下载下来上传到自己的镜像仓库里面 $ docker pull k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 $ docker tag 932b0bface75 tcr-chen.tencentcloudcr.com/docker/nfs-subdir-external-provisioner:v4.0.2 $ docker push tcr-chen.tencentcloudcr.com/docker/nfs-subdir-external-provisioner:v4.0.2 # 使用类似下面命令安装 nfs-subdir-external-provisioner 资源 # helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=10.2.2.45 \\ --set nfs.path=/tke_path \\ --set image.repository=tcr-chen.tencentcloudcr.com/docker/nfs-subdir-external-provisioner \\ --set image.tag=v4.0.2 #出现如下提示 表示安装成功 NAME: nfs-subdir-external-provisioner LAST DEPLOYED: Tue Feb 15 13:22:03 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: Non 2，配置使用 CFS 文件系统子目录的 PVC 。 使用上一步部署的nfs-subdir-external-provisioner动态创建存储卷。 部署后会默认生成一个存储类资源，默认存储类名是\"nfs-client\"(也可以在部署时自定义指定)，如下： # kubectl get sc | grep nfs-client nfs-client cluster.local/nfs-subdir-external-provisioner 4m8s 3，使用上面的StorageClass创建PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 1Mi # kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-claim Bound pvc-c0ee60d2-a0b8-4902-a4c1-5145f78e5606 1Mi RWX nfs-client 11s 会自动创建命令，命令命名方式是 namespace-pvc名称-pv名称 4，创建测试POD 挂载对应PVC kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: busybox:stable command: - \"/bin/sh\" args: - \"-c\" - \"touch /mnt/SUCCESS && exit 0 || exit 1\" volumeMounts: - name: nfs-pvc mountPath: \"/mnt\" restartPolicy: \"Never\" volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim # pwd /localfolder/tke_path/default-test-claim-pvc-c0ee60d2-a0b8-4902-a4c1-5145f78e5606 # ls -lrt total 0 -rw-r--r-- 1 root root 0 Feb 15 13:47 SUCCESS 4，手动部署 YAML 安装 1，创建账号ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 2，部署应用 apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: tcr-chen.tencentcloudcr.com/docker/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: cluster.local/nfs-subdir-external-provisioner - name: NFS_SERVER value: 10.2.2.45 - name: NFS_PATH value: /tke_path volumes: - name: nfs-client-root nfs: server: 10.2.2.45 path: /tke_path 3，创建storageClass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client provisioner: cluster.local/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" 4，创建PVC和POD验证 十，使用自建NFS挂载 TKE使用自建NFS挂载POD NFS基本配置： [root@VM-55-9-tlinux ~]# showmount -e 172.16.55.9 Export list for 172.16.55.9: /data/saas 172.16.0.0/16,10.0.0.0/8 [root@VM-55-9-tlinux ~]# cat /etc/exports /data/saas 10.0.0.0/8(rw,async,no_root_squash) /data/saas 172.16.0.0/16(rw,async,no_root_squash) [root@VM-55-9-tlinux ~]# ls -l /data/saas/ total 4 drwxr-xr-x 3 root root 4096 Apr 19 18:42 mysql_docker [root@VM-55-9-tlinux /data/saas/mysql_docker/data]# ls -lrt total 0 部署yaml文件 apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: mysql-nfs qcloud-app: mysql-nfs name: mysql-nfs namespace: cjweichen spec: replicas: 1 selector: matchLabels: k8s-app: mysql-nfs qcloud-app: mysql-nfs serviceName: \"\" template: metadata: labels: k8s-app: mysql-nfs qcloud-app: mysql-nfs spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"12345\" image: mysql:5.7 imagePullPolicy: IfNotPresent name: mysql resources: {} volumeMounts: - mountPath: /var/lib/mysql name: nfs-vol subPath: mysql_docker/data dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey volumes: - name: nfs-vol nfs: path: /data/saas server: 172.16.55.9 对应目录下也有相应文件 EKS使用自建NFS部署mysql 相同nfs客户端 apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: mysql-nfs-eks qcloud-app: mysql-nfs-eks name: mysql-nfs-eks namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: mysql-nfs-eks qcloud-app: mysql-nfs-eks serviceName: \"\" template: metadata: labels: k8s-app: mysql-nfs-eks qcloud-app: mysql-nfs-eks spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"12345\" image: mysql:5.7 imagePullPolicy: IfNotPresent name: mysql resources: {} volumeMounts: - mountPath: /var/lib/mysql name: nfs-vol subPath: mysql_docker/data dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey volumes: - name: nfs-vol nfs: path: /data/saas server: 172.16.55.9 十一CFS类型PVC删除策略 1，StorageClass 回收策略是reclaimPolicy: Delete apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: cfs parameters: pgroupid: pgroup-jsgxkfjl storagetype: SD subnetid: subnet-qtlzyxzt vers: \"3\" vpcid: vpc-a19fq3f0 zone: ap-shanghai-2 provisioner: com.tencent.cloud.csi.cfs reclaimPolicy: Delete volumeBindingMode: Immediate 2，使用这个StorageClass 自动创建CFS类型pvc apiVersion: v1 kind: PersistentVolumeClaim metadata: name: v-cjwiechen-cfs namespace: cjweichen spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: cfs volumeMode: Filesystem 会自动创建pv和一个CFS实例 3，创建POD并且挂载，这个时候去删除对应的pvc，会提示该pvc上时候操作的删除，待POD删除时候才删除pvc 如果StorageClass 回收策略是reclaimPolicy: Delete，会自动删除pv 和PV对应的 cfs实例的（测试验证过） 4，创建成功收，手动去修改PV的保留策略（控制台方式修改，编辑yaml文件） 4.1，命令方式修改 选择你的 PersistentVolumes 中的一个并更改它的回收策略： 这里的 `` 是你选择的 PersistentVolume 的名字 kubectl patch pv -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}' 4.2，验证你选择的 PersistentVolume 拥有正确的策略： 如上 5，然后去删除pvc，对应的pv不会删除，为Released状态 6，然后删除PV，看下cfs实例是否被删除，验证对应的cfs实例 不会被删除 "},"docs/tke/tke-nfs-v2.html":{"url":"docs/tke/tke-nfs-v2.html","title":"TKE集群使用自建NFS挂载","summary":"TKE集群使用自建NFS数据持久化挂载","keywords":"","body":"NFS是Network File System的缩写，即网络文件系统，是一种使用于分散式文件系统的协定。主要是通过网络让不同的机器、不同的操作系统能够彼此分享个别的数据，让应用程序在客户端通过网络访问位于服务器磁盘中的数据，是在类Unix系统间实现磁盘文件共享的一种方法。这个NFS服务器可以让你的PC来将网络远程的NFS服务器分享的目录，挂载到本地端的机器当中， 在本地端的机器看起来，那个远程主机的目录就好像是自己的一个磁盘分区槽一样。 更多NFS相关原理网上有很多，这里就不做多介绍，这里我们主要说下NFS安装和腾讯云TKE里面如何使用自建NFS进行POD的持久化存储 NFS 服务搭建 环境准备 CentOS7以NFSv4作为默认版本，NFSv4使用TCP协议（端口号是2049）和NFS服务器建立连接。 # 系统环境 系统平台：CentOS Linux release 7.9 (Final) NFS Server IP：192.168.0.17 防火墙已关闭/iptables: Firewall is not running. SELINUX=disabled 安装 NFS 服务 NFS服务端 服务端，程序包名nfs-utils、rpcbind，默认都已经安装了 可以通过rpm -ql nfs-utils查看帮助文档等信息 NFS客户端 客户端，需要安装程序包名nfs-utils，提供基本的客户端命令工具 [root@VM-0-17-tlinux ~/nfs]# yum install nfs-utils -y 查看NFS服务端口 NFS启动时会随机启动多个端口并向RPC注册，为了方便配置防火墙，需要固定NFS服务端口。 这样如果使用iptables对NFS端口进行限制就会有点麻烦，可以更改配置文件固定NFS服务相关端口 分配端口，编辑配置文件/etc/sysconfig/nfs # 使用rpcinfo -P会发现rpc启动了很多监听端口 [root@VM-0-17-tlinux ~/nfs]# vim /etc/sysconfig/nfs RQUOTAD_PORT=30001 LOCKD_TCPPORT=30002 LOCKD_UDPPORT=30002 MOUNTD_PORT=30003 STATD_PORT=30004 启动NFS服务： [root@VM-0-17-tlinux ~/nfs]# systemctl start nfs.service NFS配置 相关文件和命令 文件名 说明 /etc/exports NFS 服务器端需要设定的内容，其作用是设定谁拥有什么样的权限去访问此机器的哪个目录 /var/lib/nfs/etab 无需设定，用于纪录 NFS 服务器完整的权限设定，exports 中没有设定的缺省值也会被列出 /var/lib/nfs/xtab 纪录 NFS 连接的相关信息 /usr/sbin/exportfs NFS 设定管理命令，用于 Server 侧设定，通过此条命令使得 exports 的设定变得有效或者无效 /usr/sbin/showmount 用于显示 NFS 设定的相关信息，Server 端和 Client 端均可 配置文件/etc/exports 我们可以按照“共享目录的路径 允许访问的 NFS 客户端(共享权限参数)”的格式，定义要共享的目录与相应的权限 常用参数 作用 ro 只读 rw 读写 sync 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据 root_squash 当 NFS 客户端以 root 管理员访问时，映射为 NFS 服务器的匿名用户 all_squash 无论 NFS 客户端使用什么账户访问，均映射为 NFS 服务器的匿名用户 no_root_squash 当 NFS 客户端以 root 管理员访问时，映射为 NFS 服务器的 root 管理员 secure 这个选项是缺省选项，它使用了 1024 以下的 TCP/IP 端口实现 NFS 的连接 insecure 禁止使用了 1024 以下的 TCP/IP 端口实现 NFS 的连接 no_wdelay 这个选项关闭写延时，如果设置了 async，那么 NFS 就会忽略这个选项 nohide 如果将一个目录挂载到另外一个目录之上，那么原来的目录通常就被隐藏起来或看起来像空的一样。要禁用这种行为，需启用 hide 选项 no_subtree_check 这个选项关闭子树检查，子树检查会执行一些不想忽略的安全性检查，缺省选项是启用子树检查 no_auth_nlm 这个选项也可以作为insecure_locks指定，它告诉 NFS 守护进程不要对加锁请求进行认证。如果关心安全性问题，就要避免使用这个选项。缺省选项是auth_nlm或secure_locks。 mp(mountpoint=path) 通过显式地声明这个选项，NFS 要求挂载所导出的目录 fsid=num 这个选项通常都在 NFS 故障恢复的情况中使用，如果希望实现 NFS 的故障恢复，请参考 NFS 文 [root@VM-0-17-tlinux ~/nfs]# cat /etc/exports /nfs 192.168.*.*(rw,sync,root_squash) #修改完配置后 重新启动nfs服务 [root@VM-0-17-tlinux ~/nfs]# systemctl restart nfs [root@VM-0-17-tlinux ~/nfs]# systemctl status nfs TKE使用NFS持久化挂载 将NFS挂在到K8S容器服务的POD里面 示例一 inline方式挂载 完整yaml： apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt #POD里面的挂载路径 name: nfs volumes: - name: nfs nfs: path: /nfs #nfs文件里面目录 server: 192.168.0.17 #NFS服务器IP 登陆NFS服务 创建个临时文件，然后登陆POD里查看 [root@VM-0-17-tlinux ~/nfs]# cd /nfs/ [root@VM-0-17-tlinux /nfs]# echo hello worload > hello.txt [root@VM-0-17-tlinux /nfs]# cat hello.txt hello worload #登录容器查看挂着情况 [root@VM-0-17-tlinux /nfs]# kubectl exec -it centos-54db87ccc9-nkx86 -- /bin/bash [root@centos-54db87ccc9-nkx86 /]# [root@centos-54db87ccc9-nkx86 /]# df -h Filesystem Size Used Avail Use% Mounted on overlay 50G 11G 37G 23% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup 192.168.0.17:/nfs 50G 32G 16G 67% /mnt #挂载点，可以看到已经挂载成功的 /dev/vda1 50G 11G 37G 23% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 12K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware [root@centos-54db87ccc9-nkx86 /]# cd /mnt/ [root@centos-54db87ccc9-nkx86 mnt]# cat hello.txt hello worload [root@centos-54db87ccc9-nkx86 mnt]# echo 1111>> hello.txt #在POD往文件系统里面写文件 bash: hello.txt: Permission denied #没有权限 [root@centos-54db87ccc9-nkx86 mnt]ls -lrt total 4 -rw-r--r-- 1 root root 14 Oct 17 13:14 hello.txt 可以看到 上面我们的 NFS配置的是root_squash /etc/exports /nfs 192.168.*.*(rw,sync,root_squash) #是没有权限修改文件 下面我们将配置文件修改成如下配置进行测试 [root@VM-0-17-tlinux /nfs]# cat /etc/exports /nfs 192.168.*.*(rw,sync,no_root_squash) #重启NFS 服务 [root@VM-0-17-tlinux /nfs]# systemctl restart nfs [root@VM-0-17-tlinux /nfs]# systemctl status nfs 将POD销毁重建后登录POD里测试。可以修改文件 并且可以创建文件 [root@centos-54db87ccc9-rgnk8 mnt]# touch 222 [root@centos-54db87ccc9-rgnk8 mnt]# ls 222 hello.txt [root@centos-54db87ccc9-rgnk8 mnt]# echo \"1111\">> hello.txt [root@centos-54db87ccc9-rgnk8 mnt]# cat hello.txt hello worloada 1111 [root@centos-54db87ccc9-rgnk8 mnt]# ls -lrt total 4 -rw-r--r-- 1 root root 0 Oct 17 13:24 222 -rw-r--r-- 1 root root 20 Oct 17 13:28 hello.txt 示例二 PVC&PV方式挂载 使用PVC和PV方式创建并挂载，示例如下： --- apiVersion: v1 kind: PersistentVolume metadata: name: pv-operationdata spec: accessModes: - ReadWriteMany capacity: storage: 10Gi #针对NFS类文件存储，storage大小没有实际意义 csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 192.168.0.17 #替换成自己的NFS path: /nfs #替换成自己的目录 volumeHandle: pv-operationdata persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-operationdata namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: \"\" volumeMode: Filesystem volumeName: pv-operationdata --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt name: nfs volumes: - name: nfs persistentVolumeClaim: claimName: pvc-operationdata 示例三 CVM服务器端挂载 使用showmount命令查询NFS服务器的远程共享信息 showmount命令输出格式为“共享的目录名称 允许使用客户端地址” showmount`命令** 参数 作用 -e 显示 NFS 服务器的共享列表 -a 显示本机挂载的文件资源的情况 NFS 资源的情况 -v 显示版本号 exportfs命令 维护exports文件导出的文件系统表的专用工具，可以修改配置之后不重启NFS服务 export -ar：重新导出所有的文件系统 export -au：关闭导出的所有文件系统 export -u FS:：关闭指定的导出的文件系统 # 查看NFS服务器端共享的文件系统 # showmount -e NFSSERVER_IP [root@VM-0-17-tlinux ~/nfs]# showmount -e 192.168.0.17 Export list for 192.168.0.17: /nfs 192.168.*.* # NFS客户端创建一个挂载目录，挂载服务端NFS文件系统到本地 # mount -t nfs SERVER:/path/to/sharedfs /path/to/mount_point [root@VM-0-11-tlinux ~]# mkdir /nfsfile [root@VM-0-11-tlinux ~]# mount -t nfs 192.168.0.17:/nfs /nfsfile [root@VM-0-11-tlinux ~]# df -h | grep nfsfile 192.168.0.17:/nfs 50G 32G 16G 67% /nfsfile # 挂载成功后就应该能够顺利地看到在执行前面的操作时写入的文件内容了 [root@VM-0-11-tlinux ~]# cat /nfsfile/hello.txt hello worloada 1111 # 如果希望NFS文件共享服务能一直有效，则需要将其写入到fstab文件中 # SERVER:/PATH/TO/EXPORTED_FS /mount_point nfs defaults,_netdev 0 0 [root@VM-0-11-tlinux ~]# vim /etc/fstab 192.168.0.17:/nfs /nfsfile nfs defaults 0 0 示例四 TKE部署mysql挂载 以下示例使用的是另外搭建的NFS实例 部署mysql服务挂载自建NFS，这里是在NFS里面创建了子目录，通过子目录挂载到mysql的数据目录里面 apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: mysql qcloud-app: mysql serviceName: \"\" template: metadata: labels: k8s-app: mysql qcloud-app: mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"123456\" image: mysql:5.7 imagePullPolicy: IfNotPresent name: mysql resources: {} volumeMounts: - mountPath: /var/lib/mysql name: nfs subPath: mysql_docker/data #挂载的是NFS子目录 dnsPolicy: ClusterFirst volumes: - name: nfs nfs: path: /nfs server: 172.16.0.33 #NFS服务器 示例五 超级节点POD挂载 挂载自建的 nfs 时，事件可能会报 Operation not permitted 如果使用自建的 nfs 实现持久化存储时，连接时事件报 Operation not permitted。需要修改自建 nfs 的 /etc/exports 文件，添加 /(rw,insecure) 参数，示例如下： /nfs 172.16.*.*(rw,insecure) apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: cjweichen spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - eklet-subnet-myqjfu7t containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt name: nfs dnsPolicy: ClusterFirst restartPolicy: Always volumes: - name: nfs persistentVolumeClaim: claimName: pvc-operationdata mysql挂载自建NFS 使用inline方式 apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: mysql-nfs-eks qcloud-app: mysql-nfs-eks name: mysql-nfs-eks namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: mysql-nfs-eks qcloud-app: mysql-nfs-eks serviceName: \"\" template: metadata: labels: k8s-app: mysql-nfs-eks qcloud-app: mysql-nfs-eks spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"12345\" image: mysql:5.7 imagePullPolicy: IfNotPresent name: mysql resources: {} volumeMounts: - mountPath: /var/lib/mysql name: nfs-vol subPath: mysql_docker/data dnsPolicy: ClusterFirst volumes: - name: nfs-vol nfs: path: /nfs server: 172.16.0.33 TKE里面使用NFS介绍就到这里，下一篇介绍下如何为NFS 动态创建不同子目录的 PVC用于POD挂载 感谢！ "},"docs/tke/tke-cbs.html":{"url":"docs/tke/tke-cbs.html","title":"TKE集群使用CBS类型存储挂载","keywords":"","body":"CBS盘基本使用 由于CBS只支持单机读写，测试如果多个POD使用一个CBS盘调度到不同节点和调度到同一个节点场景 案例一：多POD挂载单个CBS PV 1，先创建一个CBS类型的PVC和PV apiVersion: v1 kind: PersistentVolumeClaim metadata: name: multpod-onenode namespace: storage spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: cbs-snapclass volumeMode: Filesystem 2，部署工作负载，调度在同一个节点上 3，分别登录POD 到挂载点里面创建文件 4，查看CBS在节点上的映射关系 [root@VM-2-36-tlinux ~]# df -h | grep /dev devtmpfs 3.9G 0 3.9G 0% /dev tmpfs 3.9G 24K 3.9G 1% /dev/shm /dev/vda1 50G 11G 37G 24% / /dev/vdb 30G 57M 30G 1% /var/lib/kubelet/plugins/kubernetes.io/qcloud-cbs/mounts/disk-632p551w /dev/vdc 9.8G 37M 9.7G 1% /var/lib/kubelet/pods/35ad6d05-5220-4a6b-a687-0dfdee0dc742/volumes/kubernetes.io~csi/pvc-384c96ec-91f2-4e49-8918-c1c53ebbb3b2/mount 5，封锁当前节点，销毁重建一个POD，让其调度到其他节点上 kubectl cordon 192.168.2.36 kubectl delete pod multpod-onenode-679c6bcc84-rxcgp 这个时候 就会有一个POD一直是Pending 状态，无法调度 案例二：CBS类型PVC删除策略 StorageClass 回收策略是reclaimPolicy: Delete 1，静态创建PV， 不指定StorageClass apiVersion: v1 kind: PersistentVolume metadata: name: cbs-pv spec: accessModes: - ReadWriteOnce capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cbs volumeHandle: disk-h2v5jzxy persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem 创建PVC时候，也需要不指定 StorageClass 才能选择到这个pv，如果使用指定了会无法选择这个pv（提示是： 当前PersistentVolume与PersistentVolumeClaim所指定的StorageClass不一致 ） apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cbs-pvc namespace: default spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: \"\" volumeMode: Filesystem volumeName: cbs-pv 删除PVC和PV时候，对应的CBS盘不会回收删除 2，静态创建PV， 指定StorageClass apiVersion: v1 kind: PersistentVolume metadata: name: cbs-pv spec: accessModes: - ReadWriteOnce capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cbs volumeHandle: disk-h2v5jzxy persistentVolumeReclaimPolicy: Retain storageClassName: cbs volumeMode: Filesystem 创建PVC，需要选择对应的storageclass apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cbs-pvc namespace: default spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: cbs volumeMode: Filesystem volumeName: cbs-pv 模拟创建一定大小文件 ： dd if=/dev/zero of=hello.txt bs=100M count=1 删除PVC和PV时候，对应的CBS盘不会回收删除 3，动态创建pvc ，不指定pv，会自动创建pv apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cbs-pvc namespace: default spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: cbs 注意：由于cbs 回收策略 是 Delete ，所以删除pvc时候，对应的PV和CBS盘也会自动删除 StorageClass 回收策略是reclaimPolicy: Retain 4，动态创建PVC，保留策略 1）创建reclaimPolicy: Retain 类型的storgeclass 和PVC apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: cbs-retain parameters: diskChargeType: POSTPAID_BY_HOUR diskType: CLOUD_PREMIUM provisioner: com.tencent.cloud.csi.cbs reclaimPolicy: Retain volumeBindingMode: Immediate allowVolumeExpansion: true --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cbs-pvc namespace: default spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: cbs-retain volumeMode: Filesystem 2）然后删除对应的PVC，而对应的PV并没有删除，PV状态是 Released [root@VM-249-33-tlinux ~]# kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-ae7a89e4-3a3c-4f87-b0c9-fffaf91559d2 10Gi RWO Retain Released default/cbs-pvc cbs-retain 2m47s 3）单独删除PV时候，对应的CBS盘也是不会删除的 [root@VM-249-33-tlinux ~]# kubectl delete pv pvc-ae7a89e4-3a3c-4f87-b0c9-fffaf91559d2 persistentvolume \"pvc-ae7a89e4-3a3c-4f87-b0c9-fffaf91559d2\" deleted 4）修改对应的PV配置，删除spec.claimRef部分，对应PV可以和其他pvc绑定 案例二：CBS类型PV挂载目录权限问题 方案一： 先把pvc 先挂载到一个非业务使用的目录下，手动chmod -R 修改目录权限为777，然后再 挂载到业务使用的目录 如下， volumeMounts: - mountPath: /mnt/data name: vol 然后再将这块数据盘挂载到应用目录，查看目录权限是777 volumeMounts: - mountPath: /var/lib/grafana/ name: vol 方案二：使用init 容器修改对应目录权限 业务容器可以使用普通用户启动服务，如果在启动过程中遇到您的普通用户对某些目录没有操作权限，这个时候您可以使用Init 容器去修改你对应目录的权限，达到普通用户能够读写，例如如下方式（示例，具体根据业务进行修改） init 容器 使用可以参考：https://kubernetes.io/zh/docs/concepts/workloads/pods/init-containers/ "},"docs/tke/tke-cbs-v2.html":{"url":"docs/tke/tke-cbs-v2.html","title":"TKE如何扩容CBS类型的PVC","keywords":"","body":"概述 TKE 中一般使用 PVC 来声明存储容量和类型，自动绑定 PV 并挂载到 Pod，通常都使用 CBS (云硬盘) 存储。当 CBS 的磁盘容量不够用了，如何进行扩容呢？分两种情况，本文会详细介绍。 存储插件类型 CBS 存储插件在 TKE 中存在两种形式: In-Tree: Kubernetes 早期只支持以 In-Tree 的方式扩展存储插件，也就是将插件的逻辑编译进 Kubernetes 的组件中，也是 TKE 集群 1.20 版本之前默认自带的存储插件。 CSI: Kubernetes 社区发展过程中，引入存储扩展卷的 API，将存储插件实现逻辑从 Kubernetes 代码库中剥离出去，各个存储插件的实现单独维护和部署，无需侵入 Kubernetes 自身组件，也是社区现在推荐的存储扩展方式。TKE 在 1.20 版本之前，如果要使用 CSI 插件，可以在扩展组件中安装 CBS CSI 插件；自 1.20 版本开始，默认安装 CBS CSI 插件，将 In-Tree 插件完全下 可以检查 PVC 对应 StorageClass 的 yaml，如果 provisioner 是 cloud.tencent.com/qcloud-cbs，说明是 In-tree，如果是 com.tencent.cloud.csi.cbs 就是 CSI。 In-Tree 插件扩容 PVC 如何符合以下两种情况，说明你的 CBS PVC 用的 In-Tree 插件: 如果你的集群版本低于 1.20，并且没有安装 CSI 插件 (默认没有安装)，那么你使用的 CBS 类型 PVC 一定用的 In-Tree 插件； 如果安装了 CSI 插件，但创建的 PVC 引用的 StorageClass 并没有使用 CSI (如下图)。 对 In-Tree 插件的 PVC 进行扩容需要手动操作，比较麻烦，操作步骤如下: 获取 pvc 所绑定的 pv: $ kubectl -n storage get pvc centos-cbs-pvc -o jsonpath='{.spec.volumeName}' pvc-42037497-d144-45b8-93fb-6143aa5eb282 获取 pv 对应的 cbs id: $ kubectl get pv -o jsonpath=\"{.spec.qcloudCbs.cbsDiskId}\" pvc-42037497-d144-45b8-93fb-6143aa5eb282 disk-2iil4p98 在云硬盘控制台 找到对应云盘，进⾏扩容操作: 4.登录 CBS 挂载的节点 (pod 所在节点)，找到这块 cbs 盘对应的设备路径: [root@VM-0-17-tlinux ~]# ls -l /dev/disk/by-id/*disk-2iil4p98* lrwxrwxrwx 1 root root 9 Sep 2 14:07 /dev/disk/by-id/virtio-disk-2iil4p98 -> ../../vde 5.执⾏命令扩容⽂件系统(替换 cbs 设备路径): # 对于 ext4 ⽂件系统(通常是这种) resize2fs /dev/vdc # 对于 xfs ⽂件系统 xfs_growfs /dev/vdc FAQ 不需要改 PVC 或 PV 吗？ 不需要，PVC 和 PV 的容量显示也还是会显示扩容之前的⼤⼩，但实际⼤⼩是扩容后的。 CSI 插件扩容 PVC 如果 TKE 集群版本在 1.20 及其以上版本，一定是用的 CSI 插件；如果低于 1.20，安装了 CBS CSI 扩展组件，且 PVC 引用的 StorageClass 是 CBS CSI 类型的，开启了在线扩容能力，那么就可以直接修改 PVC 容量实现自动扩容 PV 的容量。 所以 CBS CSI 插件扩容 PVC 过于简单，只有修改 PVC 容量一个步骤，这里就先讲下如何确保 PVC 能够在线扩容。 如果用控制台创建 StorageClass ，确保勾选 【启用在线扩容】（默认就会勾选）: 如果使用 YAML 创建，确保将 allowVolumeExpansion 设为 true: allowVolumeExpansion: true # 这里是关键 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: cbs-csi-expand parameters: diskType: CLOUD_PREMIUM provisioner: com.tencent.cloud.csi.cbs reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer 创建 PVC 时记得选择 CBS CSI 类型且开启了在线扩容的 StorageClass: 然后当需要扩容 PVC 的时候，直接修改 PVC 的容量即可： 修改完后对应的 CBS 磁盘容量会自动扩容到指定大小 (注意必须是 10Gi 的倍数)，可以自行到云硬盘控制台确认。 FAQ 需要重启 Pod 吗? 可以不重启 pod 直接扩容，但，这种情况下被扩容的云盘的文件系统被 mount 在节点上，如果有频繁 I/O 的话，有可能会出现文件系统扩容错误。为了确保文件系统的稳定性，还是推荐先让云盘文件系统处于未 mount 情况下进行扩容，可以将 Pod 副本调为 0 或修改 PV 打上非法的 zone (kubectl label pv pvc-xxx failure-domain.beta.kubernetes.io/zone=nozone) 让 Pod 重建后 Pending，然后再修改 PVC 容量进行在线扩容，最后再恢复 Pod Running 以挂载扩容后的磁盘。 担心扩容导致数据出问题，如何兜底? 可以在扩容前使用快照来备份数据，避免扩容失败导致数据丢失。 本文摘自：https://imroc.cc/tke/faq/pv-expansion/ "},"docs/tke/persistentvolume-clarim.html":{"url":"docs/tke/persistentvolume-clarim.html","title":"更改PersistentVolume的回收策略","keywords":"","body":"更改 PersistentVolume 的回收策略 本文展示了如何更改 Kubernetes PersistentVolume 的回收策略 为什么要更改 PersistentVolume 的回收策略 ​ PersistentVolumes 可以有多种回收策略，包括 \"Retain\"、\"Recycle\" 和 \"Delete\"。 对于动态配置的 PersistentVolumes 来说，默认回收策略为 \"Delete\"。 这表示当用户删除对应的 PersistentVolumeClaim 时，动态配置的 volume 将被自动删除。 如果 volume 包含重要数据时，这种自动行为可能是不合适的。 那种情况下，更适合使用 \"Retain\" 策略。 使用 \"Retain\" 时，如果用户删除 PersistentVolumeClaim，对应的 PersistentVolume 不会被删除。 相反，它将变为 Released 状态，表示所有的数据可以被手动恢复 更改 PersistentVolume 的回收策略 1，列出你集群中的 PersistentVolumes kubectl get pv 这个列表同样包含了绑定到每个卷的 claims 名称，以便更容易的识别动态配置的卷。 2，选择你的 PersistentVolumes 中的一个并更改它的回收策略： 这里的 `` 是你选择的 PersistentVolume 的名字 kubectl patch pv -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}' 3，验证你选择的 PersistentVolume 拥有正确的策略： 如上 4，也可以通过edit方式修改 kubectl edit pv pvc-c1b6f3b9-34b6-4958-b3ef-963b3771686b 参考文档：https://kubernetes.io/zh/docs/tasks/administer-cluster/change-pv-reclaim-policy/ "},"docs/tke/default-storageclass.html":{"url":"docs/tke/default-storageclass.html","title":"改变默认StorageClass","keywords":"","body":"为什么要改变默认存储类？ 取决于安装模式，你的 Kubernetes 集群可能和一个被标记为默认的已有 StorageClass 一起部署。 这个默认的 StorageClass 以后将被用于动态的为没有特定存储类需求的 PersistentVolumeClaims 配置存储 预先安装的默认 StorageClass 可能不能很好的适应你期望的工作负载；例如，它配置的存储可能太过昂贵。 如果是这样的话，你可以改变默认 StorageClass，或者完全禁用它以防止动态配置存储 ， 删除默认 StorageClass 可能行不通，因为它可能会被你集群中的扩展管理器自动重建 改变默认 StorageClass 1， 列出你的集群中的 StorageClasses： kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cbs (default) cloud.tencent.com/qcloud-cbs Delete Immediate false 22d cbs-snapclass com.tencent.cloud.csi.cbs Delete Immediate true 18d cbs-zone com.tencent.cloud.csi.cbs Delete Immediate true 16d cfs com.tencent.cloud.csi.cfs Delete Immediate false 22d 默认 StorageClass 以 (default) 标记。 2，标记默认 StorageClass 非默认： 默认 StorageClass 的注解 storageclass.beta.kubernetes.io/is-default-class: 设置为 true。 注解的其它任意值或者缺省值将被解释为 false。 要标记一个 StorageClass 为非默认的，你需要改变它的值为 false： kubectl patch storageclass cbs -p '{\"metadata\": {\"annotations\":{\"storageclass.beta.kubernetes.io/is-default-class\":\"false\"}}}' 3， 标记一个 StorageClass 为默认的： kubectl patch storageclass cbs-snapclass -p '{\"metadata\": {\"annotations\":{\"storageclass.beta.kubernetes.io/is-default-class\": \"true\"}}}' 4，验证你选用的 StorageClass 为默认的： kubectl get storageclass 参考链接：https://kubernetes.io/zh/docs/tasks/administer-cluster/change-default-storage-class/ "},"docs/tke/delete-terminating-namespace.html":{"url":"docs/tke/delete-terminating-namespace.html","title":"删除Terminating状态的namespace","keywords":"","body":"1， 查看命名空间 发现istio-system 一直处于Terminating 状态。无法删除命名空间！！ [root@VM-2-45-tlinux ~]# kubectl get ns -o wide NAME STATUS AGE default Active 100d istio-system Terminating 64d kube-node-lease Active 100d kube-public Active 100d kube-system Active 100d prom-ktn7d2ot Active 84d 2，解决方法 1）查看istio-system的namespace描述 [root@VM-2-45-tlinux ~]# kubectl get ns istio-system -o json > istio-system.json 2）编辑json文件，删除spec字段的内存，因为k8s集群时需要认证的。 将finalizers 部分删除 \"spec\": { \"finalizers\": [ \"kubernetes\" ] }, \"spec\": { }, 3）（注意） 新开一个窗口运行kubectl proxy跑一个API代理在本地的8081端口 [root@VM-2-45-tlinux ~]# kubectl proxy --port=8081 Starting to serve on 127.0.0.1:8081 4） 最后运行curl命令进行删除 ： curl -k -H \"Content-Type:application/json\" -X PUT --data-binary @istio-system.json http://127.0.0.1:8081/api/v1/namespaces/istio-system/finalize 5）出现如下表示删除成功 [root@VM-2-45-tlinux ~]# curl -k -H \"Content-Type:application/json\" -X PUT --data-binary @istio-system.json http://127.0.0.1:8081/api/v1/namespaces/istio-system/finalize { \"kind\": \"Namespace\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"istio-system\", \"selfLink\": \"/api/v1/namespaces/istio-system/finalize\", \"uid\": \"59b8d678-be37-4dbf-952e-8c1051bd85af\", \"resourceVersion\": \"5817056800\", \"creationTimestamp\": \"2021-05-15T12:07:05Z\", \"deletionTimestamp\": \"2021-07-19T08:30:30Z\" }, \"spec\": { }, \"status\": { \"phase\": \"Terminating\", \"conditions\": [ { \"type\": \"NamespaceDeletionDiscoveryFailure\", \"status\": \"False\", \"lastTransitionTime\": \"2021-07-19T08:30:39Z\", \"reason\": \"ResourcesDiscovered\", \"message\": \"All resources successfully discovered\" }, { \"type\": \"NamespaceDeletionGroupVersionParsingFailure\", \"status\": \"False\", \"lastTransitionTime\": \"2021-07-19T08:30:39Z\", \"reason\": \"ParsedGroupVersions\", \"message\": \"All legacy kube types successfully parsed\" }, { \"type\": \"NamespaceDeletionContentFailure\", \"status\": \"False\", \"lastTransitionTime\": \"2021-07-19T08:30:39Z\", \"reason\": \"ContentDeleted\", \"message\": \"All content successfully deleted\" } ] } 本文参考链接： https://cloud.tencent.com/developer/article/1678604 https://www.cnblogs.com/zhangmingcheng/p/13987093.html https://support.huaweicloud.com/cce_faq/cce_faq_00277.html EKS集群删除Terminating状态的命名空间 参考文档：https://cloud.tencent.com/developer/article/1678604 1，开启集群内网或者外网访问 2，配置kubectl 的kubeconfig文件 vi /root/.kube/config 将控制台获取的KubeConfig文件复制到config文件内 执行kubectl get nodes 命令确认当前集群 3，删除Terminating状态的命名空间 [root@chen ~]# kubectl get ns prom-d5d7opjl -o json > prom-d5d7opjl.json [root@chen ~]# vi prom-d5d7opjl.json 编辑json文件，删除spec字段的内容 更改为： 新开一个窗口运行kubectl proxy跑一个API代理在本地的8081端口 [root@chen ~]# kubectl proxy --port=8081 最后运行curl命令进行删除： [root@chen ~]# curl -k -H \"Content-Type:application/json\" -X PUT --data-binary @prom-d5d7opjl.json http://127.0.0.1:8081/api/v1/namespaces/prom-d5d7opjl/finalize 最后确认是否删除： 命名空间因APIService对象访问失败无法删除 问题现象 删除命名空间时，命名空间一直处“删除中”状态，无法删除。查看命名空间yaml配置，status中有报错“DiscoveryFailed”，示例如下： 上图中报错信息为：Discovery failed for some groups, 1 failing: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request 表示当前删除命名空间动作阻塞在kube-apiserver访问metrics.k8s.io/v1beta1 接口的APIService资源对象。 问题根因 当集群中存在APIService对象时，删除命名空间会先访问APIService对象，若APIService资源无法正常访问，会阻塞命名空间删除。除用户创建的APIService对象资源外，K8S集群部分插件也会自动创建APIService资源，如metrics-server, prometheus插件。 说明：APIService使用介绍请参考：https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/ 解决方法 可以采用如下两种方法解决： 修复报错信息中的APIService对象，使其能够正常访问，如果是插件中的APIService，请确保插件的Pod正常运行 删除报错信息中的APIService对象，如果是插件中的APIService, 可从页面卸载该插件 "},"docs/tke/delete-terminating-pv.html":{"url":"docs/tke/delete-terminating-pv.html","title":"删除Terminating状态的pv","keywords":"","body":"删除Terminating状态的pv 正常删除步骤为：先删pod 再删pvc 最后删pv 平时在处理工作时候，会遇到“Terminating”状态的PV，而且delete不掉。如下图： 解决方案： 直接删除k8s中的记录： kubectl patch pv xxx -p '{\"metadata\":{\"finalizers\":null}}' 参考信息： This happens when persistent volume is protected. You should be able to cross verify this: Command: kubectl describe pvc PVC_NAME | grep Finalizers Output: Finalizers: [kubernetes.io/pvc-protection] You can fix this by setting finalizers to null using kubectl patch: kubectl patch pvc PVC_NAME -p '{\"metadata\":{\"finalizers\": []}}' --type=merge "},"docs/tke/tke-image.html":{"url":"docs/tke/tke-image.html","title":"TKE使用海外容器镜像","keywords":"","body":"背景 在 TKE 上部署开源应用时，经常会遇到依赖的镜像拉不下来或非常慢的问题，比如 gcr, quay.io 等境外公开镜像仓库。实际 TKE 已经提供了海外镜像加速的能力，本文介绍如何使用此项能力来部署开源应用。 镜像地址映射 以下是支持的镜像仓库及其映射地址: 海外镜像仓库地址 腾讯云映射地址 gcr.io gcr.tencentcloudcr.com k8s.gcr.io k8s.tencentcloudcr.com quay.io quay.tencentcloudcr.com nvcr.io nvcr.tencentcloudcr.com 修改镜像地址 在部署应用时，修改下镜像地址，将镜像仓库域名替换为腾讯云上的映射地址 (见上方表格)，比如将 quay.io/prometheus/node-exporter:v0.18.1 改为 quay.tencentcloudcr.com/prometheus/node-exporter:v0.18.1，这样拉取镜像时就会走到加速地址。 不想修改镜像地址 ? 如果镜像太多，嫌修改镜像地址太麻烦 (比如使用 helm 部署，用到了很多镜像)，可以利用 containerd 的 mirror 配置来实现无需修改镜像地址 (前提是容器运行时使用的 containerd )。 docker 仅支持 docker hub 的 mirror 配置，所以如果容器运行时是 docker 就必须修改镜像地址。 具体方法是修改 containerd 配置 (/etc/containerd/config.toml)，将腾讯云映射地址配到 mirrors 里: [plugins.cri.registry] [plugins.cri.registry.mirrors] [plugins.cri.registry.mirrors.\"quay.io\"] endpoint = [\"https://quay.tencentcloudcr.com\"] [plugins.cri.registry.mirrors.\"k8s.gcr.io\"] endpoint = [\"https://k8s.tencentcloudcr.com\"] [plugins.cri.registry.mirrors.\"gcr.io\"] endpoint = [\"https://gcr.tencentcloudcr.com\"] [plugins.cri.registry.mirrors.\"nvcr.io\"] endpoint = [\"https://nvcr.tencentcloudcr.com\"] [plugins.cri.registry.mirrors.\"docker.io\"] endpoint = [\"https://mirror.ccs.tencentyun.com\"] 不过每个节点都去手动修改过于麻烦，我们可以在添加节点或创建节点池时指定下自定义数据 (即初始化节点时会运行的自定义脚本) 来自动修改 containerd 配置: 将下面的脚本粘贴进去: sed -i '/\\[plugins\\.cri\\.registry\\.mirrors\\]/ a\\\\ \\ \\ \\ \\ \\ \\ \\ [plugins.cri.registry.mirrors.\"quay.io\"]\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ endpoint = [\"https://quay.tencentcloudcr.com\"]' /etc/containerd/config.toml sed -i '/\\[plugins\\.cri\\.registry\\.mirrors\\]/ a\\\\ \\ \\ \\ \\ \\ \\ \\ [plugins.cri.registry.mirrors.\"gcr.io\"]\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ endpoint = [\"https://gcr.tencentcloudcr.com\"]' /etc/containerd/config.toml sed -i '/\\[plugins\\.cri\\.registry\\.mirrors\\]/ a\\\\ \\ \\ \\ \\ \\ \\ \\ [plugins.cri.registry.mirrors.\"k8s.gcr.io\"]\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ endpoint = [\"https://k8s.tencentcloudcr.com\"]' /etc/containerd/config.toml sed -i '/\\[plugins\\.cri\\.registry\\.mirrors\\]/ a\\\\ \\ \\ \\ \\ \\ \\ \\ [plugins.cri.registry.mirrors.\"nvcr.io\"]\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ endpoint = [\"https://nvcr.tencentcloudcr.com\"]' /etc/containerd/config.toml systemctl restart containerd 推荐使用节点池，扩容节点时都会自动运行脚本，就不需要每次加节点都去配下自定义数据了。 本文摘自：https://imroc.cc/tke/trick/use-foreign-container-image/ "},"docs/tke/tke-blue-deploy.html":{"url":"docs/tke/tke-blue-deploy.html","title":"简单的蓝绿发布和灰度发布","keywords":"","body":"使用 CLB 实现简单的蓝绿发布和灰度发布 原理介绍 用户通常使用 Deployment、StatefulSet 等 Kubernetes 自带的工作负载来部署业务，每个工作负载管理一组 Pod。以 Deployment 为例，示意图如下： 通常还会为每个工作负载创建对应的 Service，Service 通过 selector 来匹配后端 Pod，其他服务或者外部通过访问 Service 即可访问到后端 Pod 提供的服务 蓝绿发布原理 以 Deployment 为例，集群中已部署两个不同版本的 Deployment，其 Pod 拥有共同的 label。但有一个 label 值不同，用于区分不同的版本。Service 使用 selector 选中了其中一个版本的 Deployment 的 Pod，此时通过修改 Service 的 selector 中决定服务版本的 label 的值来改变 Service 后端对应的 Deployment，即可实现让服务从一个版本直接切换到另一个版本。示意图如下： 灰度发布原理 用户通常会为每个工作负载创建一个 Service，但 Kubernetes 未限制 Servcie 需与工作负载一一对应。Service 通过 selector 匹配后端 Pod，若不同工作负载的 Pod 可被同一 selector 选中，即可实现一个 Service 对应多个版本工作负载。调整不同版本工作版本的副本数即调整不同版本服务的权重。示意图如下： 使用 YAML 创建资源 在集群中部署第一个版本的 Deployment，本文以 nginx 为例。版本V1 YAML 示例如下： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-v1 namespace: green-blue spec: replicas: 3 selector: matchLabels: app: nginx version: v1 template: metadata: labels: app: nginx version: v1 spec: containers: - name: nginx image: \"openresty/openresty:centos\" ports: - name: http protocol: TCP containerPort: 80 volumeMounts: - mountPath: /usr/local/openresty/nginx/conf/nginx.conf name: config subPath: nginx.conf volumes: - name: config configMap: name: nginx-v1 --- apiVersion: v1 kind: ConfigMap metadata: labels: app: nginx version: v1 name: nginx-v1 namespace: green-blue data: nginx.conf: |- worker_processes 1; events { accept_mutex on; multi_accept on; use epoll; worker_connections 1024; } http { ignore_invalid_headers off; server { listen 80; location / { access_by_lua ' local header_str = ngx.say(\"nginx-v1\") '; } } } 2.再部署第二个版本的 Deployment，本文以 nginx 为例。YAML 示例如下： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-v2 namespace: green-blue spec: replicas: 3 selector: matchLabels: app: nginx version: v2 template: metadata: labels: app: nginx version: v2 spec: containers: - name: nginx image: \"openresty/openresty:centos\" ports: - name: http protocol: TCP containerPort: 80 volumeMounts: - mountPath: /usr/local/openresty/nginx/conf/nginx.conf name: config subPath: nginx.conf volumes: - name: config configMap: name: nginx-v2 --- apiVersion: v1 kind: ConfigMap metadata: labels: app: nginx version: v2 name: nginx-v2 namespace: green-blue data: nginx.conf: |- worker_processes 1; events { accept_mutex on; multi_accept on; use epoll; worker_connections 1024; } http { ignore_invalid_headers off; server { listen 80; location / { access_by_lua ' local header_str = ngx.say(\"nginx-v2\") '; } } } 3，在集群的工作负载详情页查看部署情况 [root@VM-0-17-tlinux ~]# kubectl get deployment -n green-blue NAME READY UP-TO-DATE AVAILABLE AGE nginx-v1 3/3 3 3 10m nginx-v2 3/3 3 3 9m8s 实现蓝绿发布 为部署的 Deployment 创建 LoadBalancer（内外型） 类型的 Service 对外暴露服务，指定使用 v1 版本的服务。YAML 示例如下： apiVersion: v1 kind: Service metadata: annotations: service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-mwkztq4u name: nginx namespace: green-blue spec: type: LoadBalancer ports: - port: 80 protocol: TCP name: http selector: app: nginx version: v1 2，执行以下命令，测试访问，返回结果如下，均为 v1 版本的响应。 [root@VM-0-17-tlinux ~]# for i in {1..10}; do curl 192.168.3.13; done; nginx-v1 nginx-v1 nginx-v1 nginx-v1 nginx-v1 nginx-v1 nginx-v1 nginx-v1 nginx-v1 nginx-v1 3，通过控制台或 kubectl 方式修改 Service 的 selector，使其选中 v2 版本的服务： selector: app: nginx version: v2 #通过 kubectl 修改： kubectl patch service nginx -p '{\"spec\":{\"selector\":{\"version\":\"v2\"}}}' -n green-blue 4，执行以下命令，再次测试访问，返回结果如下，均为 v2 版本的响应，成功实现了蓝绿发布。 [root@VM-0-17-tlinux ~]# for i in {1..10}; do curl 192.168.3.13; done; nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 实现灰度发布 对比蓝绿发布，不指定 Service 使用 v1 版本服务。即从 selector 中删除 version 标签，让 Service 同时选中两个版本的 Deployment 的 Pod。YAML 示例如下 apiVersion: v1 kind: Service metadata: annotations: service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-mwkztq4u name: nginx namespace: green-blue spec: type: LoadBalancer ports: - port: 80 protocol: TCP name: http selector: app: nginx 2，执行以下命令，测试访问。 for i in {1..10}; do curl EXTERNAL-IP; done; # 替换 EXTERNAL-IP 为 Service 的 CLB IP 地址 [root@VM-0-17-tlinux ~]# for i in {1..10}; do curl 192.168.3.13; done; nginx-v1 nginx-v1 nginx-v2 nginx-v1 nginx-v1 nginx-v1 nginx-v1 nginx-v1 nginx-v2 nginx-v2 #返回结果如下，一半是 v1 版本的响应，另一半是 v2 版本的响应。 3，通过控制台或 kubectl 方式调节 v1 和 v2 版本的 Deployment 的副本，将 v1 版本调至 1 个副本，v2 版本调至 4 个副本： 通过 kubectl 修改 kubectl scale deployment/nginx-v1 --replicas=1 -n green-blue kubectl scale deployment/nginx-v2 --replicas=4 -n green-blue 4，执行以下命令，再次进行访问测试。 for i in {1..10}; do curl EXTERNAL-IP; done; # 替换 EXTERNAL-IP 为 Service 的 CLB IP 地址 返回结果如下，10次访问中仅2次返回了 v1 版本，v1 与 v2 的响应比例与其副本数比例一致，为 1:4。通过控制不同版本服务的副本数就实现了灰度发布。 [root@VM-0-17-tlinux ~]# for i in {1..10}; do curl 192.168.3.13; done; nginx-v1 nginx-v1 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 nginx-v2 "},"docs/tke/tke-jenkins-cicd.html":{"url":"docs/tke/tke-jenkins-cicd.html","title":"基于TKE的Jenkins外网架构应用的构建与部署","keywords":"","body":"操作场景 Jenkins 是连接持续集成和持续交付的桥梁，采用 Jenkins Master/Slave pod 架构能够解决企业批量构建并发限制的痛点，实现和落地真正意义上持续集成。本文介绍了如何在腾讯云容器服务（TKE）中使用 Jenkins，以实现业务快速可持续性交付，减少资源及人力成本。 工作原理 本文采用基于 TKE 的 Jenkins 外网架构，即 Jenkins Master 在 TKE 集群外，slave pod 在集群内。该外网架构图如下所示： Jenkins Master 、TKE 集群位于同一 VPC 网络下。 Jenkins Master 在 TKE 集群外，slave pod 在 TKE 集群的 node 节点上。 用户提交代码到 Gitlab，触发 Jenkins Master 调用 slave pod 进行构建打包并推送镜像到 TKE 镜像仓库，TKE 集群拉取镜像并触发滚动更新进行 Pod 部署。 多 slave pod 构建可满足批量并发构建的需求。 操作环境 本节介绍了该场景中的具体环境，如下： TKE 集群 角色 Kubernetes 版本 操作系统 TKE 托管集群 1.1.8 cls-32qxo736（重庆地域） tlinux2.4x86_64 Jenkins 配置 角色 版本 Jenins Master Jenkins 2.315 Jenkins Kubernetes 插件 1.21.3 节点 角色 内网 IP 外网IP 操作系统 CPU 内存 带宽 Jenkins Master 192.168.2.42 139.186.160.196 tlinux2.4x86_64 4核 8GB 10Mbps Node 192.168.0.17 139.186.152.16 tlinux2.4x86_64 4核 8GB 10Mbps 注意事项 确保与 TKE 集群同 VPC 下已具备 Jenkins Master 节点，并且该节点已安装 Git。 确保操作步骤中用到的 gitlab 代码仓库里面已包含 Dockerfile 文件。 步骤1：TKE 集群配置 获取配置 Jenkins 时所需的集群访问地址、token 及集群 CA 证书信息 获取集群凭证 TKE集群开启RBAC控制后如何获取集群token https://cloud.tencent.com/developer/article/1762567 1，创建serviceAccount [root@VM-0-17-tlinux ~/jenkins]# kubectl create sa tke-admin serviceaccount/tke-admin created 2，为serviceAccount绑定集群角色 [root@VM-0-17-tlinux ~/jenkins]# kubectl create clusterrolebinding tke-admin-binding --clusterrole=tke:admin --serviceaccount=default:tke-admin clusterrolebinding.rbac.authorization.k8s.io/tke-admin-binding created 3，获取serviceAccount对应的token [root@VM-0-17-tlinux ~/jenkins]# kubectl get sa tke-admin -o=jsonpath='{.secrets[0].name}' tke-admin-token-7bvqr [root@VM-0-17-tlinux ~/jenkins]# kubectl get secret tke-admin-token-7bvqr -o=jsonpath='{.data.token}' | base64 -d eyJhbGciOiJSUzI1NiIsImtpZCI6IkVqUDlwYm5CMDBQSTJtclc2NzRZZV9ucnVwcy1FRkdvdHVSOW9rWlB3SGsifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InRrZS1hZG1pbi10b2tlbi03YnZxciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0a2UtYWRtaW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwMjg4M2I2Ny05NGNhLTRhZDUtOTBlNC01YzYyZjQ1MjQ0YzkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDp0a2UtYWRtaW4ifQ.zh4kjnUqWTr9-MINcWvRnbbBbY-3aJCAuSzZgypY8tro-MTQymdjST1PdsmmqVbGnLaqErxcINaPTveOpebk3RjpgIoQ324UTolSVCf7b-TpuVY1Ixw2HjYeoPkXGkbyxO-1OAU2CoLItC8SK5pIjgfWtv8ogaQn8aDah8UzcS4KbUkumm28oFz9EAzcnoGbEjvJJqSGMF5ENiHQbdspeUdT2jkdAaM5dK6E5hGwAIEr5vBdlZO7NfuYyJpUMDrfiwaJg-ynYLPpk3Np4cy0CZItBUSSMnLsoRKsGtq1r0oH3awgZ9Fg8 通过上命令获取sa的token，然后进行base64解密就是你可以使用的token了 获取集群 CA 证书 1，登录目标集群的 node 节点，执行以下命令，查看集群 CA 证书，记录并保存查询所得证书信息 [root@VM-0-17-tlinux ~/jenkins]# cat /etc/kubernetes/cluster-ca.crt 授权 docker.sock TKE 集群中的每个 node 节点系统里都有一个 docker.sock 文件，slave pod 在执行 docker build 时将会连接该文件。在此之前，需逐个登录到每个节点上，依次执行以下命令对 docker build 进行授权 [root@VM-0-17-tlinux ~/jenkins]# ls -l /var/run/docker.sock srw-rw---- 1 root root 0 Sep 27 18:14 /var/run/docker.sock [root@VM-0-17-tlinux ~/jenkins]# chmod 666 /var/run/docker.sock [root@VM-0-17-tlinux ~/jenkins]# ls -l /var/run/docker.sock srw-rw-rw- 1 root root 0 Sep 27 18:14 /var/run/docker.sock 步骤2：Jenkins 安装和配置（CVM方式） 安装参考安装文档 【实用技巧】Jenkins 安装和配置.md 添加 TKE 内网访问地址 登录 Jenkins Master 节点，执行以下命令，配置访问域名。 #该命令可在集群开启内网访问后，从集群基本信息页面中的“集群APIServer” 中获取 sudo sed -i '$a 192.168.2.28 cls-32qxo736.ccs.tencent-cloud.com' /etc/hosts Jenkins 安装必备插件 Locale：汉化语言插件，安装该插件可使 Jenkins 界面默认设置为中文版。 Kubernetes：Kubernetes-plugin 插件。 Git Parameter 和 Extended Choice Parameter：用于构建打包时传参。 开启 jnlp 端口 【系统管理】>【全局安全配置】>设置入站代理的 TCP 端口为“指定端口 50000” 添加 TKE 集群 token 【用户列表】>【admin】>【凭据】>【全局凭据 (unrestricted)】 添加 gitlab 认证 配置 slave pod 模板 【系统管理】>【系统配置】>【新增一个云】>【Kubernetes】 "},"docs/tke/tke-dynamicscheduler.html":{"url":"docs/tke/tke-dynamicscheduler.html","title":"DynamicScheduler组件实践","keywords":"","body":"DynamicScheduler组件 组件介绍 Dynamic Scheduler 是容器服务 TKE 基于 Kubernetes 原生 Kube-scheduler Extender 机制实现的动态调度器插件，可基于 Node 真实负载进行预选和优选。在 TKE 集群中安装该插件后，该插件将与 Kube-scheduler 协同生效，有效避免原生调度器基于 request 和 limit 调度机制带来的节点负载不均问题 部署在集群内的 Kubernetes 对象 最主要的是两个Deployment [root@VM-2-45-tlinux ~]# kubectl get deployment -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE dynamic-scheduler 1/1 1 1 90s node-annotator 1/1 1 1 90s 组件原理 动态调度器基于 scheduler extender 扩展机制，从 Prometheus 监控数据中获取节点负载数据，开发基于节点实际负载的调度策略，在调度预选和优选阶段进行干预，优先将 Pod 调度到低负载节点上。该组件由 node-annotator 和 Dynamic-scheduler 构成 node-annotator node-annotator 组件负责定期从监控中拉取节点负载 metric，同步到节点的 annotation。如下图所示： Dynamic-scheduler Dynamic-scheduler 是一个 scheduler-extender，根据 node annotation 负载数据，在节点预选和优选中进行过滤和评分计算。 预选策略 为了避免 Pod 调度到高负载的 Node 上，需要先通过预选过滤部分高负载的 Node（其中过滤策略和比例可以动态配置，具体请参见本文 组件参数说明）。 如下图所示，Node2 过去5分钟的负载，Node3 过去1小时的负载均超过对应的域值，因此不会参与接下来的优选阶段。 预选阈值配置 实在安装组件时候设置的，如果保持默认配置，则是全部节点都参与预选 优选策略 同时为了使集群各节点的负载尽量均衡，Dynamic-scheduler 会根据 Node 负载数据进行打分，负载越低打分越高。 如下图所示，Node1 的打分最高将会被优先调度（其中打分策略和权重可以动态配置，具体请参见本文 组件参数说明）。 比如上面得分如下 score（node1）=（1-0.3）*0.5*10+（1-0.4）*0.3*10+（1-0.7）*0.2*10 = 5.9分 （Xi 表示的是当前负载百分比 Wi表示的是设置的权重） score（node2）=（1-0.4）*0.5*10+（1-0.5）*0.3*10+（1-0.7）*0.2*10 = 5.1分 score（node2）=（1-0.5）*0.5*10+（1-0.6）*0.3*10+（1-0.6）*0.2*10 = 4.5分 依赖部署 Dynamic Scheduler 动态调度器依赖于 Node 当前和过去一段时间的真实负载情况来进行调度决策，需通过 Prometheus 等监控组件获取系统 Node 真实负载信息。在使用动态调度器之前，需要部署 Prometheus 等监控组件。在容器服务 TKE 中，您可按需选择采用自建的 Prometheus 监控服务或采用 TKE 推出的云原生监控。 聚合规则配置 配置前通过PromQL语句查询相关聚合数据是否存在 查询方式：登陆Granfana>Expore cpu_usage_active： 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) #默认是有数据 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) #默认是有数据 cpu-usage-5m： max_over_time(cpu_usage_avg_5m[1h]) #没有 数据 max_over_time(cpu_usage_avg_5m[1d]) #没有 数据 cpu-usage-1m： avg_over_time(cpu_usage_active[5m]) #没有 数据 mem-usage-5m： max_over_time(mem_usage_avg_5m[1h]) #没有 数据 max_over_time(mem_usage_avg_5m[1d]) #没有 数据 mem-usage-1m： avg_over_time(mem_usage_active[5m]) #没有 数据 查看node-annotato组件日志 添加聚合规则 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: dynamicscheduler spec: groups: - name: cpu_mem_usage_active interval: 30s rules: - record: cpu_usage_active expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) - record: mem_usage_active expr: 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name: cpu-usage-5m interval: 5m rules: - record: cpu_usage_max_avg_1h expr: max_over_time(cpu_usage_avg_5m[1h]) - record: cpu_usage_max_avg_1d expr: max_over_time(cpu_usage_avg_5m[1d]) - name: cpu-usage-1m interval: 1m rules: - record: cpu_usage_avg_5m expr: avg_over_time(cpu_usage_active[5m]) - name: mem-usage-5m interval: 5m rules: - record: mem_usage_max_avg_1h expr: max_over_time(mem_usage_avg_5m[1h]) - record: mem_usage_max_avg_1d expr: max_over_time(mem_usage_avg_5m[1d]) - name: mem-usage-1m interval: 1m rules: - record: mem_usage_avg_5m expr: avg_over_time(mem_usage_active[5m]) 再次通过PromQL语句查询相关聚合数据是否存在 cpu_usage_active： 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[30s])) * 100) #默认是有数据 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) #默认是有数据 cpu-usage-5m： max_over_time(cpu_usage_avg_5m[1h]) #有 数据 max_over_time(cpu_usage_avg_5m[1d]) #有 数据 cpu-usage-1m： avg_over_time(cpu_usage_active[5m]) #有 数据 mem-usage-5m： max_over_time(mem_usage_avg_5m[1h]) #有 数据 max_over_time(mem_usage_avg_5m[1d]) #有 数据 mem-usage-1m： avg_over_time(mem_usage_active[5m]) #有 数据 node-annotator 将 metric同步到节点的 annotation [root@VM-2-45-tlinux ~]# kubectl describe nodes 10.2.22.4 会有如下annotation： Annotations: cpu_usage_avg_5m: 0.08320,2021-10-28T21:54:23Z #cpu5分钟 cpu_usage_max_avg_1d: 0.94400,2021-10-28T21:15:23Z #cpu 1天 cpu_usage_max_avg_1h: 0.94400,2021-10-28T21:45:23Z #cpu 1小时 mem_usage_avg_5m: 0.69384,2021-10-28T21:54:23Z #内存 5分钟 mem_usage_max_avg_1d: 0.67092,2021-10-28T21:15:24Z #内存 1天 mem_usage_max_avg_1h: 0.67092,2021-10-28T21:15:23Z #内存 1小时 组件日志也是显示正常 DeScheduler 组件 组件介绍 DeScheduler 是容器服务 TKE 基于 Kubernetes 原生社区 DeScheduler 实现的一个基于 Node 真实负载进行重调度的插件。在 TKE 集群中安装该插件后，该插件会和 Kube-scheduler 协同生效，实时监控集群中高负载节点并驱逐低优先级 Pod [root@VM-2-45-tlinux ~]# kubectl get deployment -n kube-system | grep descheduler descheduler 1/1 1 1 75s 使用场景 DeScheduler 通过重调度来解决集群现有节点上不合理的运行方式。社区版本 DeScheduler 中提出的策略基于 APIServer 中的数据实现，并未基于节点真实负载。因此可以增加对于节点的监控，基于真实负载进行重调度调整。 容器服务 TKE 自研的 ReduceHighLoadNode 策略依赖 Prometheus 和 node_exporter 监控数据，根据节点 CPU 利用率、内存利用率、网络 IO、system loadavg 等指标进行 Pod 驱逐重调度，防止出现节点极端负载的情况。DeScheduler 的 ReduceHighLoadNode 与 TKE 自研的 Dynamic Scheduler 基于节点真实负载进行调度的策略需配合使用。 组件原理 DeScheduler 基于 社区版本 Descheduler 的重调度思想，定期扫描各个节点上的运行 Pod，发现不符合策略条件的进行驱逐以进行重调度。社区版本 DeScheduler 已提供部分策略，策略基于 APIServer 中的数据，例如 LowNodeUtilization 策略依赖的是 Pod 的 request 和 limit 数据，这类数据能够有效均衡集群资源分配、防止出现资源碎片。但社区策略缺少节点真实资源占用的支持，例如节点 A 和 B 分配出去的资源一致，由于 Pod 实际运行情况，CPU 消耗型和内存消耗型不同，峰谷期不同造成两个节点的负载差别巨大。 因此，腾讯云 TKE 推出 DeScheduler，底层依赖对节点真实负载的监控进行重调度。通过 Prometheus 拿到集群 Node 的负载统计信息，根据用户设置的负载阈值，定期执行策略里面的检查规则，驱逐高负载节点上的 Pod 依赖部署 通过 node-exporter 实现对于 Node 指标的监控，您可按需部署 node-exporter 和 Prometheus。 聚合规则配置 配置前通过PromQL语句查询相关聚合数据是否存在 查询方式：登陆Granfana>Expore mem_usage_active： 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) #存在数据 cpu-usage-1m： 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) #有数据 mem-usage-1m： avg_over_time(mem_usage_active[5m]) #有数据 如果只使用DeScheduler 组件，需要配置如下聚合 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: Descheduler spec: groups: - name: cpu_mem_usage_active interval: 30s rules: - record: mem_usage_active expr: 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name: cpu-usage-1m interval: 1m rules: - record: cpu_usage_avg_5m expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) - name: mem-usage-1m interval: 1m rules: - record: mem_usage_avg_5m expr: avg_over_time(mem_usage_active[5m]) 同时使用 DynamicScheduler 和 DeScheduler 时应该配置如下规则: apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: dynamicscheduler spec: groups: - name: cpu_mem_usage_active interval: 30s rules: - record: mem_usage_active expr: 100*(1-node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) - name: mem-usage-1m interval: 1m rules: - record: mem_usage_avg_5m expr: avg_over_time(mem_usage_active[5m]) - name: mem-usage-5m interval: 5m rules: - record: mem_usage_max_avg_1h expr: max_over_time(mem_usage_avg_5m[1h]) - record: mem_usage_max_avg_1d expr: max_over_time(mem_usage_avg_5m[1d]) - name: cpu-usage-1m interval: 1m rules: - record: cpu_usage_avg_5m expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) - name: cpu-usage-5m interval: 5m rules: - record: cpu_usage_max_avg_1h expr: max_over_time(cpu_usage_avg_5m[1h]) - record: cpu_usage_max_avg_1d expr: max_over_time(cpu_usage_avg_5m[1d]) 组件日志也是显示正常 驱逐 workload 1， 需要驱逐 workload（例如 statefulset、deployment 等对象），可以设置 Annotation 如下 descheduler.alpha.kubernetes.io/evictable: 'true' 2，集群至少有4个节点 ， 而且至少有3个节点负载 低于【目标利用率值】才可以驱逐 也就是集群必须四个节点，一个节点上POD高于利用阈值，其他3个节点负载低于目标利用率 "},"docs/":{"url":"docs/","title":"腾讯云EKS","keywords":"","body":"内容更新中........... 内容更新中 "},"docs/gitbook/gitbook-operation-guide.html":{"url":"docs/gitbook/gitbook-operation-guide.html","title":"Gitbook搭建","keywords":"","body":"环境准备 1，配置 Node.js 环境 使用 Gitbook 需要配置 Node.js 环境，具体的安装步骤，可查看官方文档。 由于目前 Gitbook 项目已经停止维护，Node 过高可能出现不兼容问题，文档后面有常见报错处理方案 安装成功后，执行命令可查看 node 版本和 npm 版本。 # 查看npm版本 npm -v 9.6.5 # 查看node版本 node -v v18.14.2 2，安装 Gitbook 使用下面命令，安装 gitbook 包 npm install -g gitbook-cli 3，初始化项目 3.1，Gitbook 初始化 创建一个文件夹，并进入到该文件夹中，执行下面命令，初始化 gitbook 项目。 gitbook init 执行结果 info: create SUMMARY.md info: initialization is finished 可以看到创建了 SUMMARY.md 文档，这是电子书的目录文档。 然后创建一个 REAMDE.md 文档，用来对这个项目进行介绍。 3.2，npm 初始化 执行下面命令，初始化为 npm 项目。 npm init 命令会提示输入项目信息，可默认不填写，直接回车。 最后，会显示配置信息，输入yes回车即可初始化完毕。 初始化成功后，系统会自动在当前目录创建package.json文件，这是 npm 项目的配置文件。 3.3，章节配置 GitBook 使用文件 SUMMARY.md 来定义书本的章节和子章节的结构。文件 SUMMARY.md 被用来生成书本内容的预览表。 SUMMARY.md 的格式是一个简单的链接列表，链接的名字是章节的名字，链接的指向是章节文件的路径。 子章节被简单的定义为一个内嵌于父章节的列表。 # 概要 - [章节一](chapter1.md) - [章节二](chapter2.md) - [章节三](chapter3.md) # 概要 - [第一章](part1/README.md) - [1.1 第一节](part1/writing.md) - [1.2 第二节](part1/gitbook.md) - [第二章](part2/README.md) - [2.1 第一节](part2/feedback_please.md) - [2.2 第二节](part2/better_tools.md) 4，启动项目 在package.json文件的scripts中配置如下的脚本命令： \"scripts\": { \"serve\": \"gitbook serve\", \"build\": \"gitbook build\" } 分别是 gitbook 在本地启动的命令，和 gitbook 打包成 HTML 静态文件的命令。 对于本地演示，我们可以直接通过下面命令启动。 npm run serve 这条命令其实就是执行了package.json文件的scripts中的serve脚本，即gitbook serve。 启动成功后，就可以在浏览器输入http://localhost:4000/，如图所示。 5，忽略文件 任何在文件夹下的文件，在最后生成电子书时都会被拷贝到输出目录中，如果想要忽略某些文件，和 Git 一样， Gitbook 会依次读取 .gitignore, .bookignore 和 .ignore 文件来将一些文件和目录排除。 6，配置文件 Gitbook 在编译书籍的时候会读取书籍源码顶层目录中的 book.js 或者 book.json，这里以 book.json 为例，参考 gitbook 文档 可以知道，book.json 常用的配置如下。 { // 书籍信息 \"title\": \"学习杂技\", \"description\": \"笔记\", \"isbn\": \"图书编号\", \"author\": \"chen1900s\", \"lang\": \"zh-cn\", // 插件列表 \"plugins\": [], // 插件全局配置 \"pluginsConfig\": { \"fontSettings\": { \"theme\": \"sepia\", \"night\" or \"white\", \"family\": \"serif\" or \"sans\", \"size\": 1 to 4 } }, // 模板变量 \"variables\": { // 自定义 } } gitbook插件使用 Gitbook 最灵活的地方就是有很多插件可以使用，当然如果对插件不满意，也可以自己写插件。所有插件的命名都是以gitbook-plugin-xxx的形式。下面，我们就介绍一些常用的插件。 使用插件前，现在当前项目的根目录中创建一个book.js文件，这是 Gitbook 的配置文件，文件内容可以根据自己来定制，内容格式如上。 1，搜索插件 在命令行输入下面命令安装搜索插件。 npm install gitbook-plugin-search-pro 安装成功后，在book.js中添加插件的配置。 { plugins: ['search-pro']; } 2，代码框插件 在命令行输入下面命令安装代码插件。 npm install gitbook-plugin-code 安装成功后，在book.js中添加插件的配置。 { plugins: ['code']; } 3，自定义主题插件 在命令行输入下面命令安装自定义主题插件。 npm install gitbook-plugin-theme-主题名 安装成功后，在book.js中添加插件的配置。 { plugins: [\"theme-主题名\"] } 4，菜单折叠插件 在命令行输入下面命令安装菜单栏折叠插件。 npm install gitbook-plugin-expandable-chapters 安装成功后，在book.js中添加插件的配置。 { plugins: ['expandable-chapters']; } 5，返回顶部插件 在命令行输入下面命令安装返回顶部插件。 npm install gitbook-plugin-back-to-top-button 安装成功后，在book.js中添加插件的配置。 { plugins: ['back-to-top-button']; } 6，最终效果 下面我们来看看我的运行效果图，比刚开始美观多了。 更多插件可以从 https://plugins.gitbook.com/ 获取。 遇到的问题 1，TypeError [ERR_INVALID_ARG_TYPE]报错 gitbook init 报 TypeError [ERR_INVALID_ARG_TYPE]: The \"data\" argument must be of type string or an instance of Buffer, TypedArray, or DataView. Received an instance of Promise GitBook version: 3.2.3 npm 9.6.5 nodejs v18.14.2 解决方案 将 C:\\Users\\用户\\.gitbook\\versions\\3.2.3\\lib\\ init.js 中第71行附近的 return fs.writeFile(filePath, summary.toText(extension)); 修改为 return summary.toText(extension).then(stx=>{return fs.writeFile(filePath, stx);}) 2，cb.apply is not a function报错 2，高版本Node版本运行gitbook init报错 cb.apply is not a function C:\\Users\\ac_chenjw\\AppData\\Roaming\\npm\\node_modules\\gitbook-cli\\node_modules\\npm\\node_modules\\graceful-fs\\polyfills.js:287 if (cb) cb.apply(this, arguments) 原因是 npm版本的问题 导致 解决方案 查看报错的源码,在node_module/graceful-fs/polyfills.js的285行，对应函数是在 61-63行 ，注释掉就可以 文件路径 C:\\Users\\用户\\AppData\\Roaming\\npm\\node_modules\\gitbook-cli\\node_modules\\npm\\node_modules\\graceful-fs\\polyfills.js:287 fs.stat = statFix(fs.stat) fs.fstat = statFix(fs.fstat) fs.lstat = statFix(fs.lstat) "},"docs/appdeploy/Insatll-crictl-client.html":{"url":"docs/appdeploy/Insatll-crictl-client.html","title":"Crictl客户端安装","keywords":"","body":"安装 CRI 客户端 crictl # https://github.com/kubernetes-sigs/cri-tools/releases/ 选择版本 wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.2/crictl-v1.24.2-linux-amd64.tar.gz sudo tar crictl-v1.24.2-linux-amd64.tar.gz -C /usr/local/bin vi /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false # 验证是否可用 crictl pull centos:latest crictl image crictl rmi centos:latest containerd节点如何使用密码拉取镜像 crictl pull --creds USERNAME[:PASSWORD] 镜像地址 示例： [root@VM-155-12-tlinux ~]#crictl pull --creds 100006305462:chen188289 ccr.ccs.tencentyun.com/chenjingwei/goproxy:latest Image is up to date for sha256:ca30c529755f98c53f660f86fe42f1b38f19ca6127c57aa6025afaf9a016742a "},"docs/appdeploy/Install-harbor.html":{"url":"docs/appdeploy/Install-harbor.html","title":"Harbor镜像仓库部署","keywords":"","body":"一 什么是Harbor Harbor 是由 VMware 公司中国团队为企业用户设计的 Registry server 开源项目，包括了权限管理(RBAC)、LDAP、审计、管理界面、自我注册、HA 等企业必需的功能，同时针对中国用户的特点，设计镜像复制和中文支持等功能 官方文档：https://github.com/goharbor/harbor 部署参考文档：https://my.oschina.net/u/2277632/blog/3095815 二 部署安装 CentOS Linux release 7.8.2003 (Core) Docker version 19.03.13 docker-compose version 1.24.1 1 docker安装 下载地址：https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz 二进制安装，所有节点操作 1.1，下载并解压二进制包 wget https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz tar zxvf docker-19.03.9.tgz mv docker/* /usr/bin 1.2，systemd管理docker cat > /usr/lib/systemd/system/docker.service 1.3，创建配置文件 mkdir /etc/docker cat > /etc/docker/daemon.json 1.4，启动并设置开机启动 systemctl daemon-reload systemctl start docker systemctl enable docker systemctl status docker 2 docker-compose安装 官网文档介绍：https://docs.docker.com/compose/install/ 2.1，下载安装包 或者离线下载安装包我们可以从 Github 上下载它的二进制包来使用，最新发行的版本地址：https://github.com/docker/compose/releases 或通过命令行下载 sudo curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-(uname -s)−(uname -m)\" -o /usr/local/bin/docker-compose 2.2，安装docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 3 安装Harbor wget https://github.com/goharbor/harbor/releases/download/v2.1.4/harbor-offline-installer-v2.1.4.tgz tar xvf harbor-offline-installer-v2.1.4.tgz 根据需要修改相关参数 [root@chen harbor]# cp harbor.yml.tmpl harbor.yml #默认是harbor.yml.tmpl需要将这个文件重命名一下 [root@chen harbor]# vi harbor.yml [root@chen harbor]# ./install.sh 有如上提示表示安装成功 三 基本使用 然后我们访问一下这个地址，账号是admin123，密码就是配置文件里面那个harbor.yml文件里面 1 仓库管理 可以添加其他仓管，以腾讯云TCR镜像仓库为例 2 复制管理 复制管理 可以将本地镜像复制到其他镜像仓库，也可以将其他镜像仓库复制到本地 3 项目管理 4 仓库管理 "},"docs/appdeploy/Install-docker.html":{"url":"docs/appdeploy/Install-docker.html","title":"Docker安装和配置","keywords":"","body":"CentOS Docker 安装 1，使用 Docker 仓库进行安装 设置仓库 安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2 sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 阿里源： sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 清华源： sudo yum-config-manager \\ --add-repo \\ https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 安装 Docker Engine-Community： sudo yum install docker-ce docker-ce-cli containerd.io 2，安装docker 二进制安装 二级制包下载地址：https://download.docker.com/linux/static/stable/x86_64/ tar -zxvf docker-18.09.6.tgz mv docker/* /usr/bin/ vi /usr/lib/systemd/system/docker.service [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFLE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target 3，配置镜像加速源 vi /etc/docker/daemon.json { \"registry-mirrors\": [ \"https://mirror.ccs.tencentyun.com\" ] } 更多镜像加速源可以参考：https://www.runoob.com/docker/docker-mirror-acceleration.html 启动 systemctl start docker systemctl enable docker systemctl status docker "},"docs/appdeploy/Install-gitlab.html":{"url":"docs/appdeploy/Install-gitlab.html","title":"GitLab的安装及使用教程","keywords":"","body":"GitLab的安装及使用 安装 1、配置yum源 vim /etc/yum.repos.d/gitlab-ce.repo 复制以下内容： [root@chen ~]# cat /etc/yum.repos.d/gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7 repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key 2、更新本地yum缓存 sudo yum makecache [root@chen ~]# yum makecache yum install -y postfix systemctl enable postfix vim /etc/postfix/main.cf #删除 inet_interfaces = all 前的 #，在 inet_interfaces = localhost 前加上 systemctl start postfix 3、安装GitLab社区版 sudo yum install gitlab-ce #自动安装最新版 [root@chen ~]# sudo EXTERNAL_URL=\"实例公网 IP 地址\" yum install -y gitlab-ce 4，安装完 需要修改下配置文件，将指的域名替换成公网IP vim /etc/gitlab/gitlab.rb #将external_url 变量的地址修改为gitlab所在centos的ip地址。 external_url ‘http://git.home.com’ gitlab-ctl reconfigure //让配置生效，重新执行此命令时间也比较长 gitlab-ctl restart 5，获得用户数据，修改用户密码 [root@VM-3-9-tlinux /opt/gitlab/bin]# gitlab-rails console irb(main):007:0> User.where(id: 1).first => # irb(main):008:0> user = User.where(id: 1).first => # irb(main):009:0> user.password=12345678 => 12345678 irb(main):010:0> user.password_confirmation=12345678 => 12345678 irb(main):011:0> user.save! Enqueued ActionMailer::MailDeliveryJob (Job ID: 4977da90-a2bf-4687-b39b-bb65430f8530) to Sidekiq(mailers) with arguments: \"DeviseMailer\", \"password_change\", \"deliver_now\", {:args=>[#>]} => true irb(main):012:0> quit GitLab常用命令 udo gitlab-ctl start # 启动所有 gitlab 组件； sudo gitlab-ctl stop # 停止所有 gitlab 组件； sudo gitlab-ctl restart # 重启所有 gitlab 组件； sudo gitlab-ctl status # 查看服务状态； sudo gitlab-ctl reconfigure # 启动服务； sudo vim /etc/gitlab/gitlab.rb # 修改默认的配置文件； gitlab-rake gitlab:check SANITIZE=true --trace # 检查gitlab； sudo gitlab-ctl tail # 查看日志； GitLab使用 登录GitLab 1、在浏览器的地址栏中输入公网IP即可登录GitLab的界面，使用上面修改的的用户名和密码为 root 和 xxxxxxx 2、首次登录会强制用户修改密码。密码修改成功后，输入新密码进行登录。 创建Project 1，安装Git工具linux：安装Git，使用自带的源安装。或者Windows 安装git [root@VM-3-9-tlinux ~]# yum install git 2，生成密钥文件：使用ssh-keygen生成密钥文件.ssh/id_rsa.pub。 [root@VM-3-9-tlinux ~]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:8O/MfCKmOcz6SfDk8adEhgXfFQbXH44Rn4meKZs8i8Y root@VM-3-9-tlinux The key's randomart image is: +---[RSA 2048]----+ | . ..=+. | | o . + .+.o| | . o . .+=.| | = ..o..| | . + S . + | | = = .. + | | o+ + o= | | .++oEo + | | .o=+oo=+ | +----[SHA256]-----+ 3，在GitLab的主页中新建一个Project 4，添加ssh key导入步骤2中生成的密钥文件内容： 5， ssh key添加完成： 可以通过命令验证 ssh -T git@github.com 6，项目地址，该地址在进行clone操作时需要用到: 克隆项目 在已纳入管理的 PC 上执行以下命令，配置使用 Git 仓库的人员姓名。 git config --global user.name \"username\" 执行以下命令，配置使用 Git 仓库的人员邮箱。 git config --global user.email \"xxx@example.com\" 执行以下命令，克隆项目。其中“项目地址”请替换为项目地址。 git clone “项目地址” 克隆项目成功后，会在本地生成同名目录且包含项目中所有文件。 初始化本地项目 PS F:\\容器wiki> git init Reinitialized existing Git repository in F:/容器wiki/.git/ PS F:\\容器wiki> git remote add origin git@1.116.17.152:root/kubernetes.git PS F:\\容器wiki> git add . PS F:\\容器wiki> git commit -m \"first\" PS F:\\容器wiki> git push -u origin master 上传文件 执行以下命令，进入项目目录。 cd test/ 执行以下命令，创建需上传至 GitLab 的目标文件。本文以 test.sh 为例。 echo \"test\" > test.sh 执行以下命令，将 test.sh 文件加入索引中。 git add test.sh 执行以下命令，将 test.sh 提交至本地仓库。 git commit -m \"test.sh\" 执行以下命令，将 test.sh 同步至 GitLab 服务器。 git push -u origin master "},"docs/appdeploy/Install-helm.html":{"url":"docs/appdeploy/Install-helm.html","title":"Helm的安装和使用","keywords":"","body":"一 安装 Helm 客户端 Helm项目提供了两种获取和安装Helm的方式。这是官方提供的获取Helm发布版本的方法。另外， Helm社区提供了通过不同包管理器安装Helm的方法。这些方法可以在下面的官方方法之后看到。 用二进制版本安装 每个Helm 版本都提供了各种操作系统的二进制版本，这些版本可以手动下载和安装。 下载 需要的版本 解压(tar -zxvf helm-v3.0.0-linux-amd64.tar.gz) 在解压目中找到helm程序，移动到需要的目录中(mv linux-amd64/helm /usr/local/bin/helm) 具体可以参考： https://helm.sh/zh/docs/intro/install/ 二 添加helm仓库 以腾讯云TCR镜像仓库为例 helm repo add $instance-$namespace https://$instance.tencentcloudcr.com/chartrepo/$namespace --username $username --password $instance-token #helm repo add tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm --username 10002438xxxx --password 密码xxxxxxxx $instance-$namespace：为 helm repo 名称，建议使用实例名称+命名空间名称组合的方式命名，以便于区分各个实例及命名空间。 https://$instance.tencentcloudcr.com/chartrepo/$namespace ：为 helm repo 的远端地址。 $username：已获取的用户名。 $instance-token：已获取的登录密码。 如添加成功将提示以下信息。 \"tcr-chen-helm\" has been added to your repositories 使用该命令可以查看当前的helm 仓库信息 # helm repo list NAME URL nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ tcr-chen-helm https://tcr-chen.tencentcloudcr.com/chartrepo/helm 三 推送 Helm Chart 安装 Helm Push 插件 注意： 请安装 0.9.0 及以上版本的 helm-push 插件，避免因版本不兼容等问题造成无法正常推送 helm chart。 使用 Helm CLI 上传 Chart 包需要安装 helm-push 插件，该插件支持使用helm push 指令推送 helm chart 至指定 repo，同时支持上传目录及压缩包。 helm plugin install https://github.com/chartmuseum/helm-push 在节点上执行以下命令，创建一个 Chart。 helm create tcr-chart-demo 执行以下命令，可直接推送指定目录至 Chart 仓库（可选）。 helm push tcr-chart-demo $instance-$namespace #helm cm-push tcr-chart-demo $instance-$namespace #高级版本使用的是cm-push命令 其中 $instance-$namespace 为已添加的本地仓库名称。 执行以下命令，可压缩指定目录，并推送至 Chart 仓库。 tar zcvf tcr-chart-demo-1.0.0.tgz tcr-chart-demo/ helm push tcr-chart-demo-1.0.0.tgz $instance-$namespace 其中$instance-$namespace为已添加的本地仓库名称。 四 拉取 Helm Chart 在节点上执行以下命令，获取最新的 Chart 信息。 helm repo update 执行以下命令，拉取指定版本 Helm Chart。 helm fetch / --version 以从企业版实例 tcr-demo 中拉取命名空间 project-a 内 tcr-chart-demo 1.0.0 版本为例： helm fetch tcr-chen-heml/tcr-chart-demo --version 1.0.0 五 Harbor 启用 helmchart 服务 1，安装 harbor 的 helmchart repository 默认新版 harbor 不会启用 chart repository service，如果需要管理 helm，我们需要在安装时添加额外的参数，例如：启用 chart repository service 服务的安装方式要添加一个参数 --with-chartmuseum [root@VM-55-9-tlinux ~/docker-compose/harbor]# ./install.sh --with-chartmuseum 安装完成后，会有这个提示 说明是安装成功： ⠿ Container chartmuseum Started 2，发布 helm charts 方式一、基于dashboard 的可视化上传 使用浏览器登录 harbor 后，在对应的管理界面操作即可，如下图： 方式二、基于命令行的 CLI 推送 更多时候基于第1种UI界面的上传并不能满足我们的实际需求，大部分情况我们都是要通过脚本发布helmchart 的。 1、安装插件 为了能使用命令推送，我们需要安装并使用 helm push 插件包，地址： https://github.com/chartmuseum/helm-push/releases a) 在线安装插件： helm plugin install https://github.com/chartmuseum/helm-pus b) 离线安装插件： 下载安装包 helm-push_0.10.1_linux_amd64.tar.gz，再使用命令 helm env 获取 HELM_PLUGINS 路径，然后放置和解压安装包，最后使用 helm plugin list 查看结果，如下： [root@VM-55-9-tlinux ~/harbor]# helm env | grep HELM_PLUGINS HELM_PLUGINS=\"/root/.local/share/helm/plugins [root@VM-55-9-tlinux ~/harbor]# mkdir -p /root/.local/share/helm/plugins/helm-push [root@VM-55-9-tlinux ~/harbor]# mv helm-push_0.10.1_linux_amd64.tar.gz /root/.local/share/helm/plugins/helm-push/ [root@VM-55-9-tlinux ~/harbor]# cd /root/.local/share/helm/plugins/helm-push/ [root@VM-55-9-tlinux helm-push]# tar -xzvf helm-push_0.10.1_linux_amd64.tar.gz [root@VM-55-9-tlinux helm-push]# helm plugin list NAME VERSION DESCRIPTION cm-push 0.10.1 Push chart package to ChartMuseum 2、添加 harbor 仓库到本地 helm 仓库列表 查看本地仓库列表(列出的是我已经添加其他仓库) [root@VM-55-9-tlinux ~/helm]# helm repo list NAME URL tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm # 添加仓库地址到本地列表(其中 myharbor-helm 为这个仓库地址在 helm 本地的名称，连接是仓库URL，后面是登录 harbor 的用户名和密码) # URL格式：http(s)://{harbor域名或iP:端口(如果默认443或80可不加)}/chartrepo/{yourHarborProjectName} [root@VM-55-9-tlinux ~/helm]# helm repo add myharbor-helm http://101.35.6.116:88/chartrepo/charts --username admin --password admin123 \"myharbor-helm\" has been added to your repositories # 再查看(发现已添加成功) [root@VM-55-9-tlinux ~/helm]# helm repo list NAME URL tke-pass-helm https://tke-pass.tencentcloudcr.com/chartrepo/helm myharbor-helm http://101.35.6.116:88/chartrepo/charts ##更新本地仓库缓存内容 [root@VM-55-9-tlinux ~/helm]# helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"myharbor-helm\" chart repository ...Successfully got an update from the \"tke-pass-helm\" chart repository Update Complete. ⎈Happy Helming!⎈ 注意点 1.harbor 仓库 URL 中的 chartrepo 是固定值。 2.在操作之前，请务必先在 harbor 中创建好项目，例如 charts即为先创建好的项目名称。 3.如果你还是搞不清这个URL，可以在harbor界面中上传一个外面下着的 chart 包，上次成功后进入这个 chart 详细页面，在 “概要这个Tab” 的最底部区域，harbor会告诉你在本地添加仓库的URL和命令。 4.推送 chart 以及 chart 的更多操作 推送 chart 示例 # 推送chart文件夹方式 helm push mychartdemo myharbor-helm # 推送chart压缩包方式 helm push mychartdemo-1.0.1.tgz myharbor-helm "},"docs/appdeploy/Install-jenkins.html":{"url":"docs/appdeploy/Install-jenkins.html","title":"Jenkins安装和配置","keywords":"","body":"Jenkins 安装和配置（CVM方式） 1，安装jdk以及配置环境变量 下载 JDK，输入如下命令： mkdir /usr/java # 创建 java 文件夹 cd /usr/java # 进入 java 文件夹 上传 JDK 安装包（推荐） rz jdk-8u151-linux-x64.tar.gz tar -xvf jdk-8u151-linux-x64.tar.gz #解压 设置环境变量 vi /etc/profile #添加如下环境变量================= #############2020-10-10 jdk env################ export JAVA_HOME=/usr/java/jdk1.8.0_151 export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib export PATH=$JAVA_HOME/bin:$PATH 加载环境变量 source /etc/profile 查看 JDK 是否安装成功 [root@VM-2-42-tlinux ~]# java -version java version \"1.8.0_151\" Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) 2，安装maven以及配置环境变量（可选） mkdir /usr/maven # 创建 maven 文件夹 cd /usr/maven # 进入 maven 文件夹 上传 maven安装包并解压 tar -xvf apache-maven-3.5.3-bin.tar.gz -C /usr/maven/ 设置环境变量 vi /etc/profile #############2021-10-10 maven env ################## export MAVEN_HOME=/usr/maven/apache-maven-3.5.3 export PATH=$MAVEN_HOME/bin:$PATH 检查maven是否安装成功 [root@chen ~]# mvn -v Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-25T03:49:05+08:00) Maven home: /usr/maven/apache-maven-3.5.3 Java version: 1.8.0_151, vendor: Oracle Corporation Java home: /usr/java/jdk1.8.0_151/jre Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"3.10.0-1127.13.1.el7.x86_64\", arch: \"amd64\", family: \"unix\" 3，安装tomcat 上传或者下载安装包 [root@VM-2-42-tlinux ~]# cd /opt/tomcat/ [root@VM-2-42-tlinux /opt/tomcat]# ls apache-tomcat-8.5.39.tar.gz #### # 镜像地址会改变，Tomcat 版本也会不断升级。如果下载链接失效，请您到 [Tomcat 官网](https://tomcat.apache.org/download-80.cgi)选择合适的安装包地址。 wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.39/bin/apache-tomcat-8.5.39.tar.gz tar -xzvf apache-tomcat-8.5.39.tar.gz 解压安装包 [root@VM-2-42-tlinux /opt/tomcat]# tar -xvf apache-tomcat-8.5.39.tar.gz 4，下载Jenkins安装的war包 Jenkins War Packages https://get.jenkins.io/war-stable/ # wget http://mirrors.jenkins.io/war-stable/2.107.1/jenkins.war mv jenkins.war /opt/tomcat/apache-tomcat-8.5.39/webapps/ 5，启动tomcat服务 进入 Tomcat 服务器的 bin 目录，然后执行./startup.sh命令启动 Tomcat 服务器。 cd /opt/tomcat/apache-tomcat-8.5.39/bin ./startup.sh 6，登陆 在浏览器地址栏中输入 http://公网IP:端口（端口为 server.xml 中设置的 connector port）进行访问，登陆服务控制台http://139.186.160.196:8080/jenkins #密码获取 [root@VM-2-42-tlinux ~]# cat /root/.jenkins/secrets/initialAdminPassword 用户名：admin 密码：admin123 "},"docs/appdeploy/Install-nfs.html":{"url":"docs/appdeploy/Install-nfs.html","title":"Linux服务之NFS服务器","keywords":"","body":"Linux服务之NFS服务器 1. NFS 协议 NFS服务工作在TCP的2049端口，UDP的2049端口。 NFS是Network File System的缩写，即网络文件系统，是一种使用于分散式文件系统的协定。功能是通过网络让不同的机器、不同的操作系统能够彼此分享个别的数据，让应用程序在客户端通过网络访问位于服务器磁盘中的数据，是在类Unix系统间实现磁盘文件共享的一种方法。 这个NFS服务器可以让你的PC来将网络远程的NFS服务器分享的目录，挂载到本地端的机器当中， 在本地端的机器看起来，那个远程主机的目录就好像是自己的一个磁盘分区槽一样。 1.1 工作原理 因为 NFS 支持的功能相当的多，而不同的功能都会使用不同的程序来启动， 每启动一个功能就会启用一些端口来传输数据，因此， NFS 的功能所对应的端口才没有固定住， 而是随机取用一些未被使用的小于 1024 的端口来作为传输之用。但如此一来又造成客户端想要连上服务器时的困扰， 因为客户端得要知道服务器端的相关端口才能够联机吧！ NFS在文件传送或信息传送过程中依赖于RPC协议。RPC，即远程过程调用的缩写，是能使客户端执行其他系统中程序的一种机制。RPC 最主要的功能就是在指定每个 NFS 功能所对应的端口号，并且回报给客户端，让客户端可以连结到正确的端口上去。 NFS本身是没有提供信息传输的协议和功能的，但NFS却能让我们通过网络进行资料的分享，这是因为NFS使用了一些其它的传输协议。而这些传输协议用到这个RPC功能的。可以说NFS本身就是使用RPC的一个程序，或者说NFS也是一个RPC SERVER。所以只要用到NFS的地方都要启动RPC服务，不论是NFS SERVER或者NFS CLIENT。这样SERVER和CLIENT才能通过RPC来实现PROGRAM PORT的对应。可以这么理解RPC和NFS的关系：NFS是一个文件系统，而RPC是负责负责信息的传输。 事实上，有很多这样的服务器都是向 RPC 注册的，举例来说，NIS (Network Information Service) 也是 RPC server 的一种。 Linux服务之NFS服务器 那RPC又是如何知道每个NFS的端口呢？ 这是因为当服务器在启动 NFS 时会随机取用数个端口，并主动的向 RPC 注册，因此 RPC 可以知道每个埠口对应的 NFS 功能，然后 RPC 又是固定使用 111 端口来监听客户端的需求并报客户端正确的埠口， 所以当然可以让 NFS 的启动更为轻松愉快了。 所以你要注意，要启动 NFS 之前，RPC 就要先启动了，否则 NFS 会无法向 RPC 注册。 另外，RPC 若重新启动时，原本注册的数据会不见，因此 RPC 重新启动后，它管理的所有服务都要重新启动来重新向 RPC 注册。 那客户端如何向NFS服务端交换数据数据呢？ (1) 客户端会向服务器端的 RPC 的111端口发出 NFS 档案存取功能的询问要求 (2) 服务器端找到对应的已注册的 NFS 守护进程端口后，会回报给客户端 (3) 客户端了解正确的端口后，就可以直接与 NFS 守护进程来联机 1.2 激活 NFS 服务 NFS 服务需要激活几个重要的 RPC 守护进程 工作流程 nfs—client => portmapper => mountd => nfs-server(nfsd) Linux服务之NFS服务器 (1) rpc.nfsd 这个守护进程主要的功能，则是在管理客户端是否能够登入主机的权限，其中还包含这个登入者的 ID 的判别。 (2) rpc.mountd 主要功能 这个守护进程主要的功能，则是在管理 NFS 的档案系统，用于给用户提供访问令牌。 访问的令牌，由本地的RPC提供随机端口。本地的RPC叫做portmapper，可以使用rpcinfo -P查看。 RPC的portmapper服务工作在1111端口。 请求过程 当客户端顺利的通过 rpc.nfsd 而登入主机之后，在它可以使用 NFS server 提供的档案之前，还会经过档案使用权限 的认证程序，就是那个-rwxrwxrwx、owner、group那几个权限啦。 然后它会去读 NFS 的设定档 /etc/exports 来比对客户端的权限，当通过这一关之后，客户端就可以取得使用 NFS 档案的权限啦。 注释：NFS需要有两个套件 nfs-utils NFS服务的主要套件 提供rpc.nfsd和rpc.mountd两个NFS守护进程和与其它相关文档与说明文件、执行档等的套件 portmap 主要负责RPC端口和守护进程的映射关系，即portmapper 在激活任何一个RPC server之前，我们都需要激活portmapper才行 1.3 各个版本之间的比较 NFS是一种网络文件系统，从1985年推出至今，共发布了3个版本：NFSv2、NFSv3、NFSv4，NFSv4包含两个次版本NFSv4.0和NFSv4.1。经过20多年发展，NFS发生了非常大的变化，最大的变化就是推动者从Sun变成了NetApp，NFSv2和NFSv3基本上是Sun起草的，NetApp从NFSv4.0参与进来，并且主导了NFSv4.1标准的制定过程，而Sun已经被Oracle收购了。 编号 协议版本 RFC 时间 页数 1 NFSv2 rfc1094 1989 年 3 月 27 页 2 NFSv3 rfc1813 1995 年 6 月 126 页 3 NFSv4.0 rfc3530 2003 年 4 月 275 页 4 NFSv4.1 rfc5661 2010 年 1 月 617 页 1. NFSv2 NFSv2是第一个以RFC形式发布的版本，实现了基本的功能。 2. NFSv3 协议特点 NFSv3修正了NFSv2的一些bug，两者有如下一些差别，但是感觉没有本质的差别。 区别差别 (1) NFSv2只支持同步写，如果客户端向服务器端写入数据，服务器必须将数据写入磁盘中才能发送应答消息。NFSv3支持异步写操作，服务器只需要将数据写入缓存中就可以发送应答信息了。 (2) NFSv3增加了ACCESS请求，ACCESS用来检查用户的访问权限。因为服务器端可能进行uid映射，因此客户端的uid和gid不能正确反映用户的访问权限。 (3) 一些请求调整了参数和返回信息，毕竟NFSv3和NFSv2发布的间隔有6年，经过长期运行可能觉得NFSv2某些请求参数和返回信息需要改进。 3. NFSv4.0 协议特点 相比NFSv3，NFSv4发生了比较大的变化，最大的变化是NFSv4有状态了。NFSv2和NFSv3都是无状态协议，服务区端不需要维护客户端的状态信息。 无状态协议的一个优点在于灾难恢复，当服务器出现问题后，客户端只需要重复发送失败请求就可以了，直到收到服务器的响应信息。 区别差别 (1) NFSv4增加了安全性，支持RPCSEC-GSS身份认证。 (2) NFSv4设计成了一种有状态的协议，自身实现了文件锁功能和获取文件系统根节点功能。 (3) NFSv4只提供了两个请求NULL和COMPOUND，所有的操作都整合进了COMPOUND中，客户端可以根据实际请求将多个操作封装到一个COMPOUND请求中，增加了灵活性。 (4) NFSv4文件系统的命令空间发生了变化，服务器端必须设置一个根文件系统(fsid=0)，其他文件系统挂载在根文件系统上导出。 (5) NFSv4支持delegation。由于多个客户端可以挂载同一个文件系统，为了保持文件同步，NFSv3中客户端需要经常向服务器发起请求，请求文件属性信息，判断其他客户端是否修改了文件。如果文件系统是只读的，或者客户端对文件的修改不频繁，频繁向服务器请求文件属性信息会降低系统性能。NFSv4可以依靠delegation实现文件同步。 (6) NFSv4修改了文件属性的表示方法。由于NFS是Sun开发的一套文件系统，设计之出NFS文件属性参考了UNIX中的文件属性，可能Windows中不具备某些属性，因此NFS对操作系统的兼容性不太好。 4. NFSv4.1 与NFSv4.0相比，NFSv4.1最大的变化是支持并行存储了。在以前的协议中，客户端直接与服务器连接，客户端直接将数据传输到服务器中。 当客户端数量较少时这种方式没有问题，但是如果大量的客户端要访问数据时，NFS服务器很快就会成为一个瓶颈，抑制了系统的性能。NFSv4.1支持并行存储，服务器由一台元数据服务器(MDS)和多台数据服务器(DS)构成，元数据服务器只管理文件在磁盘中的布局，数据传输在客户端和数据服务器之间直接进行。由于系统中包含多台数据服务器，因此数据可以以并行方式访问，系统吞吐量迅速提升。 2. NFS 服务搭建 CentOS7以NFSv4作为默认版本，NFSv4使用TCP协议（端口号是2049）和NFS服务器建立连接。 # 系统环境 系统平台：CentOS Linux release 7.9 (Final) NFS Server IP：192.168.0.17 防火墙已关闭/iptables: Firewall is not running. SELINUX=disabled 2.1 安装 NFS 服务 服务端 服务端，程序包名nfs-utils、rpcbind，默认都已经安装了 可以通过rpm -ql nfs-utils查看帮助文档等信息 客户端 客户端，需要安装程序包名nfs-utils，提供基本的客户端命令工具 [root@VM-0-17-tlinux ~/nfs]# yum install nfs-utils -y 查看NFS服务端口 NFS启动时会随机启动多个端口并向RPC注册，为了方便配置防火墙，需要固定NFS服务端口。 这样如果使用iptables对NFS端口进行限制就会有点麻烦，可以更改配置文件固定NFS服务相关端口 分配端口，编辑配置文件/etc/sysconfig/nfs # 使用rpcinfo -P会发现rpc启动了很多监听端口 [root@VM-0-17-tlinux ~/nfs]# rpcinfo -p localhost:q1 # 添加如下 [root@VM-0-17-tlinux ~/nfs]# vim /etc/sysconfig/nfs RQUOTAD_PORT=30001 LOCKD_TCPPORT=30002 LOCKD_UDPPORT=30002 MOUNTD_PORT=30003 STATD_PORT=30004 启动服务： [root@VM-0-17-tlinux ~/nfs]# systemctl start nfs.service 2.2 服务文件配置 相关文件和命令 文件名 说明 /etc/exports NFS 服务器端需要设定的内容，其作用是设定谁拥有什么样的权限去访问此机器的哪个目录 /var/lib/nfs/etab 无需设定，用于纪录 NFS 服务器完整的权限设定，exports 中没有设定的缺省值也会被列出 /var/lib/nfs/xtab 纪录 NFS 连接的相关信息 /usr/sbin/exportfs NFS 设定管理命令，用于 Server 侧设定，通过此条命令使得 exports 的设定变得有效或者无效 /usr/sbin/showmount 用于显示 NFS 设定的相关信息，Server 端和 Client 端均可 配置文件/etc/exports 我们可以按照“共享目录的路径 允许访问的 NFS 客户端(共享权限参数)”的格式，定义要共享的目录与相应的权限 常用参数 作用 ro 只读 rw 读写 sync 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据 root_squash 当 NFS 客户端以 root 管理员访问时，映射为 NFS 服务器的匿名用户 all_squash 无论 NFS 客户端使用什么账户访问，均映射为 NFS 服务器的匿名用户 no_root_squash 当 NFS 客户端以 root 管理员访问时，映射为 NFS 服务器的 root 管理员 secure 这个选项是缺省选项，它使用了 1024 以下的 TCP/IP 端口实现 NFS 的连接 insecure 禁止使用了 1024 以下的 TCP/IP 端口实现 NFS 的连接 no_wdelay 这个选项关闭写延时，如果设置了 async，那么 NFS 就会忽略这个选项 nohide 如果将一个目录挂载到另外一个目录之上，那么原来的目录通常就被隐藏起来或看起来像空的一样。要禁用这种行为，需启用 hide 选项 no_subtree_check 这个选项关闭子树检查，子树检查会执行一些不想忽略的安全性检查，缺省选项是启用子树检查 no_auth_nlm 这个选项也可以作为insecure_locks指定，它告诉 NFS 守护进程不要对加锁请求进行认证。如果关心安全性问题，就要避免使用这个选项。缺省选项是auth_nlm或secure_locks。 mp(mountpoint=path) 通过显式地声明这个选项，NFS 要求挂载所导出的目录 fsid=num 这个选项通常都在 NFS 故障恢复的情况中使用，如果希望实现 NFS 的故障恢复，请参考 NFS 文 [root@VM-0-17-tlinux ~/nfs]# cat /etc/exports /nfs 192.168.*.*(rw,sync,root_squash) #修改完配置后 重新启动nfs服务 root@VM-0-17-tlinux ~/nfs]# systemctl restart nfs [root@VM-0-17-tlinux ~/nfs]# systemctl status nfs 3. 实战演示 将NFS挂在到K8S容器服务的POD里面 案例一 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt name: nfs volumes: - name: nfs nfs: path: /nfs server: 192.168.0.17 [root@VM-0-17-tlinux ~/nfs]# cd /nfs/ [root@VM-0-17-tlinux /nfs]# echo hello worload > hello.txt [root@VM-0-17-tlinux /nfs]# cat hello.txt hello worload #登录容器查看挂着情况 [root@VM-0-17-tlinux /nfs]# kubectl exec -it centos-54db87ccc9-nkx86 -- /bin/bash [root@centos-54db87ccc9-nkx86 /]# [root@centos-54db87ccc9-nkx86 /]# df -h Filesystem Size Used Avail Use% Mounted on overlay 50G 11G 37G 23% / tmpfs 64M 0 64M 0% /dev tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup 192.168.0.17:/nfs 50G 32G 16G 67% /mnt /dev/vda1 50G 11G 37G 23% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 3.9G 12K 3.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.9G 0 3.9G 0% /proc/acpi tmpfs 3.9G 0 3.9G 0% /proc/scsi tmpfs 3.9G 0 3.9G 0% /sys/firmware [root@centos-54db87ccc9-nkx86 /]# cd /mnt/ [root@centos-54db87ccc9-nkx86 mnt]# cat hello.txt hello worload [root@centos-54db87ccc9-nkx86 mnt]# echo 1111>> hello.txt bash: hello.txt: Permission denied [root@centos-54db87ccc9-nkx86 mnt]ls -lrt total 4 -rw-r--r-- 1 root root 14 Oct 17 13:14 hello.txt 可以看到 上面我们的 /etc/exports /nfs 192.168.*.*(rw,sync,root_squash) #是没有权限修改文件 下面我们将配置文件修改成如下配置进行测试 [root@VM-0-17-tlinux /nfs]# cat /etc/exports /nfs 192.168.*.*(rw,sync,no_root_squash) #重启NFS 服务 [root@VM-0-17-tlinux /nfs]# systemctl restart nfs [root@VM-0-17-tlinux /nfs]# systemctl status nfs 将POD销毁重建后登录POD里测试。可以修改文件 并且可以创建文件 [root@centos-54db87ccc9-rgnk8 mnt]# touch 222 [root@centos-54db87ccc9-rgnk8 mnt]# ls 222 hello.txt [root@centos-54db87ccc9-rgnk8 mnt]# echo \"1111\">> hello.txt [root@centos-54db87ccc9-rgnk8 mnt]# cat hello.txt hello worloada 1111 [root@centos-54db87ccc9-rgnk8 mnt]# ls -lrt total 4 -rw-r--r-- 1 root root 0 Oct 17 13:24 222 -rw-r--r-- 1 root root 20 Oct 17 13:28 hello.txt 案例二 使用PVC和PV方式创建并挂载 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv-operationdata spec: accessModes: - ReadWriteMany capacity: storage: 10Gi csi: driver: com.tencent.cloud.csi.cfs volumeAttributes: host: 192.168.0.17 path: /nfs volumeHandle: cfs-pv2 persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-operationdata namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: \"\" volumeMode: Filesystem volumeName: pv-operationdata --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: centos name: centos namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: centos template: metadata: labels: k8s-app: centos spec: containers: - args: - -c - sleep 360000 command: - /bin/sh image: centos:latest imagePullPolicy: IfNotPresent name: centos resources: {} volumeMounts: - mountPath: /mnt name: nfs volumes: - name: nfs persistentVolumeClaim: claimName: pvc-operationdata 案例三 服务器端挂载 使用showmount命令查询NFS服务器的远程共享信息 showmount命令输出格式为“共享的目录名称 允许使用客户端地址” showmount命令 参数 作用 -e 显示 NFS 服务器的共享列表 -a 显示本机挂载的文件资源的情况 NFS 资源的情况 -v 显示版本号 exportfs命令 维护exports文件导出的文件系统表的专用工具，可以修改配置之后不重启NFS服务 export -ar：重新导出所有的文件系统 export -au：关闭导出的所有文件系统 export -u FS:：关闭指定的导出的文件系统 # 查看NFS服务器端共享的文件系统 # showmount -e NFSSERVER_IP [root@VM-0-17-tlinux ~/nfs]# showmount -e 192.168.0.17 Export list for 192.168.0.17: /nfs 192.168.*.* # NFS客户端创建一个挂载目录，挂载服务端NFS文件系统到本地 # mount -t nfs SERVER:/path/to/sharedfs /path/to/mount_point [root@VM-0-11-tlinux ~]# mkdir /nfsfile [root@VM-0-11-tlinux ~]# mount -t nfs 192.168.0.17:/nfs /nfsfile [root@VM-0-11-tlinux ~]# df -h | grep nfsfile 192.168.0.17:/nfs 50G 32G 16G 67% /nfsfile # 挂载成功后就应该能够顺利地看到在执行前面的操作时写入的文件内容了 [root@VM-0-11-tlinux ~]# cat /nfsfile/hello.txt hello worloada 1111 # 如果希望NFS文件共享服务能一直有效，则需要将其写入到fstab文件中 # SERVER:/PATH/TO/EXPORTED_FS /mount_point nfs defaults,_netdev 0 0 [root@VM-0-11-tlinux ~]# vim /etc/fstab 192.168.0.17:/nfs /nfsfile nfs defaults 0 0 参考文档：https://www.escapelife.site/posts/c49dfbab.html apiVersion: apps/v1 kind: StatefulSet metadata: labels: k8s-app: mysql qcloud-app: mysql name: mysql spec: podManagementPolicy: OrderedReady replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: mysql qcloud-app: mysql serviceName: \"\" template: metadata: labels: k8s-app: mysql qcloud-app: mysql spec: containers: - env: - name: MYSQL_ROOT_PASSWORD value: \"123456\" image: mysql:5.7 imagePullPolicy: IfNotPresent name: mysql volumeMounts: - mountPath: /var/lib/mysql name: nfs subPath: mysql_docker/data dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - name: nfs nfs: path: / server: 172.16.3.7 "},"docs/appdeploy/linux-install-mysql.html":{"url":"docs/appdeploy/linux-install-mysql.html","title":"Linux安装mysql数据库","summary":"Linux安装mysql数据库","keywords":"","body":"Linux安装MySQL数据库 1. 环境准备 云服务器或者虚拟机 Linux的版本为CentOS7或者Redhat; 2. 下载安装包 ​ 下载地址 3. 上传安装包 4. 创建目录&并解压 mkdir mysql tar -xvf mysql-8.0.26-1.el7.x86_64.rpm-bundle.tar -C mysql 5. 安装mysql的安装包 cd mysql rpm -ivh mysql-community-common-8.0.26-1.el7.x86_64.rpm rpm -ivh mysql-community-client-plugins-8.0.26-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-8.0.26-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-compat-8.0.26-1.el7.x86_64.rpm yum install openssl-devel rpm -ivh mysql-community-devel-8.0.26-1.el7.x86_64.rpm rpm -ivh mysql-community-client-8.0.26-1.el7.x86_64.rpm rpm -ivh mysql-community-server-8.0.26-1.el7.x86_64.rpm #安装时候 可能需要有些依赖安装包，可以直接执行如下命令 rpm -Uvh *.rpm --nodeps --force 6. 启动MySQL服务 #启动 systemctl start mysqld #重启 systemctl restart mysqld #停服务 systemctl stop mysqld #查询mysql状态 systemctl status mysqld 7. 查询自动生成的root用户密码 使用rpm安装时候，会自动给root生成密码，我们可以通过查询mysql启动日志，查看自动生成的密码 cat /var/log/mysqld.log | grep -C 2 'temporary password' 命令行执行指令 : mysql -u root -p 然后输入上述查询到的自动生成的密码, 完成登录 8. 修改root用户密码 登录到MySQL之后，需要将自动生成的不便记忆的密码修改了，修改成自己熟悉的便于记忆的密码。 ALTER USER 'root'@'localhost' IDENTIFIED BY '1234'; 执行上述的SQL会报错，原因是因为设置的密码太简单，密码复杂度不够。我们可以设置密码的复杂度为简单类型，密码长度为4。 set global validate_password.policy = 0; set global validate_password.length = 4; 降低密码的校验规则之后，再次执行上述修改密码的指令。 9. 创建用户 默认的root用户只能当前节点localhost访问，是无法远程访问的，我们还需要创建一个root账户，用户远程访问 create user 'root'@'%' IDENTIFIED WITH mysql_native_password BY '1234'; 10. 并给root用户分配权限 grant all on *.* to 'root'@'%'; 11. 重新连接MySQL mysql -u root -p 然后输入密码 "},"docs/appdeploy/linux-instal-nodejs.html":{"url":"docs/appdeploy/linux-instal-nodejs.html","title":"Linux安装NodeJS","summary":"Linux安装NodeJS","keywords":"","body":"Linux安装NodeJS Node.js 是一个基于Chrome V8 引擎 JavaScript 运行时环境， nodejs 官网 下载安装包 1，可以在官网控制台下载上传至服务器 2，或者使用命令直接下载 [root@xxx ~/nodejs]# wget https://nodejs.org/dist/v16.14.2/node-v16.14.2-linux-x64.tar.xz // 下载 [root@xxx ~/nodejs]# tar xf node-v16.14.2-linux-x64.tar.xz // 解压 [root@xxx ~/nodejs]# cd nnode-v16.14.2-linux-x64/ // 进入解压目录 历史版本下载 使用RZ上传到服务器并解压 Linux的sz和rz命令，可使用yum命令安装：yum install -y lrzsz [root@xxx ~/nodejs]# pwd /root/nodejs [root@xxx ~/nodejs]# ls -lrt total 21428 -rw-rw-rw- 1 root root 21941244 Apr 9 11:52 node-v16.14.2-linux-x64.tar.xz [root@xxx ~/nodejs]# tar -xvf node-v16.14.2-linux-x64.tar.xz [root@xxx ~/nodejs]# cd /usr/local/ [root@xxx /usr/local]# ls bin etc games include lib lib64 libexec lost+found qcloud sa sbin share src [root@xxx /usr/local]# mv /root/nodejs/node-v16.14.2-linux-x64 . [root@xxx /usr/local]# mv node-v16.14.2-linux-x64/ nodejs 配置环境变量 方式一：环境变量 　1）、加入环境变量，在 /etc/profile 文件末尾增加配置 vi /etc/profile export PATH=$PATH:/usr/local/nodejs/bin 　2）、执行命令使配置文件生效 source /etc/profile 方式二：软链接方式 ln -s /usr/local/nodejs/bin/npm /usr/local/bin/ ln -s /usr/local/nodejs/bin/node /usr/local/bin/ 验证 [root@xxx /usr/local]# node -v v16.14.2 [root@xxx /usr/local]# npm -v 8.5.0 Npm 更换淘宝镜像 npm config set registry https://registry.npm.taobao.org npm install "},"docs/appdeploy/linux-close-ping.html":{"url":"docs/appdeploy/linux-close-ping.html","title":"Linux关闭-开启ICMP协议ping的方法","keywords":"","body":"Linux服务器关闭/开启ICMP协议(ping)的方法 一、内核参数设置 1、允许ping设置\\ 临时 echo 0 >/proc/sys/net/ipv4/icmp_echo_ignore_all 永久 echo net.ipv4.icmp_echo_ignore_all=0 >> /etc/sysctl.conf sysctl -p # 执行这条命令使更改后的 /etc/sysctl.conf 配置文件生效 注意：如果 /etc/sysctl.conf 配置文件里已经有 net.ipv4.icmp_echo_ignore_all 字段了，那么直接用 vim 进去更改对应的值即可。 2、禁止ping设置 临时 echo 1 >/proc/sys/net/ipv4/icmp_echo_ignore_all 永久 echo net.ipv4.icmp_echo_ignore_all=1 >> /etc/sysctl.conf sysctl -p # 执行这条命令使更改后的 /etc/sysctl.conf 配置文件生效 注意：如果 /etc/sysctl.conf 配置文件里已经有 net.ipv4.icmp_echo_ignore_all 字段了，那么直接用 vim 进去更改对应的值即可。 二、防火墙设置 注：使用以下方法的前提是内核配置是默认值，也就是内核没有禁ping 1、允许PING设置 iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT iptables -A OUTPUT -p icmp --icmp-type echo-reply -j ACCEPT 2、禁止PING设置 iptables -A INPUT -p icmp --icmp-type 8 -s 0/0 -j DROP --icmp-type 8 echo request 表示回显请求（ping请求） 0/0 表示所有 IP "},"docs/appdeploy/linux-create-file.html":{"url":"docs/appdeploy/linux-create-file.html","title":"Linux创建指定大小文件","summary":"Linux创建指定大小文件","keywords":"","body":"在使用Linux时候，经常使用用 touch 命令创建一个空文件。当我们排除故障或想在某些特定场景中进行测试时，我们可能需要特定大小的大文件，比如1024MB 或5GB大小的文件，下面介绍几种常用的方法 使用 dd 命令创建大文件 dd 命令用于复制和转换文件 dd 命令是实际写入硬盘，文件产生的速度取决于硬盘的读写速度，根据文件的大小，该命令将需要一些时间才能完成。 假设我们要创建一个名为test.img 的 2 GB 大小的文本文件，可以执行以下操作： dd if=/dev/zero of=test1.txt bs=2G count=1 [root@chen tmp]# dd if=/dev/zero of=text.img bs=2G count=1 #命令 0+1 records in 0+1 records out 2147479552 bytes (2.1 GB) copied, 94.4318 s, 22.7 MB/s [root@cjweichen tmp]# ls -lh text.img #查看文件大小 -rw-r--r-- 1 root root 2.0G Dec 8 10:46 text.img 我们可以根据需要来更改块大小和块数。例如，可以使用 bs=1M 和 count=1024 来获得 1024 Mb 的文件。 使用 truncate 命令创建大文件 truncate 命令将一个文件缩小或者扩展到所需大小。使用 -s 选项来指定文件的大小，执行速度会快些 我们使用 truncare 命令来创建一个 2GB 大小的文件。 truncate -s 2G test2.txt 可以使用ls -lh test2.txt命令查看生成的文件。 默认情况下，如果请求的输出文件不存在，truncate 命令将创建新文件。我们可以使用 -c 选项来避免创建新文件。 使用 fallocate 命令创建大文件 fallocate 命令创建大文件的方法，创建大文件的速度是三个中最快的。 假设我们要创建一个2 GB 的文件，可以执行以下操作： fallocate -l 2G test3.txt 可以使用ls -lh test3.txt查看生成的文件。 结论 dd 和 truncate 创建的文件是稀疏文件。在计算机世界中，稀疏文件是一种特殊文件，具有不同的表观文件大小（它们可以扩展到的最大大小）和真实文件大小（为磁盘上的数据分配了多少空间）。 fallocate 命令则不会创建稀疏文件，而且它的速度更快，这也是我比较推荐使用 fallocate 创建大文件的原因。 "},"docs/Prometheus/kube-state-metrics.html":{"url":"docs/Prometheus/kube-state-metrics.html","title":"Kube-state-metrics部署安装","summary":"在kubernetes集群中部署Kube-state-metrics","keywords":"","body":"背景 在kubernetes集群中已经有了 cadvisor、heapster、metric-server，基本上容器运行的所有指标都能拿到，但是针对下面这种情况获取不到： 比如调度了多少个 replicasset ，现在可用的有几个？ 多少个 Pod 是 running/stopped/terminated 状态？ Pod 重启了多少次？ 有多少 job 在运行中 而这些指标采集则需要 kube-state-metrics 进行采集，它基于 client-go 开发，负责监听 K8s apiserver 从而生成metrics数据，指标数据通过 /metrics Endpoint 暴露，主要是适配 Prometheus 部署安装 1，下载将 kube-state-metrics examples 几个文件，分别为 kube-state-metrics/ ├── cluster-role-binding.yaml ├── cluster-role-binding.yaml ├── deployment.yaml ├── service-account.yaml ├── service.yaml 2，修改镜像地址（默认镜像地址k8s.gcr.io 在国外。可以找个海外机器拉取下来上传到国内镜像仓库进行拉取上传到自己的镜像仓库里面） - image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0 #本人已经拉取到自己镜像仓库 可以修改成 - image: ccr.ccs.tencentyun.com/chenjingwei/kube-state-metrics:v2.6.0 3，安装kube-state-metrics kubectl apply -f ./ 4，查看是否安装成功 # kubectl get pods -n kube-system -o wide | grep kube-state-metrics kube-state-metrics-57768576b6-mh5d8 1/1 Running 0 2m19s 172.16.0.12 172.30.249.130 #通过 /healthz 健康检查端口查看Pod状态。 # curl 172.16.0.12:8080/healthz OK #通过 /metrics 接口可查看其采集的全量数据 # curl 172.16.0.12:8080/metrics #登陆启动POD里面访问测试 [root@centos-777bdddd57-zv7bv /]# curl kube-state-metrics.kube-system.svc.cluster.local:8080 Kube Metrics Server Kube Metrics metrics healthz #表示安装成功 启动时候有可能端口冲突 可以按照如下文档 添加启动参数，参考文档 与prometheus集成 修改prometheus配置文件，添加job_name kube-state-metrics是部署在kube-system命名空间下的，因此在正则匹配上，命名空间为kube-system，svc名称为kube-state-metrics，否则就不进行监控，k8s kube-state-metrics监控任务 job_name: \"kube-state-metrics\" kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_endpoints_name] regex: kube-system;kube-state-metrics action: keep - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 6，查看prometheus的targets 部署成功后，prometheus的target会出现如下标志 可以看到kube-state-metrics监控任务下已经有两个Targets实例了。我们可以到grup界面通过PromQL语句查询相关监控数据。 示例： 查看当前K8s集群有多少configmap资源对象 count(kube_configmap_created) 使用 kube-state-metrics 后的常用场景有： 存在执行失败的 Job kube_job_status_failed{job=\"kubernetes-service-endpoints\",k8s_app=\"kube-state-metrics\"}==1 集群节点状态错误 kube_node_status_condition{condition=\"Ready\",status!=\"true\"}==1 集群中存在启动失败的 Pod kube_pod_status_phase{phase=~\"Failed|Unknown|Pending\"}==1 最近30分钟内有 Pod 容器重启 changes(kube_pod_container_status_restarts[30m])>0 查看当前集群有多少个Pod正在运行 count (kube_pod_container_state_started) 配合报警alertmanager可以更好地监控集群的运行 与metric-server的对比 metric-server（或heapster）是从 api-server 中获取 cpu、内存使用率这种监控指标，并把他们发送给存储后端，如 influxdb 或云厂商，他当前的核心作用是：为 HPA 等组件提供决策指标支持。 kube-state-metrics 关注于获取 k8s 各种资源的最新状态，如 deployment 或者 daemonset，之所以没有把kube-state-metrics 纳入到 metric-server 的能力中，是因为他们的关注点本质上是不一样的。metric-server仅仅是获取、格式化现有数据，写入特定的存储，实质上是一个监控系统。而 kube-state-metrics 是将 k8s 的运行状况在内存中做了个快照，并且获取新的指标，但他没有能力导出这些指标 换个角度讲，kube-state-metrics 本身是 metric-server 的一种数据来源，虽然现在没有这么做。 另外，像 Prometheus 这种监控系统，并不会去用 metric-server 中的数据，他都是自己做指标收集、集成的（Prometheus包含了metric-server的能力），但 Prometheus 可以监控 metric-server 本身组件的监控状态并适时报警，这里的监控就可以通过 kube-state-metrics 来实现，如 metric-server pod 的运行状态。 优化点和问题 因为 kube-state-metrics 是监听资源的 add、delete、update 事件，那么在 kube-state-metrics 部署之前已经运行的资源，岂不是拿不到数据？kube-state-metric 利用 client-go 可以初始化所有已经存在的资源对象，确保没有任何遗漏 kube-state-metrics 当前不会输出 metadata 信息(如 help 和 description） 缓存实现是基于 golang 的 map，解决并发读问题当期是用了一个简单的互斥锁，可以解决问题，后续会考虑golang 的 sync.Map 安全 map。 kube-state-metrics 通过比较 resource version 来保证 event 的顺序 kube-state-metrics 并不保证包含所有资源 相关yaml文件 附上部署的yaml： apiVersion: v1 automountServiceAccountToken: false kind: ServiceAccount metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics rules: - apiGroups: - \"\" resources: - configmaps - secrets - nodes - pods - services - serviceaccounts - resourcequotas - replicationcontrollers - limitranges - persistentvolumeclaims - persistentvolumes - namespaces - endpoints verbs: - list - watch - apiGroups: - apps resources: - statefulsets - daemonsets - deployments - replicasets verbs: - list - watch - apiGroups: - batch resources: - cronjobs - jobs verbs: - list - watch - apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - list - watch - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - policy resources: - poddisruptionbudgets verbs: - list - watch - apiGroups: - certificates.k8s.io resources: - certificatesigningrequests verbs: - list - watch - apiGroups: - storage.k8s.io resources: - storageclasses - volumeattachments verbs: - list - watch - apiGroups: - admissionregistration.k8s.io resources: - mutatingwebhookconfigurations - validatingwebhookconfigurations verbs: - list - watch - apiGroups: - networking.k8s.io resources: - networkpolicies - ingresses verbs: - list - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - list - watch - apiGroups: - rbac.authorization.k8s.io resources: - clusterrolebindings - clusterroles - rolebindings - roles verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metrics subjects: - kind: ServiceAccount name: kube-state-metrics namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics namespace: kube-system spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: kube-state-metrics template: metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 spec: automountServiceAccountToken: true containers: - image: ccr.ccs.tencentyun.com/chenjingwei/kube-state-metrics:v2.6.0 livenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 5 timeoutSeconds: 5 name: kube-state-metrics ports: - containerPort: 8080 name: http-metrics - containerPort: 8081 name: telemetry readinessProbe: httpGet: path: / port: 8081 initialDelaySeconds: 5 timeoutSeconds: 5 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true runAsUser: 65534 nodeSelector: kubernetes.io/os: linux serviceAccountName: kube-state-metrics --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics namespace: kube-system spec: clusterIP: None ports: - name: http-metrics port: 8080 targetPort: http-metrics - name: telemetry port: 8081 targetPort: telemetry selector: app.kubernetes.io/name: kube-state-metrics "},"docs/Prometheus/missing-container-metrics.html":{"url":"docs/Prometheus/missing-container-metrics.html","title":"missing-container-metrics部署安装和使用","summary":"missing-container-metrics监控pod oomkill","keywords":"","body":" 本文主要介绍missing-container-metrics监控pod OOMKilled 背景 Kubernetes 默认情况下使用 cAdvisor 以及 kube-state-metrics 来收集容器的各项指标，绝大多数场景下能够满足业务的基本的需求，但还是有所欠缺，比如缺少对以下几个指标的收集 OOM kill 容器重启的次数 容器的退出码 missing-container-metrics 这个metrics 弥补了 cAdvisor 的缺陷，新增了以上几个指标，用户可以利用这些指标迅速定位某些故障。例如，假设某个容器有多个子进程，其中某个子进程被 OOM kill，但容器还在运行，如果不对 OOM kill 进行监控，用户很难对故障进行定位 安装部署 missing-container-metrics项目介绍 1，添加helm仓库 helm repo add missing-container-metrics https://draganm.github.io/missing-container-metrics 2， 下载helm到本地，可以修改对应的 value.yaml helm pull missing-container-metrics/missing-container-metrics tar xvf missing-container-metrics-0.1.1.tgz cd missing-container-metrics ls Chart.yaml README.md templates values.yaml 3， 可配置项 变量 描述说明 默认值 image.repository 镜像名称 dmilhdef/missing-container-metrics image.pullPolicy 镜像拉取策略 IfNotPresent image.tag 镜像tag v0.21.0 imagePullSecrets 拉取镜像的secret [] nameOverride 覆盖生成的图表名称。默认为 .Chart.Name。 fullnameOverride 覆盖生成的版本名称。默认为 .Release.Name。 podAnnotations Pod 的Annotations {\"prometheus.io/scrape\": \"true\", \"prometheus.io/port\": \"3001\"} podSecurityContext 为 pod 设置安全上下文 securityContext 为 pod 中的容器设置安全上下文 resources PU/内存资源请求/限制 {} useDocker 从 Docker 获取容器信息,如果容器运行时为docker ,设置为true false useContainerd 从 Containerd 获取容器信息,如果容器运行时为containers ,设置为true true 由于我们集群的运行时是docker的 所以需要修改missing-container-metrics/values.yaml 中`useDocker为true，然后安装 4，执行安装 # kubectl create namespace missing-container-metrics # helm install missing-container-metrics missing-container-metrics -n missing-container-metrics NAME: missing-container-metrics LAST DEPLOYED: Sun Jun 26 13:32:43 2022 NAMESPACE: missing-container-metrics STATUS: deployed REVISION: 1 TEST SUITE: None #helm -n missing-container-metrics list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION missing-container-metrics missing-container-metrics 1 2022-09-17 22:43:56.998806908 +0800 CST deployed missing-container-metrics-0.1.1 0.21.0 [root@VM-249-130-tlinux ~]# kubectl -n missing-container-metrics get pods | grep missing-container-metrics missing-container-metrics-bf7gq 1/1 Running 0 60s 可以通过访问服务的3001端口查看metrics,例如 #curl 172.16.0.29:3001/metrics | grep memory-request-limit 服务公开了如下的指标： 指标 说明 container_restarts 容器的重启次数 container_ooms 容器的 OOM 杀死数。这涵盖了容器 cgroup 中任何进程的 OOM 终止 container_last_exit_code 容器的最后退出代码 每一个指标包含如下标签： 指标ID 说明 docker_container_id 容器的完整 ID container_short_id Docker 容器 ID 的前 6 个字节 docker_container_id 容器 id 以与 kubernetes pod 指标相同的格式表示 - 以容器运行时为前缀docker://并containerd://取决于容器运行时。这使得 Prometheus 中的kube_pod_container_info`` name 容器的名称 image_id 镜像id 以与 k8s pod 的指标相同的格式表示。这使得 Prometheus 中的kube_pod_container_info pod 如果io.kubernetes.pod.name在容器上设置了pod标签，则其值将设置为指标中的标签 namespace 如果io.kubernetes.pod.namespace容器上设置了namespace`标签，则其值将设置为指标的标签 与自建prometheus集成 1，修改prometheus配置文件 kube-state-metrics是部署在kube-system命名空间下的，因此在正则匹配上，命名空间为kube-system，svc名称为missing-container-metrics，否则就不进行监控 job_name: \"missing-container-metrics\" kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_endpoints_name] regex: missing-container-metrics;missing-container-metrics action: keep - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 由于helm部署只部署了DaemonSet ，可以手动创建一个svc apiVersion: v1 kind: Service metadata: name: missing-container-metrics namespace: missing-container-metrics spec: ports: - name: 3001-3001-tcp-2tsffjchpoi port: 3001 protocol: TCP targetPort: 3001 selector: app.kubernetes.io/instance: missing-container-metrics app.kubernetes.io/name: missing-container-metrics sessionAffinity: None type: ClusterIP 查看prometheus的targets 部署成功后，prometheus的target会出现如下标志 创建告警规则 ###添加 prometheusOperator: podMonitor: # Create a Prometheus Operator PodMonitor resource enabled: true # Namespace defaults to the Release namespace but can be overridden namespace: \"\" # Additional labels to add to the PodMonitor so it matches the Operator's podMonitorSelector selector: app.kubernetes.io/name: missing-container-metrics prometheusRule: # Create a Prometheus Operator PrometheusRule resource enabled: true # Namespace defaults to the Release namespace but can be overridden namespace: \"\" # Additional labels to add to the PrometheusRule so it matches the Operator's ruleSelector selector: prometheus: k8s role: alert-rules # The rules can be set here. An example is defined here but can be overridden. rules: - alert: ContainerOOMObserved annotations: message: A process in this Pod has been OOMKilled due to exceeding the Kubernetes memory limit at least twice in the last 15 minutes. Look at the metrics to determine if a memory limit increase is required. expr: sum(increase(container_ooms[15m])) by (exported_namespace, exported_pod) > 2 labels: severity: warning - alert: ContainerOOMObserved annotations: message: A process in this Pod has been OOMKilled due to exceeding the Kubernetes memory limit at least ten times in the last 15 minutes. Look at the metrics to determine if a memory limit increase is required. expr: sum(increase(container_ooms[15m])) by (exported_namespace, exported_pod) > 10 labels: severity: critical 2，模拟OOM 创建一个 Pod，尝试分配超出其限制的内存。 这是一个 Pod 的配置文件，其拥有一个容器，该容器的内存请求为 50 MiB，内存限制为 100 MiB： 在配置文件的 args 部分中，你可以看到容器会尝试分配 250 MiB 内存，这远高于 100 MiB 的限制。 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: memory-request-limit qcloud-app: memory-request-limit name: memory-request-limit namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: memory-request-limit qcloud-app: memory-request-limit template: metadata: labels: k8s-app: memory-request-limit qcloud-app: memory-request-limit spec: containers: - args: - --vm - \"1\" - --vm-bytes - 250M - --vm-hang - \"1\" command: - stress image: polinux/stress:latest imagePullPolicy: IfNotPresent name: memory-demo-ctr resources: limits: memory: 100Mi requests: memory: 50Mi sum(increase(container_ooms[15m])) by (exported_namespace, exported_pod) > 3 云原生监控TMP使用missing-container-metrics 1，添加数据采集配置 登陆Promtheus 监控控制台，选择集群监控> 数据采集配置>自定义监控> 新增数据采集配置 2，查看当前采集，如下图所示，表示已经采集到监控数据 3， 配置告警规则prometheusRule 查看告警历史 参考文档：https://github.com/draganm/missing-container-metrics "},"docs/istio/istioe-demo.html":{"url":"docs/istio/istioe-demo.html","title":"服务网格demo","keywords":"","body":"部署 demo 应用 Demo 应用概览 Demo 应用是一个电商网站，基于 Istio 社区的官方样例 bookinfo 改造，由 6 个服务组成： frontend：网站前端，调用 user、product、cart、order 服务。 product：商品服务，提供商品信息。product 包含两个版本，版本一没有顶部广告 banner；版本二有顶部广告 banner。 user：用户登录服务，提供登录功能。 cart：购物车服务，提供添加、查看购物车功能，调用库存服务提供库存告警功能，需要登录才可以下单。 order：订单结算服务，登录后点击 checkout 后可发起结算，结算时需要调用 stock 库存服务查询库存情况，库存不足会下单失败。order 包含两个版本，版本一无积分抵扣运费的功能，版本二有积分抵扣运费的功能。 stock：库存服务，为 order 购物车服务的库存告警功能和 order 订单结算服务的库存查询提供库存信息。 Demo 应用架构 Demo 应用首页 安装 Demo 应用 您可以在 TCM Demo 仓库 中获取 Demo 应用，由于 TCM 的 sidecar 自动注入需要标记 Istio 版本，您需要选择与您 Istio 版本一致的分支，或直接修改 master 分支，路径 mesh-demo/yamls/step01-apps-zone-a.yaml 中 base namespace 中的版本 label： 下载tcm-demo [root@VM-249-47-tlinux ~]# git clone git@github.com:Tencent-Cloud-Mesh/mesh-demo.git 修改labels和服务网格版本保持一致 apiVersion: v1 kind: Namespace metadata: name: base labels: istio.io/rev: 1-10-3 spec: finalizers: - kubernetes 例如，您的 istio 版本为 1.8.1，则需要将 istio.io/rev: 1-10-3``istio.io/rev: 1-8-1 使用如下命令可快速部署 Demo 应用： kubectl apply -f yamls/step01-apps-zone-a.yaml 配置公网访问 1. 创建 Gateway 配置监听规则 首先需要创建 Gateway 资源，配置 istio-ingressgateway 的监听器规则，端口为 80，协议为 http。用户只需要配置 Gateway 规则，TCM 后台会自动实现 istio-ingressgateway 相关的 pod、service 和绑定的负载均衡器 CLB 的配置同步。 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: frontend-gw namespace: base spec: servers: - port: number: 80 name: http protocol: HTTP hosts: - '*' selector: app: istio-ingressgateway istio: ingressgateway 2. 配置路由规则 监听器规则配置完成后，还需要通过 Virtual Service 资源配置路由规则，将来自 istio-ingressgateway 的流量路由至 frontend 服务。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend-vs namespace: base spec: hosts: - '*' gateways: - base/frontend-gw http: - route: - destination: host: frontend.base.svc.cluster.local 配置完成后，通过 istio-ingressgateway 的公网 IP 地址即可访问到 Demo 网站，当前部署的网站的结构如下图所示： 单击链接访问网站后，可登录（1-5 均可登录，其中 1-3 为会员，4-5 为非会员）、添加购物车，下单，以产生调用完所有部署的服务的请求，网站界面右下角的悬浮窗展示了前端服务当前调用服务的名称、地域、版本、pod name 信息。悬浮窗信息展示如下图所示： 账号直接输入1，或者2-5 任一数字就可以 网格拓扑如下图所示： 需要客户使用prometheus监控才能看到 链路追踪 : 链路追踪需要将应用性能观测服务 APM 作为网格调用追踪消费端才能正常使用，您可以在网格基本信息-调用追踪-消费端设置中启用应用性能观测服务 APM 服务。 多版本路由 1，操作场景 网站计划推出会员积分抵扣运费的活动以发展会员。电商网站策划了会员积分抵扣订单金额的新功能，当前部署的 order 服务由 v1 deployment 提供，没有运费抵扣的功能；网站新开发了 order 服务的 v2 版本，有积分抵扣运费的功能。网站希望可以于请求的 header 中是否会员的 cookie 信息进行路由，会员路由至 order v2（有运费抵扣功能），非会员路由至 order v1（无运费抵扣功能）。 服务多版本路由概览图如下所示 部署 order v2 至集群 : apiVersion: apps/v1 kind: Deployment metadata: name: order-v2 namespace: base labels: app: order version: v2 spec: replicas: 1 selector: matchLabels: app: order version: v2 template: metadata: labels: app: order version: v2 spec: containers: - name: order image: ccr.ccs.tencentyun.com/zhulei/testorder2:v1 imagePullPolicy: Always env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: REGION value: \"guangzhou-zoneA\" ports: - containerPort: 7000 protocol: TCP 部署完成后，由于还未配置路由规则，此时访问 order 服务的流量会被随机路由至 v1 版本或 v2 版本。如下图所示： 配置基于流量特征内容的路由规则前先需要通过 DestinationRule 定义 order 服务的两个版本。如下图所示： 定义 order 服务的版本如下图所示： 2，操作步骤 创建DestinationRule 或者 YAML 文件至主集群完成 DestinationRule 的创建： apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: order namespace: base spec: host: order subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 exportTo: - '*' 配置VirtualService 两个版本定义完成后，通过 VirtualService 定义按流量特征进行路由，请求的 header-cookie 中 vip=false 时路由至 order 服务的 v1 subset，vip=true 时路由至 order 服务的 v2 subset。即会员的请求路由至 order v2，非会员的请求路由至 order v1。提交以下 yaml 资源至主集群，即可完成上述配置。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: order-vs namespace: base spec: hosts: - order.base.svc.cluster.local http: - match: - headers: cookie: exact: vip=false route: - destination: host: order.base.svc.cluster.local subset: v1 - match: - headers: cookie: exact: vip=true route: - destination: host: order.base.svc.cluster.local subset: v2 配置完成后，可登录会员帐号（ID：1-3）加购和买单，发现有运费抵扣功能，流量被路由到了 order v2 版本；登录非会员账号（ID：4-5）加购和买单，发现无运费抵扣功能，根据 header 中的 VIP 字段信息，请求被路由到了最初部署的 order v1 版本。版本信息也可通过左下角悬浮窗中的信息观察。 会员用户请求被路由到 v2 版本如下图所示： 灰度发布 1，操作场景 随着网站流量的增加，网站开始有了广告投放的需求，广告投放需要在商品页面增加广告位。网站的开发人员新开发了 product 服务的 v2 版本，以 product v2 的 deployment 的形式提供，并希望对 product-v2 版本做灰度发布。 灰度发布概览图如下所示： 2，操作步骤 部署product v2 版本 apiVersion: apps/v1 kind: Deployment metadata: name: product-v2 namespace: base labels: app: product version: v2 spec: replicas: 1 selector: matchLabels: app: product version: v2 template: metadata: labels: app: product version: v2 spec: containers: - name: product image: ccr.ccs.tencentyun.com/zhulei/testproduct2:v1 imagePullPolicy: Always env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: REGION value: \"guangzhou-zoneA\" ports: - containerPort: 7000 创建DR和VS 通过 DR 定义服务版本 + 通过 VS 定义权重路由来完成灰度发布的第一步，将部分流量（50%）路由至 product v2 subset 以验证新版本，剩余部分（50%）的流量仍然路由至 product v1 版本。将以下 YAML 文件提交至主集群即可完成以上设定。 product v2 版本验证通过后，即可修改关联 product 的 VirtualService 中路由规则目的端的权重，设置访问 product 服务的所有流量（100%）至 v2 版本，设置完成后可刷新商品列表页面验证。基于 virtual Service 更改权重如下图所示： 故障注入测试 电商网站业务团队需要模拟访问库存服务存在延迟故障时网站系统的行为，以测试服务弹性，网站用户的优化访问体验。 stock 服务 fixed delay 7s 如下图所示： 操作步骤 通过配置绑定 stock 服务的 VritualService，设置访问 stock 服务的故障注入策略：100%的请求会有 7 秒的固定延迟。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: stock-vs namespace: base spec: hosts: - stock.base.svc.cluster.local http: - route: - destination: host: stock.base.svc.cluster.local fault: delay: fixedDelay: 7000ms percentage: value: 100 配置完成后，在 Demo 网站页面单击 “ADD TO CART” 加入购物车或者单击 “YOUR CART” 调用购物车服务，购物车服务会调用 stock 服务查询库存，访问 stock 服务有 7 秒的固定延迟故障注入策略，以及在购物车页面单击“CHECKOUT”发起结算会调用 order 服务，order 服务会调用 stock 服务查询库存，访问 stock 服务有 7 秒的固定延迟故障注入策略。此时页面处于加载中的状态会持续 7 秒一直等待故障结束，造成网站用户的不流畅浏览体验。 cart 服务调用 stock 服务的等待状态如下图所示： 服务超时配置 order 服务 timeout 3s 如下图所示： 通过对 stock 服务配置故障注入，发现由于故障会导致网站用户的请求一直处于等待状态，为优化网站用户的浏览体验，需要为服务配置 timeout。 应用以下 VS，为 order 服务配置 3 秒的超时时间，cart 服务不设置超时时间作为参照对比。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: order-vs namespace: base spec: hosts: - order.base.svc.cluster.local http: - match: - headers: cookie: exact: vip=false route: - destination: host: order.base.svc.cluster.local subset: v1 timeout: 3000ms - match: - headers: cookie: exact: vip=true route: - destination: host: order.base.svc.cluster.local subset: v2 timeout: 3000ms 配置完成后，选择商品加入购物车，此时访问 cart 服务会有 7 秒的故障注入访问延时，且没有 timeout 处理，点击“CHECKOUT”发起结算调用 order 服务，此时虽然访问 order 服务也会有 7 秒的故障注入访问延时，但是有 3 秒的 timeout 超时处理，在调用 order 服务 3 秒内没有反应会做超时处理。 cart 服务调用 order 服务 timeout 显示如下图所示： 超时配置已完成，对于服务的故障注入测试已完成，可删除关联 stock 服务的 VirtualService 资源以解除对 stock 服务配置的故障注入策略。 删除 stock 服务关联 Virtual Service 操作如下图所示： 会话保持 购物车服务由多个 pod 副本运行，需要会话保持功能，以保证同一用户请求被路由至同一个 pod，保证同一用户的购物车信息不会丢失。 会话保持如下图所示： 操作步骤 会话保持功能可通过设置 cart 服务 DestinationRule 的负载均衡策略实现，以请求中 header 中的 UserID 做一致性 hash 负载均衡。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: cart namespace: base spec: host: cart trafficPolicy: loadBalancer: consistentHash: httpHeaderName: UserID exportTo: - '*' 配置完成后，可在登录状态多次点击 “Your Cart” 或点击 “ADD TO CART” 调用 cart 服务验证会话保持功能，同一用户的多次请求会被路由至同一个 pod，左下角悬浮窗可查看提供 cart 服务的 pod name。同一用户多次请求的 pod name 不会变化。来自同一用户的多次请求被负载均衡至相同 pod 如下图所示： 使用连接池限制并发 操作场景 随着电商网站业务规模的增大，对网站的访问请求并发量开始增加，网站业务人员计划限制服务最大并发数，保证服务运行健壮性。 操作步骤 为模拟“高并发”请求场景，首先通过提交以下 YAML 部署 client 服务（10 pods），模拟对 user 服务的高并发请求。 apiVersion: v1 kind: Namespace metadata: name: test labels: istio-injection: enabled spec: finalizers: - kubernetes --- apiVersion: apps/v1 kind: Deployment metadata: name: client namespace: test labels: app: client spec: replicas: 10 selector: matchLabels: app: client template: metadata: labels: app: client spec: containers: - name: client image: ccr.ccs.tencentyun.com/zhulei/testclient:v1 imagePullPolicy: Always env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: REGION value: \"guangzhou-zoneA\" ports: - containerPort: 7000 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: client namespace: test labels: app: client spec: ports: - name: http port: 7000 protocol: TCP selector: app: client type: ClusterIP 此时对于访问 user 服务没有最大并发数限制，所有请求均可访问成功。通过 TKE 控制台 client deployment 查看 client pod 日志，所有的请求均返回了用户名 Kevin，证明访问请求成功。 高并发请求如下图所示： 通过配置 user 服务的 Destination Rule 限制最大并发数为 1： apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: user namespace: base spec: host: user trafficPolicy: connectionPool: http: http1MaxPendingRequests: 1 http2MaxRequests: 1 maxRequestsPerConnection: 1 exportTo: - '*' 此时查看 client pod 日志，部分请求开始出现异常，未返回用户名，请求失败，连接池起到了限制访问服务最大并发数的作用。 部分请求访问失败如下图所示： 删除流量策略相关配置如下图所示： 连接池测试完成后，在 user 服务的详情页面删除连接池相关流量策略配置。 "},"docs/istio/tentcent-istio.html":{"url":"docs/istio/tentcent-istio.html","title":"腾讯云服务网格","keywords":"","body":"Service Mesh Service Mesh 的中文译为 “服务网格” ，是一个用于处理服务和服务之间通信的基础设施层，它负责为构建复杂的云原生应用传递可靠的网络请求，并为服务通信实现了微服务所需的基本组件功能，例如服务发现、负载均衡、监控、流量管理、 访问控制等。在实践中，服务网格通常实现为一组和应用程序部署在一起的轻量级的网络代理，但对应用程序来说是透明的。右图， 绿色方块为应用服务，蓝色方块为 Sidecar Proxy，应用服务之间通过 Sidecar Proxy 进行通信，整个服务通信形成图中的蓝色网络连线，图中所有蓝色部分就形成一个网络，这个就是服务网格名字的由来 Service Mesh特点 Service Mesh有以下特点： • 治理能力独立（Sidecar） • 应用程序无感知 • 服务通信的基础设施层 • 解耦应用程序的重试/超时、监控、追踪和服务发现 Istio概述 Isito是Service Mesh的产品化落地，是目前最受欢迎的服务网格，功能丰富、成熟度高。 Linkerd是世界上第一个服务网格类的产品 • 连接（Connect） - 流量管理 - 负载均衡 - 灰度发布 • 安全（Secure） - 认证 - 鉴权 • 控制（Control） - 限流 - ACL • 观察（Observe） - 监控 - 调用链 Istio版本变化 在Istio1.5版本发生了一个重大变革，彻底推翻原有控制平面的架构，将有原有多个组件整合为单体结构 “istiod”， 同时废弃了Mixer 组件，如果你正在使用之前版本，必须了解这些变化 Istio架构与组件 Istio服务网格在逻辑上分为数据平面和控制平面。 • 控制平面： 使用全新的部署模式： istiod，这个组件负责处理Sidecar注入、证书分发、配置管理等功能，替 代原有组件，降低复杂度，提高易用性。 • Pilot：策略配置组件，为Proxy提供服务发现、智能路由、错误处理等。 • Citadel： 安全组件，提供证书生成下发、加密通信、访问控制。 • Galley： 配置管理、验证、分发。 • 数据平面： 由一组Proxy组成， 这些Proxy负责所有微服务网络通信，实现高效转发和策略。使用envoy实现， envoy是一个基于C++实现的L4/L7 Proxy转发器，是Istio在数据平面唯一的组件。 Istio基本概念 Istio 有 4 个配置资源，落地所有流量管理需求： • VirtualService（虚拟服务）：实现服务请求路由规则的功能。 • DestinationRule（目标规则）：实现目标服务的负载均衡、服务发现、故障处理和故障注入的功能。 • Gateway（网关）：让服务网格内的服务，可以被全世界看到。 • ServiceEntry（服务入口） ：允许管理网格外的服务的流量 部署Istio tar zxvf istio-1.8.2-linux.tar.gz cd istio-1.8.2 mv bin/istioctl /usr/bin istioctl install kubectl get pods -n istio-system kubectl get svc -n istio-system 卸载： istioctl manifest generate | kubectl delete -f - Sidercar注入 部署httpbin Web示例： cd istio-1.8.2/samples/httpbin # 手动注入 kubectl apply -f Istio与K8s集成流程 Istio 流量管理核心资源 核心资源： • VirtualService（虚拟服务） • DestinationRule（目标规则） • Gateway（网关） • ServiceEntry（服务入口） VirtualService VirtualService（虚拟服务）： • 定义路由规则 • 描述满足条件的请求去哪里 DestinationRule DestinationRule（目标规则）：定义虚拟服务路由目标地址 真实地址，即子集（subset），支持多种负载均衡策略： • 随机 • 权重 • 最小请求数 Gateway Gateway（网关）：为网格内服务对外访问入口，管理进出网格的流量，根据流入流出方向分为： • IngressGateway：接收外部访问，并将流量转发到网格内的服务。 • EgressGateway：网格内服务访问外部应用。 Gateway（网关）与Kubernetes Ingress有什么区别？ Kubernetes Ingress与Getaway都是用于为集群内服务提供访问入口， 但Ingress主要功能比较单一，不易于Istio现有流量管理功能集成。 目前Gateway支持的功能： • 支持L4-L7的负载均衡 • 支持HTTPS和mTLS • 支持流量镜像、熔断等 ServiceEntry ServiceEntry（服务入口）： 将网格外部服务添加到网格内，像网格内其他服务一样管理。 Istio 流量管理案例 主流发布方案 • 蓝绿发布 • 滚动发布 • 灰度发布（金丝雀发布） • A/B Test 蓝绿发布 项目逻辑上分为AB组，在项目升级时，首先把A组从负载均衡 中摘除，进行新版本的部署。 B组仍然继续提供服务。 A组升级 完成上线， B组从负载均衡中摘除。 特点： • 策略简单 • 升级/回滚速度快 • 用户无感知，平滑过渡 缺点： • 需要两倍以上服务器资源 • 短时间内浪费一定资源成本 • 有问题影响范围大 滚动发布 每次只升级一个或多个服务，升级完成后加入生产环境，不断执行这个过程，直到集群中的全部旧版升级新版本。Kubernetes的默认发布策略。 特点： • 用户无感知，平滑过渡 缺点： • 部署周期长 • 发布策略较复杂 • 不易回滚 • 有影响范围较大 灰度发布（金丝雀发布） 只升级部分服务，即让一部分用户继续用老版本，一部分用户，开始用新版本，如果用户对新版本没有什么意见，那么逐步扩大范围，把所有用户都迁移到新版本上面来。 特点： • 保证整体系统稳定性 • 用户无感知，平滑过渡 缺点： • 自动化要求高 A/B Test 灰度发布的一种方式，主要对特定用户采样后，对收集到的反馈数据做相关对比，然后根据比对结果作出决策。用来测试应 用功能表现的方法，侧重应用的可用性，受欢迎程度等， 最后决定是否升级 部署Bookinfo微服务项目 Bookinfo 是官方提供一个图书评测系统微服务项目示例， 分为四个微服务： 服务 说明 调用服务 productpage 主页 reviews、 details reviews 评论内容 ratings details 详细内容 ratings 评分 1、创建命名空间并开启自动注入 kubectl create ns bookinfo kubectl label ns bookinfo istio-injection=enabled kubectl get ns --show-labels 2、部署应用YAML cd istio-1.8.2/samples/bookinfo kubectl apply -f platform/kube/bookinfo.yaml -n bookinfo kubectl get pod -n bookinfo 3、创建Ingress网关 kubectl apply -f networking/bookinfo-gateway.yaml -n book 4、确认网关和访问地址，访问应用页面 kubectl get pods -n istio-system 访问地址： http://192.168.31.62:31928/productpage "}}